{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 1: 环境设置、数据加载与spaCy配置**\n",
    "\n",
    "**目标:** 导入所有必需的库，加载核心数据和规则文件，并利用`spaCy`的强大功能为后续处理做好准备。\n",
    "\n",
    "**关键操作:** 遍历我们 `merge_dict.pkl` 中的所有“超级词”（如 `peoples_bank_of_china`），将它们作为**特殊规则**添加到`spaCy`的分词器（Tokenizer）中，以确保`spaCy`在处理文本时，**绝不会**将这些超级词错误地拆分开。"
   ],
   "id": "81329b116c9ab35a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:46:45.287796Z",
     "start_time": "2025-08-07T14:46:38.789711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 1: 环境设置、数据加载与spaCy配置 (升级版) ---\n",
    "# =============================================================================\n",
    "\n",
    "# 作用: 导入所有项目运行所需的Python库。\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.attrs import ORTH\n",
    "import gc\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import re  # [新增] 导入正则表达式库\n",
    "\n",
    "# --- 核心配置区 ---\n",
    "# 作用: 全局控制参数，方便调试与切换运行模式。\n",
    "RUNNING_ENV = 'local'\n",
    "TEST_MODE = False\n",
    "TEST_SAMPLE_SIZE = 1000\n",
    "\n",
    "# --- 并行处理配置 ---\n",
    "# 作用: 智能检测CPU核心数，并为多进程处理设定合理的进程数。\n",
    "cpu_cores = psutil.cpu_count(logical=False)\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8)\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "\n",
    "# 作用: 主动检测Windows操作系统，并强制使用单进程，以规避spaCy在Windows上的已知多进程问题。\n",
    "if os.name == 'nt':\n",
    "    print(\"检测到Windows系统，为避免spaCy多进程已知问题，将强制使用单进程(N_PROCESSES=1)。\")\n",
    "    N_PROCESSES = 1\n",
    "\n",
    "# --- 路径智能管理 ---\n",
    "# 作用: 根据运行环境（本地或服务器）自动构建正确的文件路径。\n",
    "print(f\"检测到运行环境为: 【{RUNNING_ENV.upper()}】\")\n",
    "BASE_DATA_PROCESSED_PATH = '../data_processed' if RUNNING_ENV == 'local' else '/mnt/data/data_processed'\n",
    "BASE_CONFIG_PATH = '../configs' if RUNNING_ENV == 'local' else '/mnt/data/configs'\n",
    "\n",
    "# 作用: 确保配置文件目录存在，防止后续写入文件时出错。\n",
    "if not os.path.exists(BASE_CONFIG_PATH):\n",
    "    os.makedirs(BASE_CONFIG_PATH)\n",
    "    print(f\"创建了configs目录: {BASE_CONFIG_PATH}\")\n",
    "\n",
    "# 作用: 定义所有输入输出文件的完整路径。\n",
    "# 输入文件路径\n",
    "SOLIDIFIED_TEXT_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_solidified.pkl')\n",
    "MERGE_DICT_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'merge_dict.pkl')\n",
    "NEW_STOPWORDS_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'new_stopwords.pkl')\n",
    "PROJECT_STOPWORDS_PATH = os.path.join(BASE_CONFIG_PATH, 'project_specific_stopwords.txt')\n",
    "# 输出文件路径\n",
    "CLEANED_TOKENS_PKL_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_cleaned_tokens.pkl')\n",
    "CLEANED_TOKENS_CSV_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_cleaned_tokens_for_review.csv')\n",
    "\n",
    "# --- 打印最终配置信息 ---\n",
    "# 作用: 在程序开始时清晰地展示所有配置，便于检查和追溯。\n",
    "print(\"\\n--- 环境准备 ---\")\n",
    "if TEST_MODE:\n",
    "    print(f\"🚀🚀🚀 运行在【快速测试模式】下，将处理前 {TEST_SAMPLE_SIZE} 行新闻！🚀🚀🚀\")\n",
    "else:\n",
    "    print(f\"🚢🚢🚢 运行在【完整数据模式】下，将处理所有新闻。🚢🚢🚢\")\n",
    "print(f\"固化文本输入: {SOLIDIFIED_TEXT_PATH}\")\n",
    "print(f\"合并词典输入: {MERGE_DICT_PATH}\")\n",
    "print(f\"审核停用词输入: {NEW_STOPWORDS_PATH}\")\n",
    "print(f\"项目停用词输入: {PROJECT_STOPWORDS_PATH}\")\n",
    "print(f\"最终Tokens输出 (PKL): {CLEANED_TOKENS_PKL_PATH}\")\n",
    "print(f\"最终Tokens输出 (CSV): {CLEANED_TOKENS_CSV_PATH}\")\n",
    "print(f\"spaCy将使用 {N_PROCESSES} 个进程进行处理。\")\n",
    "\n",
    "\n",
    "# --- 加载数据和规则 ---\n",
    "# 作用: 从磁盘读取经过实体固化的新闻数据和合并规则字典。\n",
    "print(\"\\n--- 阶段一: 加载数据与规则 ---\\n\")\n",
    "try:\n",
    "    read_nrows = TEST_SAMPLE_SIZE if TEST_MODE else None\n",
    "    df = pd.read_pickle(SOLIDIFIED_TEXT_PATH)\n",
    "    if read_nrows:\n",
    "        df = df.head(read_nrows)\n",
    "    print(f\"✅ 成功加载 {len(df)} 篇固化文本。\")\n",
    "\n",
    "    with open(MERGE_DICT_PATH, 'rb') as f:\n",
    "        merge_dict = pickle.load(f)\n",
    "    print(f\"✅ 成功加载 {len(merge_dict)} 条合并规则。\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 错误: 缺少必要的输入文件: {e.filename}。请确保已成功运行之前的Notebook。\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# --- 加载和配置spaCy及NLTK模型 ---\n",
    "# 作用: 加载spaCy模型并为其分词器添加特殊规则，以保护固化实体不被拆分。同时检查并下载NLTK WordNet语料库。\n",
    "if not df.empty:\n",
    "    print(\"\\n正在加载和配置spaCy模型...\")\n",
    "    start_time_spacy = time.time()\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "    special_cases = {}\n",
    "    for standard_form in set(merge_dict.values()):\n",
    "        special_cases[standard_form] = [{ORTH: standard_form}]\n",
    "\n",
    "    for case, rule in special_cases.items():\n",
    "        nlp.tokenizer.add_special_case(case, rule)\n",
    "\n",
    "    print(f\"✅ spaCy模型加载并配置完成，已添加 {len(special_cases)} 条特殊分词规则。耗时: {time.time() - start_time_spacy:.2f} 秒。\")\n",
    "\n",
    "    print(\"\\n正在检查NLTK WordNet语料库...\")\n",
    "    try:\n",
    "        wordnet.ensure_loaded()\n",
    "        print(\"✅ NLTK WordNet 已加载。\")\n",
    "    except LookupError:\n",
    "        print(\"NLTK WordNet 未找到，正在下载...\")\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')\n",
    "        print(\"✅ NLTK WordNet 下载完成。\")"
   ],
   "id": "18299ff651675a30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到Windows系统，为避免spaCy多进程已知问题，将强制使用单进程(N_PROCESSES=1)。\n",
      "检测到运行环境为: 【LOCAL】\n",
      "\n",
      "--- 环境准备 ---\n",
      "🚢🚢🚢 运行在【完整数据模式】下，将处理所有新闻。🚢🚢🚢\n",
      "固化文本输入: ../data_processed\\china_news_solidified.pkl\n",
      "合并词典输入: ../data_processed\\merge_dict.pkl\n",
      "审核停用词输入: ../data_processed\\new_stopwords.pkl\n",
      "项目停用词输入: ../configs\\project_specific_stopwords.txt\n",
      "最终Tokens输出 (PKL): ../data_processed\\china_news_cleaned_tokens.pkl\n",
      "最终Tokens输出 (CSV): ../data_processed\\china_news_cleaned_tokens_for_review.csv\n",
      "spaCy将使用 1 个进程进行处理。\n",
      "\n",
      "--- 阶段一: 加载数据与规则 ---\n",
      "\n",
      "✅ 成功加载 180630 篇固化文本。\n",
      "✅ 成功加载 1826 条合并规则。\n",
      "\n",
      "正在加载和配置spaCy模型...\n",
      "✅ spaCy模型加载并配置完成，已添加 1507 条特殊分词规则。耗时: 1.23 秒。\n",
      "\n",
      "正在检查NLTK WordNet语料库...\n",
      "✅ NLTK WordNet 已加载。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 2: 集成的、流式深度清洗 (含优雅降级)**\n",
    "\n",
    "**目标:** 将NLP处理和多步骤清洗流程整合到一个单一的、内存高效的流式管道中。\n",
    "\n",
    "**关键优化:**\n",
    "1.  **优雅降级**: 代码会首先**尝试**使用配置的`N_PROCESSES`进行并行处理。如果失败（例如，在非Windows的特殊Jupyter环境中），它将捕获异常并**自动降级**到`n_process=1`的单线程模式重试，确保任务最终能够成功完成。\n",
    "2.  **流式处理**: 整个过程是流式的，从根本上解决了处理完整数据时可能出现的`MemoryError`。"
   ],
   "id": "9dbbfb95ae2ba5e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T16:34:53.864501Z",
     "start_time": "2025-08-07T14:46:45.457669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 2: 集成的、流式深度清洗 (智能连字符处理版) ---\n",
    "# =============================================================================\n",
    "\n",
    "if 'nlp' in locals() and not df.empty:\n",
    "    print(\"\\n--- 阶段二 & 三: 开始集成的、流式深度清洗流程 (终极版) ---\")\n",
    "    start_time_cleaning = time.time()\n",
    "\n",
    "    # --- “终极版”词形还原策略的函数定义 ---\n",
    "    def convert_adv_to_adj_wordnet(adverb: str) -> str | None:\n",
    "        \"\"\"作用: 尝试使用NLTK WordNet的派生关系，将副词转换为其对应的形容词形式。\"\"\"\n",
    "        for syn in wordnet.synsets(adverb, pos=wordnet.ADV):\n",
    "            for lemma in syn.lemmas():\n",
    "                for related_form in lemma.derivationally_related_forms():\n",
    "                    if related_form.synset().pos() == 'a': # 'a' 代表形容词\n",
    "                        return related_form.name().replace('_', '_')\n",
    "        return None\n",
    "\n",
    "    def convert_adv_to_adj_spacy(token: spacy.tokens.Token) -> str | None:\n",
    "        \"\"\"作用: 当WordNet失败时，使用spaCy词汇表进行启发式转换（移除'ly'后缀并校验）。\"\"\"\n",
    "        text_lower = token.text.lower()\n",
    "        if not text_lower.endswith('ly'):\n",
    "            return None\n",
    "        potential_adj = text_lower[:-2]\n",
    "        if potential_adj.endswith('i'):\n",
    "            potential_adj = potential_adj[:-1] + 'y'\n",
    "        if not potential_adj or not potential_adj.isalpha():\n",
    "            return None\n",
    "        if (nlp.vocab.has_vector and nlp.vocab[potential_adj].has_vector) or \\\n",
    "           (not nlp.vocab.has_vector and potential_adj in nlp.vocab):\n",
    "            if token.lemma_ == 'early' and potential_adj == 'ear':\n",
    "                return None\n",
    "            return potential_adj\n",
    "        return None\n",
    "\n",
    "    def ultimate_lemmatizer(token: spacy.tokens.Token) -> str:\n",
    "        \"\"\"作用: 总调度函数。根据词性决定使用何种策略进行词形还原。\"\"\"\n",
    "        final_lemma = token.lemma_.lower()\n",
    "        if token.pos_ == 'ADV':\n",
    "            wn_adj = convert_adv_to_adj_wordnet(token.text)\n",
    "            if wn_adj: return wn_adj\n",
    "            spacy_adj = convert_adv_to_adj_spacy(token)\n",
    "            if spacy_adj: return spacy_adj\n",
    "        return final_lemma\n",
    "\n",
    "    # --- 智能文本规范化函数 ---\n",
    "    def normalize_text_for_spacy(text: str) -> str:\n",
    "        \"\"\"\n",
    "        作用: 在送入spaCy处理前，对文本进行智能的规范化预处理。\n",
    "        1. 保护复合词：仅当连字符前后都是字母时，才将其替换为下划线。\n",
    "        2. 清理分隔符：将连续的多个连字符视作分隔符，替换为空格。\n",
    "        3. 压缩空白符：将多个连续的空白字符统一为一个空格。\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = re.sub(r'(?<=[a-zA-Z])-(?=[a-zA-Z])', '_', text)\n",
    "        text = re.sub(r'-{2,}', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 步骤 1: 构建并保存最终停用词库\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"正在构建最终停用词集...\")\n",
    "    FINAL_STOP_WORDS = spacy.lang.en.stop_words.STOP_WORDS.copy()\n",
    "    print(f\"  - (基础层) 加载 {len(FINAL_STOP_WORDS)} 个spaCy默认停用词。\")\n",
    "    try:\n",
    "        with open(NEW_STOPWORDS_PATH, 'rb') as f:\n",
    "            new_stopwords = pickle.load(f)\n",
    "            FINAL_STOP_WORDS.update(new_stopwords)\n",
    "        print(f\"  - (审核层) 添加了 {len(new_stopwords)} 个来自人工审核的停用词。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - ℹ️ 未找到审核生成的停用词文件: {NEW_STOPWORDS_PATH}，跳过。\")\n",
    "    try:\n",
    "        with open(PROJECT_STOPWORDS_PATH, 'r', encoding='utf-8') as f:\n",
    "            project_stopwords = {line.strip().lower() for line in f if line.strip()}\n",
    "            FINAL_STOP_WORDS.update(project_stopwords)\n",
    "        print(f\"  - (专家层) 添加了 {len(project_stopwords)} 个项目专属停用词。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - ℹ️ 未找到项目专属停用词文件: {PROJECT_STOPWORDS_PATH}。\")\n",
    "    print(f\"✅ 最终停用词集构建完成，共包含 {len(FINAL_STOP_WORDS)} 个不重复的停用词。\")\n",
    "\n",
    "    try:\n",
    "        stopwords_df = pd.DataFrame(sorted(list(FINAL_STOP_WORDS)), columns=['stopword'])\n",
    "        stopwords_output_path = os.path.join(BASE_DATA_PROCESSED_PATH, 'stopwords.csv')\n",
    "        stopwords_df.to_csv(stopwords_output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"✅ 最终停用词列表已保存到: {stopwords_output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 警告: 保存停用词CSV文件失败: {e}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 步骤 2: 定义清洗规则\n",
    "    # ------------------------------------------------------------------------\n",
    "    ALLOWED_POS = {'NOUN', 'PROPN', 'ADJ', 'VERB', 'ADV'}\n",
    "    super_word_values = set(merge_dict.values())\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 步骤 3: 创建并执行流式处理管道\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\n开始流式处理和清洗文本...\")\n",
    "    texts_iterator = (normalize_text_for_spacy(text) for text in df['content_solidified'].astype(str))\n",
    "\n",
    "    disabled_pipes = ['parser', 'ner']\n",
    "    docs_iterator = None\n",
    "\n",
    "    try:\n",
    "        if N_PROCESSES != 1:\n",
    "             print(f\"乐观尝试: 使用 {N_PROCESSES} 个进程进行并行处理...\")\n",
    "        docs_iterator = nlp.pipe(texts_iterator, disable=disabled_pipes, n_process=N_PROCESSES, batch_size=200)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 警告: 并行处理失败，错误信息: {e}\")\n",
    "        print(\"✅ 优雅降级: 自动切换到稳定的单线程模式重试...\")\n",
    "        texts_iterator = (normalize_text_for_spacy(text) for text in df['content_solidified'].astype(str))\n",
    "        docs_iterator = nlp.pipe(texts_iterator, disable=disabled_pipes, n_process=1, batch_size=200)\n",
    "\n",
    "    final_token_lists = []\n",
    "\n",
    "    for doc in tqdm(docs_iterator, total=len(df), desc=\"流式清洗Tokens (终极版)\"):\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            if token.is_punct or token.is_space or token.is_digit:\n",
    "                continue\n",
    "\n",
    "            is_super = token.text in super_word_values\n",
    "\n",
    "            if not is_super and token.pos_ not in ALLOWED_POS:\n",
    "                continue\n",
    "\n",
    "            lemma = ultimate_lemmatizer(token)\n",
    "\n",
    "            if lemma in FINAL_STOP_WORDS:\n",
    "                continue\n",
    "\n",
    "            if len(lemma) < 3:\n",
    "                continue\n",
    "\n",
    "            cleaned_tokens.append(lemma)\n",
    "\n",
    "        final_token_lists.append(cleaned_tokens)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # 步骤 4: 将结果添加回DataFrame并清理内存\n",
    "    # ------------------------------------------------------------------------\n",
    "    df['tokens_for_lda'] = final_token_lists\n",
    "    print(f\"✅ 清洗流程完成！耗时: {(time.time() - start_time_cleaning) / 60:.2f} 分钟。\")\n",
    "\n",
    "    del docs_iterator, texts_iterator, final_token_lists\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"spaCy模型未加载或DataFrame为空，跳过处理。\")"
   ],
   "id": "c11a1ea9e997748a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 阶段二 & 三: 开始集成的、流式深度清洗流程 (终极版) ---\n",
      "正在构建最终停用词集...\n",
      "  - (基础层) 加载 326 个spaCy默认停用词。\n",
      "  - (审核层) 添加了 736 个来自人工审核的停用词。\n",
      "  - (专家层) 添加了 228 个项目专属停用词。\n",
      "✅ 最终停用词集构建完成，共包含 1038 个不重复的停用词。\n",
      "✅ 最终停用词列表已保存到: ../data_processed\\stopwords.csv\n",
      "\n",
      "开始流式处理和清洗文本...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "流式清洗Tokens (终极版):   0%|          | 0/180630 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bd0c0a5fbd54de7b0fc01a92a8ddc03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 清洗流程完成！耗时: 108.12 分钟。\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 3: 最终检查与保存**\n",
    "\n",
    "**目标:** 抽样检查清洗效果，并保存最终产物。这是我们**深度清洗文本**宏观阶段的最终交付。"
   ],
   "id": "87848316f94ea172"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T16:35:26.805747Z",
     "start_time": "2025-08-07T16:34:53.892570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 3: 最终检查与保存 ---\n",
    "# =============================================================================\n",
    "\n",
    "# 作用: 负责对清洗后的结果进行抽样检查，并保存为最终的Pickle和CSV文件。\n",
    "if 'df' in locals() and 'tokens_for_lda' in df.columns:\n",
    "    print(\"\\n--- 阶段四: 最终检查与保存 ---\")\n",
    "\n",
    "    # 作用: 随机抽取5篇文章，打印其固化后文本和最终token列表，用于人工快速验证清洗效果。\n",
    "    print(\"\\n--- 抽样检查结果 ---\")\n",
    "    sample_size = min(5, len(df))\n",
    "    if sample_size > 0:\n",
    "        pd.set_option('display.max_colwidth', 300)\n",
    "        display(df.sample(sample_size)[['content_solidified', 'tokens_for_lda']])\n",
    "\n",
    "    # 作用: 将最终结果保存为两种格式。\n",
    "    try:\n",
    "        # Pickle格式: 用于后续Python脚本高效读取，保留数据类型。\n",
    "        df.to_pickle(CLEANED_TOKENS_PKL_PATH)\n",
    "        print(f\"\\n✅ [机器用] 包含最终Tokens的DataFrame已保存到Pickle: {CLEANED_TOKENS_PKL_PATH}\")\n",
    "\n",
    "        # CSV格式: 用于人工查阅和审计，token列表会转换为空格分隔的字符串。\n",
    "        df_for_csv = df.copy()\n",
    "        df_for_csv['tokens_for_lda'] = df_for_csv['tokens_for_lda'].apply(lambda tokens: ' '.join(tokens))\n",
    "        df_for_csv.to_csv(CLEANED_TOKENS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "        print(f\"✅ [人类用] 包含最终Tokens的DataFrame已保存到CSV: {CLEANED_TOKENS_CSV_PATH}\")\n",
    "\n",
    "        print(f\"\\n🎉🎉🎉 深度文本清洗流程全部完成！ 🎉🎉🎉\")\n",
    "        print(f\"\\n下一步是运行 '04_Topic_Modeling.ipynb' 或 '04_sLDA_Modeling.ipynb'，开始主题建模分析。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 保存文件时发生错误: {e}\")\n",
    "else:\n",
    "    print(\"没有可供保存的最终Tokens数据。\")"
   ],
   "id": "ec4f4e4650b3e7c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 阶段四: 最终检查与保存 ---\n",
      "\n",
      "--- 抽样检查结果 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                 content_solidified  \\\n",
       "104063  The Southwest Border Is Open for Business . Over the last few weeks, mayors, sheriffs, business leaders and citizens have joined together with a simple but powerful message: America's Southwest border communities are open for business. This is a message the American people need to hear. Unfortun...   \n",
       "41544   justice_department Examines CIA Role In Probe Into Hughes's China Dealings . WASHINGTON -- The justice_department is examining whether the central_intelligence_agency impeded an investigation of Hughes Electronics Corp.'s business dealings in China. Investigators are trying to determine whether ...   \n",
       "5842    Film: The Mousekewitz Migration . I wouldn't say that walt_disney warped me exactly; however, on a recent trip to Italy I bought a set of ceramic bowls because the deer painted on them reminded me of Bambi. I still have small yellow records with the sound track from Cinderella tucked away in a c...   \n",
       "101770  Why We're Always Fooled by north_korea . According to Siegfried Hecker, the former director of the Los Alamos National Laboratory, north_korea is working on two new nuclear facilities, a light water power reactor in early stages of construction, and a \"modern, clean centrifuge plant\" for uranium...   \n",
       "116722  Big hong_kong Investors See Time to Sell --- Some of the Wealthiest Families Believe the Red-Hot Real-Estate Market Is Coming to an End; 'Selling at the Very Top' . hong_kong -- With the government growing confident that it has halted the meteoric rise in property prices, some of this city's big...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     tokens_for_lda  \n",
       "104063  [southwest, border, open, business, mayor, sheriff, business, leader, citizen, join, simple, powerful, message, america, southwest, border, open, business, message, american, hear, unfortunate, widespread, misperception, southwest, wrack, violence, spill, mexico, ongoing, drug, war, different, a...  \n",
       "41544   [justice_department, examine, cia, probe, hughes, china, washington, justice_department, examine, central_intelligence_agency, impede, investigation, hughes, electronics, corp., business, china, investigator, cia, official, act, improper, alert, hughes, electronics, employee, intelligence, commi...  \n",
       "5842    [film, mousekewitz, migration, walt_disney, warp, exact, trip, italy, buy, ceramic, bowl, deer, paint, remind, bambi, yellow, record, sound, track, cinderella, tuck, closet, simple, draw, childhood, mythology, disney, animated, teach, dream, evil, anxiety, especial, realize, coat, syrup, safe, b...  \n",
       "101770  [fool, north_korea, accord, siegfried, hecker, los, alamos, national, laboratory, north_korea, work, nuclear, facility, light, water, power, reactor, stage, modern, clean, centrifuge, plant, uranium, enrichment, mr., hecker, visit, facility, weekend, appear, near, complete, centrifuge, plant, pa...  \n",
       "116722  [hong_kong, investor, sell, wealthiest, families, red_hot, real_estate, market, end, sell, hong_kong, government, grow, confident, halt, meteoric, rise, property, price, city, real_estate, investor, hong_kong, wealthy, initial, public, offering, hotel, office, real_estate, asset, coming, price, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_solidified</th>\n",
       "      <th>tokens_for_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104063</th>\n",
       "      <td>The Southwest Border Is Open for Business . Over the last few weeks, mayors, sheriffs, business leaders and citizens have joined together with a simple but powerful message: America's Southwest border communities are open for business. This is a message the American people need to hear. Unfortun...</td>\n",
       "      <td>[southwest, border, open, business, mayor, sheriff, business, leader, citizen, join, simple, powerful, message, america, southwest, border, open, business, message, american, hear, unfortunate, widespread, misperception, southwest, wrack, violence, spill, mexico, ongoing, drug, war, different, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41544</th>\n",
       "      <td>justice_department Examines CIA Role In Probe Into Hughes's China Dealings . WASHINGTON -- The justice_department is examining whether the central_intelligence_agency impeded an investigation of Hughes Electronics Corp.'s business dealings in China. Investigators are trying to determine whether ...</td>\n",
       "      <td>[justice_department, examine, cia, probe, hughes, china, washington, justice_department, examine, central_intelligence_agency, impede, investigation, hughes, electronics, corp., business, china, investigator, cia, official, act, improper, alert, hughes, electronics, employee, intelligence, commi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5842</th>\n",
       "      <td>Film: The Mousekewitz Migration . I wouldn't say that walt_disney warped me exactly; however, on a recent trip to Italy I bought a set of ceramic bowls because the deer painted on them reminded me of Bambi. I still have small yellow records with the sound track from Cinderella tucked away in a c...</td>\n",
       "      <td>[film, mousekewitz, migration, walt_disney, warp, exact, trip, italy, buy, ceramic, bowl, deer, paint, remind, bambi, yellow, record, sound, track, cinderella, tuck, closet, simple, draw, childhood, mythology, disney, animated, teach, dream, evil, anxiety, especial, realize, coat, syrup, safe, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101770</th>\n",
       "      <td>Why We're Always Fooled by north_korea . According to Siegfried Hecker, the former director of the Los Alamos National Laboratory, north_korea is working on two new nuclear facilities, a light water power reactor in early stages of construction, and a \"modern, clean centrifuge plant\" for uranium...</td>\n",
       "      <td>[fool, north_korea, accord, siegfried, hecker, los, alamos, national, laboratory, north_korea, work, nuclear, facility, light, water, power, reactor, stage, modern, clean, centrifuge, plant, uranium, enrichment, mr., hecker, visit, facility, weekend, appear, near, complete, centrifuge, plant, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116722</th>\n",
       "      <td>Big hong_kong Investors See Time to Sell --- Some of the Wealthiest Families Believe the Red-Hot Real-Estate Market Is Coming to an End; 'Selling at the Very Top' . hong_kong -- With the government growing confident that it has halted the meteoric rise in property prices, some of this city's big...</td>\n",
       "      <td>[hong_kong, investor, sell, wealthiest, families, red_hot, real_estate, market, end, sell, hong_kong, government, grow, confident, halt, meteoric, rise, property, price, city, real_estate, investor, hong_kong, wealthy, initial, public, offering, hotel, office, real_estate, asset, coming, price, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ [机器用] 包含最终Tokens的DataFrame已保存到Pickle: ../data_processed\\china_news_cleaned_tokens.pkl\n",
      "✅ [人类用] 包含最终Tokens的DataFrame已保存到CSV: ../data_processed\\china_news_cleaned_tokens_for_review.csv\n",
      "\n",
      "🎉🎉🎉 深度文本清洗流程全部完成！ 🎉🎉🎉\n",
      "\n",
      "下一步是运行 '04_Topic_Modeling.ipynb' 或 '04_sLDA_Modeling.ipynb'，开始主题建模分析。\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
