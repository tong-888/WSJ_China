{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 1: ç¯å¢ƒè®¾ç½®ã€æ•°æ®åŠ è½½ä¸spaCyé…ç½®**\n",
    "\n",
    "**ç›®æ ‡:** å¯¼å…¥æ‰€æœ‰å¿…éœ€çš„åº“ï¼ŒåŠ è½½æ ¸å¿ƒæ•°æ®å’Œè§„åˆ™æ–‡ä»¶ï¼Œå¹¶åˆ©ç”¨`spaCy`çš„å¼ºå¤§åŠŸèƒ½ä¸ºåç»­å¤„ç†åšå¥½å‡†å¤‡ã€‚\n",
    "\n",
    "**å…³é”®æ“ä½œ:** éå†æˆ‘ä»¬ `merge_dict.pkl` ä¸­çš„æ‰€æœ‰â€œè¶…çº§è¯â€ï¼ˆå¦‚ `peoples_bank_of_china`ï¼‰ï¼Œå°†å®ƒä»¬ä½œä¸º**ç‰¹æ®Šè§„åˆ™**æ·»åŠ åˆ°`spaCy`çš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰ä¸­ï¼Œä»¥ç¡®ä¿`spaCy`åœ¨å¤„ç†æ–‡æœ¬æ—¶ï¼Œ**ç»ä¸ä¼š**å°†è¿™äº›è¶…çº§è¯é”™è¯¯åœ°æ‹†åˆ†å¼€ã€‚"
   ],
   "id": "81329b116c9ab35a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:46:45.287796Z",
     "start_time": "2025-08-07T14:46:38.789711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 1: ç¯å¢ƒè®¾ç½®ã€æ•°æ®åŠ è½½ä¸spaCyé…ç½® (å‡çº§ç‰ˆ) ---\n",
    "# =============================================================================\n",
    "\n",
    "# ä½œç”¨: å¯¼å…¥æ‰€æœ‰é¡¹ç›®è¿è¡Œæ‰€éœ€çš„Pythonåº“ã€‚\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.attrs import ORTH\n",
    "import gc\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import re  # [æ–°å¢] å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼åº“\n",
    "\n",
    "# --- æ ¸å¿ƒé…ç½®åŒº ---\n",
    "# ä½œç”¨: å…¨å±€æ§åˆ¶å‚æ•°ï¼Œæ–¹ä¾¿è°ƒè¯•ä¸åˆ‡æ¢è¿è¡Œæ¨¡å¼ã€‚\n",
    "RUNNING_ENV = 'local'\n",
    "TEST_MODE = False\n",
    "TEST_SAMPLE_SIZE = 1000\n",
    "\n",
    "# --- å¹¶è¡Œå¤„ç†é…ç½® ---\n",
    "# ä½œç”¨: æ™ºèƒ½æ£€æµ‹CPUæ ¸å¿ƒæ•°ï¼Œå¹¶ä¸ºå¤šè¿›ç¨‹å¤„ç†è®¾å®šåˆç†çš„è¿›ç¨‹æ•°ã€‚\n",
    "cpu_cores = psutil.cpu_count(logical=False)\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8)\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "\n",
    "# ä½œç”¨: ä¸»åŠ¨æ£€æµ‹Windowsæ“ä½œç³»ç»Ÿï¼Œå¹¶å¼ºåˆ¶ä½¿ç”¨å•è¿›ç¨‹ï¼Œä»¥è§„é¿spaCyåœ¨Windowsä¸Šçš„å·²çŸ¥å¤šè¿›ç¨‹é—®é¢˜ã€‚\n",
    "if os.name == 'nt':\n",
    "    print(\"æ£€æµ‹åˆ°Windowsç³»ç»Ÿï¼Œä¸ºé¿å…spaCyå¤šè¿›ç¨‹å·²çŸ¥é—®é¢˜ï¼Œå°†å¼ºåˆ¶ä½¿ç”¨å•è¿›ç¨‹(N_PROCESSES=1)ã€‚\")\n",
    "    N_PROCESSES = 1\n",
    "\n",
    "# --- è·¯å¾„æ™ºèƒ½ç®¡ç† ---\n",
    "# ä½œç”¨: æ ¹æ®è¿è¡Œç¯å¢ƒï¼ˆæœ¬åœ°æˆ–æœåŠ¡å™¨ï¼‰è‡ªåŠ¨æ„å»ºæ­£ç¡®çš„æ–‡ä»¶è·¯å¾„ã€‚\n",
    "print(f\"æ£€æµ‹åˆ°è¿è¡Œç¯å¢ƒä¸º: ã€{RUNNING_ENV.upper()}ã€‘\")\n",
    "BASE_DATA_PROCESSED_PATH = '../data_processed' if RUNNING_ENV == 'local' else '/mnt/data/data_processed'\n",
    "BASE_CONFIG_PATH = '../configs' if RUNNING_ENV == 'local' else '/mnt/data/configs'\n",
    "\n",
    "# ä½œç”¨: ç¡®ä¿é…ç½®æ–‡ä»¶ç›®å½•å­˜åœ¨ï¼Œé˜²æ­¢åç»­å†™å…¥æ–‡ä»¶æ—¶å‡ºé”™ã€‚\n",
    "if not os.path.exists(BASE_CONFIG_PATH):\n",
    "    os.makedirs(BASE_CONFIG_PATH)\n",
    "    print(f\"åˆ›å»ºäº†configsç›®å½•: {BASE_CONFIG_PATH}\")\n",
    "\n",
    "# ä½œç”¨: å®šä¹‰æ‰€æœ‰è¾“å…¥è¾“å‡ºæ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚\n",
    "# è¾“å…¥æ–‡ä»¶è·¯å¾„\n",
    "SOLIDIFIED_TEXT_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_solidified.pkl')\n",
    "MERGE_DICT_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'merge_dict.pkl')\n",
    "NEW_STOPWORDS_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'new_stopwords.pkl')\n",
    "PROJECT_STOPWORDS_PATH = os.path.join(BASE_CONFIG_PATH, 'project_specific_stopwords.txt')\n",
    "# è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "CLEANED_TOKENS_PKL_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_cleaned_tokens.pkl')\n",
    "CLEANED_TOKENS_CSV_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_cleaned_tokens_for_review.csv')\n",
    "\n",
    "# --- æ‰“å°æœ€ç»ˆé…ç½®ä¿¡æ¯ ---\n",
    "# ä½œç”¨: åœ¨ç¨‹åºå¼€å§‹æ—¶æ¸…æ™°åœ°å±•ç¤ºæ‰€æœ‰é…ç½®ï¼Œä¾¿äºæ£€æŸ¥å’Œè¿½æº¯ã€‚\n",
    "print(\"\\n--- ç¯å¢ƒå‡†å¤‡ ---\")\n",
    "if TEST_MODE:\n",
    "    print(f\"ğŸš€ğŸš€ğŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†å‰ {TEST_SAMPLE_SIZE} è¡Œæ–°é—»ï¼ğŸš€ğŸš€ğŸš€\")\n",
    "else:\n",
    "    print(f\"ğŸš¢ğŸš¢ğŸš¢ è¿è¡Œåœ¨ã€å®Œæ•´æ•°æ®æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†æ‰€æœ‰æ–°é—»ã€‚ğŸš¢ğŸš¢ğŸš¢\")\n",
    "print(f\"å›ºåŒ–æ–‡æœ¬è¾“å…¥: {SOLIDIFIED_TEXT_PATH}\")\n",
    "print(f\"åˆå¹¶è¯å…¸è¾“å…¥: {MERGE_DICT_PATH}\")\n",
    "print(f\"å®¡æ ¸åœç”¨è¯è¾“å…¥: {NEW_STOPWORDS_PATH}\")\n",
    "print(f\"é¡¹ç›®åœç”¨è¯è¾“å…¥: {PROJECT_STOPWORDS_PATH}\")\n",
    "print(f\"æœ€ç»ˆTokensè¾“å‡º (PKL): {CLEANED_TOKENS_PKL_PATH}\")\n",
    "print(f\"æœ€ç»ˆTokensè¾“å‡º (CSV): {CLEANED_TOKENS_CSV_PATH}\")\n",
    "print(f\"spaCyå°†ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹è¿›è¡Œå¤„ç†ã€‚\")\n",
    "\n",
    "\n",
    "# --- åŠ è½½æ•°æ®å’Œè§„åˆ™ ---\n",
    "# ä½œç”¨: ä»ç£ç›˜è¯»å–ç»è¿‡å®ä½“å›ºåŒ–çš„æ–°é—»æ•°æ®å’Œåˆå¹¶è§„åˆ™å­—å…¸ã€‚\n",
    "print(\"\\n--- é˜¶æ®µä¸€: åŠ è½½æ•°æ®ä¸è§„åˆ™ ---\\n\")\n",
    "try:\n",
    "    read_nrows = TEST_SAMPLE_SIZE if TEST_MODE else None\n",
    "    df = pd.read_pickle(SOLIDIFIED_TEXT_PATH)\n",
    "    if read_nrows:\n",
    "        df = df.head(read_nrows)\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(df)} ç¯‡å›ºåŒ–æ–‡æœ¬ã€‚\")\n",
    "\n",
    "    with open(MERGE_DICT_PATH, 'rb') as f:\n",
    "        merge_dict = pickle.load(f)\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(merge_dict)} æ¡åˆå¹¶è§„åˆ™ã€‚\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„è¾“å…¥æ–‡ä»¶: {e.filename}ã€‚è¯·ç¡®ä¿å·²æˆåŠŸè¿è¡Œä¹‹å‰çš„Notebookã€‚\")\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# --- åŠ è½½å’Œé…ç½®spaCyåŠNLTKæ¨¡å‹ ---\n",
    "# ä½œç”¨: åŠ è½½spaCyæ¨¡å‹å¹¶ä¸ºå…¶åˆ†è¯å™¨æ·»åŠ ç‰¹æ®Šè§„åˆ™ï¼Œä»¥ä¿æŠ¤å›ºåŒ–å®ä½“ä¸è¢«æ‹†åˆ†ã€‚åŒæ—¶æ£€æŸ¥å¹¶ä¸‹è½½NLTK WordNetè¯­æ–™åº“ã€‚\n",
    "if not df.empty:\n",
    "    print(\"\\næ­£åœ¨åŠ è½½å’Œé…ç½®spaCyæ¨¡å‹...\")\n",
    "    start_time_spacy = time.time()\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "    special_cases = {}\n",
    "    for standard_form in set(merge_dict.values()):\n",
    "        special_cases[standard_form] = [{ORTH: standard_form}]\n",
    "\n",
    "    for case, rule in special_cases.items():\n",
    "        nlp.tokenizer.add_special_case(case, rule)\n",
    "\n",
    "    print(f\"âœ… spaCyæ¨¡å‹åŠ è½½å¹¶é…ç½®å®Œæˆï¼Œå·²æ·»åŠ  {len(special_cases)} æ¡ç‰¹æ®Šåˆ†è¯è§„åˆ™ã€‚è€—æ—¶: {time.time() - start_time_spacy:.2f} ç§’ã€‚\")\n",
    "\n",
    "    print(\"\\næ­£åœ¨æ£€æŸ¥NLTK WordNetè¯­æ–™åº“...\")\n",
    "    try:\n",
    "        wordnet.ensure_loaded()\n",
    "        print(\"âœ… NLTK WordNet å·²åŠ è½½ã€‚\")\n",
    "    except LookupError:\n",
    "        print(\"NLTK WordNet æœªæ‰¾åˆ°ï¼Œæ­£åœ¨ä¸‹è½½...\")\n",
    "        nltk.download('wordnet')\n",
    "        nltk.download('omw-1.4')\n",
    "        print(\"âœ… NLTK WordNet ä¸‹è½½å®Œæˆã€‚\")"
   ],
   "id": "18299ff651675a30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æµ‹åˆ°Windowsç³»ç»Ÿï¼Œä¸ºé¿å…spaCyå¤šè¿›ç¨‹å·²çŸ¥é—®é¢˜ï¼Œå°†å¼ºåˆ¶ä½¿ç”¨å•è¿›ç¨‹(N_PROCESSES=1)ã€‚\n",
      "æ£€æµ‹åˆ°è¿è¡Œç¯å¢ƒä¸º: ã€LOCALã€‘\n",
      "\n",
      "--- ç¯å¢ƒå‡†å¤‡ ---\n",
      "ğŸš¢ğŸš¢ğŸš¢ è¿è¡Œåœ¨ã€å®Œæ•´æ•°æ®æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†æ‰€æœ‰æ–°é—»ã€‚ğŸš¢ğŸš¢ğŸš¢\n",
      "å›ºåŒ–æ–‡æœ¬è¾“å…¥: ../data_processed\\china_news_solidified.pkl\n",
      "åˆå¹¶è¯å…¸è¾“å…¥: ../data_processed\\merge_dict.pkl\n",
      "å®¡æ ¸åœç”¨è¯è¾“å…¥: ../data_processed\\new_stopwords.pkl\n",
      "é¡¹ç›®åœç”¨è¯è¾“å…¥: ../configs\\project_specific_stopwords.txt\n",
      "æœ€ç»ˆTokensè¾“å‡º (PKL): ../data_processed\\china_news_cleaned_tokens.pkl\n",
      "æœ€ç»ˆTokensè¾“å‡º (CSV): ../data_processed\\china_news_cleaned_tokens_for_review.csv\n",
      "spaCyå°†ä½¿ç”¨ 1 ä¸ªè¿›ç¨‹è¿›è¡Œå¤„ç†ã€‚\n",
      "\n",
      "--- é˜¶æ®µä¸€: åŠ è½½æ•°æ®ä¸è§„åˆ™ ---\n",
      "\n",
      "âœ… æˆåŠŸåŠ è½½ 180630 ç¯‡å›ºåŒ–æ–‡æœ¬ã€‚\n",
      "âœ… æˆåŠŸåŠ è½½ 1826 æ¡åˆå¹¶è§„åˆ™ã€‚\n",
      "\n",
      "æ­£åœ¨åŠ è½½å’Œé…ç½®spaCyæ¨¡å‹...\n",
      "âœ… spaCyæ¨¡å‹åŠ è½½å¹¶é…ç½®å®Œæˆï¼Œå·²æ·»åŠ  1507 æ¡ç‰¹æ®Šåˆ†è¯è§„åˆ™ã€‚è€—æ—¶: 1.23 ç§’ã€‚\n",
      "\n",
      "æ­£åœ¨æ£€æŸ¥NLTK WordNetè¯­æ–™åº“...\n",
      "âœ… NLTK WordNet å·²åŠ è½½ã€‚\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 2: é›†æˆçš„ã€æµå¼æ·±åº¦æ¸…æ´— (å«ä¼˜é›…é™çº§)**\n",
    "\n",
    "**ç›®æ ‡:** å°†NLPå¤„ç†å’Œå¤šæ­¥éª¤æ¸…æ´—æµç¨‹æ•´åˆåˆ°ä¸€ä¸ªå•ä¸€çš„ã€å†…å­˜é«˜æ•ˆçš„æµå¼ç®¡é“ä¸­ã€‚\n",
    "\n",
    "**å…³é”®ä¼˜åŒ–:**\n",
    "1.  **ä¼˜é›…é™çº§**: ä»£ç ä¼šé¦–å…ˆ**å°è¯•**ä½¿ç”¨é…ç½®çš„`N_PROCESSES`è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚å¦‚æœå¤±è´¥ï¼ˆä¾‹å¦‚ï¼Œåœ¨éWindowsçš„ç‰¹æ®ŠJupyterç¯å¢ƒä¸­ï¼‰ï¼Œå®ƒå°†æ•è·å¼‚å¸¸å¹¶**è‡ªåŠ¨é™çº§**åˆ°`n_process=1`çš„å•çº¿ç¨‹æ¨¡å¼é‡è¯•ï¼Œç¡®ä¿ä»»åŠ¡æœ€ç»ˆèƒ½å¤ŸæˆåŠŸå®Œæˆã€‚\n",
    "2.  **æµå¼å¤„ç†**: æ•´ä¸ªè¿‡ç¨‹æ˜¯æµå¼çš„ï¼Œä»æ ¹æœ¬ä¸Šè§£å†³äº†å¤„ç†å®Œæ•´æ•°æ®æ—¶å¯èƒ½å‡ºç°çš„`MemoryError`ã€‚"
   ],
   "id": "9dbbfb95ae2ba5e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T16:34:53.864501Z",
     "start_time": "2025-08-07T14:46:45.457669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 2: é›†æˆçš„ã€æµå¼æ·±åº¦æ¸…æ´— (æ™ºèƒ½è¿å­—ç¬¦å¤„ç†ç‰ˆ) ---\n",
    "# =============================================================================\n",
    "\n",
    "if 'nlp' in locals() and not df.empty:\n",
    "    print(\"\\n--- é˜¶æ®µäºŒ & ä¸‰: å¼€å§‹é›†æˆçš„ã€æµå¼æ·±åº¦æ¸…æ´—æµç¨‹ (ç»ˆæç‰ˆ) ---\")\n",
    "    start_time_cleaning = time.time()\n",
    "\n",
    "    # --- â€œç»ˆæç‰ˆâ€è¯å½¢è¿˜åŸç­–ç•¥çš„å‡½æ•°å®šä¹‰ ---\n",
    "    def convert_adv_to_adj_wordnet(adverb: str) -> str | None:\n",
    "        \"\"\"ä½œç”¨: å°è¯•ä½¿ç”¨NLTK WordNetçš„æ´¾ç”Ÿå…³ç³»ï¼Œå°†å‰¯è¯è½¬æ¢ä¸ºå…¶å¯¹åº”çš„å½¢å®¹è¯å½¢å¼ã€‚\"\"\"\n",
    "        for syn in wordnet.synsets(adverb, pos=wordnet.ADV):\n",
    "            for lemma in syn.lemmas():\n",
    "                for related_form in lemma.derivationally_related_forms():\n",
    "                    if related_form.synset().pos() == 'a': # 'a' ä»£è¡¨å½¢å®¹è¯\n",
    "                        return related_form.name().replace('_', '_')\n",
    "        return None\n",
    "\n",
    "    def convert_adv_to_adj_spacy(token: spacy.tokens.Token) -> str | None:\n",
    "        \"\"\"ä½œç”¨: å½“WordNetå¤±è´¥æ—¶ï¼Œä½¿ç”¨spaCyè¯æ±‡è¡¨è¿›è¡Œå¯å‘å¼è½¬æ¢ï¼ˆç§»é™¤'ly'åç¼€å¹¶æ ¡éªŒï¼‰ã€‚\"\"\"\n",
    "        text_lower = token.text.lower()\n",
    "        if not text_lower.endswith('ly'):\n",
    "            return None\n",
    "        potential_adj = text_lower[:-2]\n",
    "        if potential_adj.endswith('i'):\n",
    "            potential_adj = potential_adj[:-1] + 'y'\n",
    "        if not potential_adj or not potential_adj.isalpha():\n",
    "            return None\n",
    "        if (nlp.vocab.has_vector and nlp.vocab[potential_adj].has_vector) or \\\n",
    "           (not nlp.vocab.has_vector and potential_adj in nlp.vocab):\n",
    "            if token.lemma_ == 'early' and potential_adj == 'ear':\n",
    "                return None\n",
    "            return potential_adj\n",
    "        return None\n",
    "\n",
    "    def ultimate_lemmatizer(token: spacy.tokens.Token) -> str:\n",
    "        \"\"\"ä½œç”¨: æ€»è°ƒåº¦å‡½æ•°ã€‚æ ¹æ®è¯æ€§å†³å®šä½¿ç”¨ä½•ç§ç­–ç•¥è¿›è¡Œè¯å½¢è¿˜åŸã€‚\"\"\"\n",
    "        final_lemma = token.lemma_.lower()\n",
    "        if token.pos_ == 'ADV':\n",
    "            wn_adj = convert_adv_to_adj_wordnet(token.text)\n",
    "            if wn_adj: return wn_adj\n",
    "            spacy_adj = convert_adv_to_adj_spacy(token)\n",
    "            if spacy_adj: return spacy_adj\n",
    "        return final_lemma\n",
    "\n",
    "    # --- æ™ºèƒ½æ–‡æœ¬è§„èŒƒåŒ–å‡½æ•° ---\n",
    "    def normalize_text_for_spacy(text: str) -> str:\n",
    "        \"\"\"\n",
    "        ä½œç”¨: åœ¨é€å…¥spaCyå¤„ç†å‰ï¼Œå¯¹æ–‡æœ¬è¿›è¡Œæ™ºèƒ½çš„è§„èŒƒåŒ–é¢„å¤„ç†ã€‚\n",
    "        1. ä¿æŠ¤å¤åˆè¯ï¼šä»…å½“è¿å­—ç¬¦å‰åéƒ½æ˜¯å­—æ¯æ—¶ï¼Œæ‰å°†å…¶æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ã€‚\n",
    "        2. æ¸…ç†åˆ†éš”ç¬¦ï¼šå°†è¿ç»­çš„å¤šä¸ªè¿å­—ç¬¦è§†ä½œåˆ†éš”ç¬¦ï¼Œæ›¿æ¢ä¸ºç©ºæ ¼ã€‚\n",
    "        3. å‹ç¼©ç©ºç™½ç¬¦ï¼šå°†å¤šä¸ªè¿ç»­çš„ç©ºç™½å­—ç¬¦ç»Ÿä¸€ä¸ºä¸€ä¸ªç©ºæ ¼ã€‚\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = re.sub(r'(?<=[a-zA-Z])-(?=[a-zA-Z])', '_', text)\n",
    "        text = re.sub(r'-{2,}', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # æ­¥éª¤ 1: æ„å»ºå¹¶ä¿å­˜æœ€ç»ˆåœç”¨è¯åº“\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"æ­£åœ¨æ„å»ºæœ€ç»ˆåœç”¨è¯é›†...\")\n",
    "    FINAL_STOP_WORDS = spacy.lang.en.stop_words.STOP_WORDS.copy()\n",
    "    print(f\"  - (åŸºç¡€å±‚) åŠ è½½ {len(FINAL_STOP_WORDS)} ä¸ªspaCyé»˜è®¤åœç”¨è¯ã€‚\")\n",
    "    try:\n",
    "        with open(NEW_STOPWORDS_PATH, 'rb') as f:\n",
    "            new_stopwords = pickle.load(f)\n",
    "            FINAL_STOP_WORDS.update(new_stopwords)\n",
    "        print(f\"  - (å®¡æ ¸å±‚) æ·»åŠ äº† {len(new_stopwords)} ä¸ªæ¥è‡ªäººå·¥å®¡æ ¸çš„åœç”¨è¯ã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - â„¹ï¸ æœªæ‰¾åˆ°å®¡æ ¸ç”Ÿæˆçš„åœç”¨è¯æ–‡ä»¶: {NEW_STOPWORDS_PATH}ï¼Œè·³è¿‡ã€‚\")\n",
    "    try:\n",
    "        with open(PROJECT_STOPWORDS_PATH, 'r', encoding='utf-8') as f:\n",
    "            project_stopwords = {line.strip().lower() for line in f if line.strip()}\n",
    "            FINAL_STOP_WORDS.update(project_stopwords)\n",
    "        print(f\"  - (ä¸“å®¶å±‚) æ·»åŠ äº† {len(project_stopwords)} ä¸ªé¡¹ç›®ä¸“å±åœç”¨è¯ã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  - â„¹ï¸ æœªæ‰¾åˆ°é¡¹ç›®ä¸“å±åœç”¨è¯æ–‡ä»¶: {PROJECT_STOPWORDS_PATH}ã€‚\")\n",
    "    print(f\"âœ… æœ€ç»ˆåœç”¨è¯é›†æ„å»ºå®Œæˆï¼Œå…±åŒ…å« {len(FINAL_STOP_WORDS)} ä¸ªä¸é‡å¤çš„åœç”¨è¯ã€‚\")\n",
    "\n",
    "    try:\n",
    "        stopwords_df = pd.DataFrame(sorted(list(FINAL_STOP_WORDS)), columns=['stopword'])\n",
    "        stopwords_output_path = os.path.join(BASE_DATA_PROCESSED_PATH, 'stopwords.csv')\n",
    "        stopwords_df.to_csv(stopwords_output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"âœ… æœ€ç»ˆåœç”¨è¯åˆ—è¡¨å·²ä¿å­˜åˆ°: {stopwords_output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è­¦å‘Š: ä¿å­˜åœç”¨è¯CSVæ–‡ä»¶å¤±è´¥: {e}\")\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # æ­¥éª¤ 2: å®šä¹‰æ¸…æ´—è§„åˆ™\n",
    "    # ------------------------------------------------------------------------\n",
    "    ALLOWED_POS = {'NOUN', 'PROPN', 'ADJ', 'VERB', 'ADV'}\n",
    "    super_word_values = set(merge_dict.values())\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # æ­¥éª¤ 3: åˆ›å»ºå¹¶æ‰§è¡Œæµå¼å¤„ç†ç®¡é“\n",
    "    # ------------------------------------------------------------------------\n",
    "    print(\"\\nå¼€å§‹æµå¼å¤„ç†å’Œæ¸…æ´—æ–‡æœ¬...\")\n",
    "    texts_iterator = (normalize_text_for_spacy(text) for text in df['content_solidified'].astype(str))\n",
    "\n",
    "    disabled_pipes = ['parser', 'ner']\n",
    "    docs_iterator = None\n",
    "\n",
    "    try:\n",
    "        if N_PROCESSES != 1:\n",
    "             print(f\"ä¹è§‚å°è¯•: ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†...\")\n",
    "        docs_iterator = nlp.pipe(texts_iterator, disable=disabled_pipes, n_process=N_PROCESSES, batch_size=200)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ è­¦å‘Š: å¹¶è¡Œå¤„ç†å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {e}\")\n",
    "        print(\"âœ… ä¼˜é›…é™çº§: è‡ªåŠ¨åˆ‡æ¢åˆ°ç¨³å®šçš„å•çº¿ç¨‹æ¨¡å¼é‡è¯•...\")\n",
    "        texts_iterator = (normalize_text_for_spacy(text) for text in df['content_solidified'].astype(str))\n",
    "        docs_iterator = nlp.pipe(texts_iterator, disable=disabled_pipes, n_process=1, batch_size=200)\n",
    "\n",
    "    final_token_lists = []\n",
    "\n",
    "    for doc in tqdm(docs_iterator, total=len(df), desc=\"æµå¼æ¸…æ´—Tokens (ç»ˆæç‰ˆ)\"):\n",
    "        cleaned_tokens = []\n",
    "        for token in doc:\n",
    "            if token.is_punct or token.is_space or token.is_digit:\n",
    "                continue\n",
    "\n",
    "            is_super = token.text in super_word_values\n",
    "\n",
    "            if not is_super and token.pos_ not in ALLOWED_POS:\n",
    "                continue\n",
    "\n",
    "            lemma = ultimate_lemmatizer(token)\n",
    "\n",
    "            if lemma in FINAL_STOP_WORDS:\n",
    "                continue\n",
    "\n",
    "            if len(lemma) < 3:\n",
    "                continue\n",
    "\n",
    "            cleaned_tokens.append(lemma)\n",
    "\n",
    "        final_token_lists.append(cleaned_tokens)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # æ­¥éª¤ 4: å°†ç»“æœæ·»åŠ å›DataFrameå¹¶æ¸…ç†å†…å­˜\n",
    "    # ------------------------------------------------------------------------\n",
    "    df['tokens_for_lda'] = final_token_lists\n",
    "    print(f\"âœ… æ¸…æ´—æµç¨‹å®Œæˆï¼è€—æ—¶: {(time.time() - start_time_cleaning) / 60:.2f} åˆ†é’Ÿã€‚\")\n",
    "\n",
    "    del docs_iterator, texts_iterator, final_token_lists\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(\"spaCyæ¨¡å‹æœªåŠ è½½æˆ–DataFrameä¸ºç©ºï¼Œè·³è¿‡å¤„ç†ã€‚\")"
   ],
   "id": "c11a1ea9e997748a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- é˜¶æ®µäºŒ & ä¸‰: å¼€å§‹é›†æˆçš„ã€æµå¼æ·±åº¦æ¸…æ´—æµç¨‹ (ç»ˆæç‰ˆ) ---\n",
      "æ­£åœ¨æ„å»ºæœ€ç»ˆåœç”¨è¯é›†...\n",
      "  - (åŸºç¡€å±‚) åŠ è½½ 326 ä¸ªspaCyé»˜è®¤åœç”¨è¯ã€‚\n",
      "  - (å®¡æ ¸å±‚) æ·»åŠ äº† 736 ä¸ªæ¥è‡ªäººå·¥å®¡æ ¸çš„åœç”¨è¯ã€‚\n",
      "  - (ä¸“å®¶å±‚) æ·»åŠ äº† 228 ä¸ªé¡¹ç›®ä¸“å±åœç”¨è¯ã€‚\n",
      "âœ… æœ€ç»ˆåœç”¨è¯é›†æ„å»ºå®Œæˆï¼Œå…±åŒ…å« 1038 ä¸ªä¸é‡å¤çš„åœç”¨è¯ã€‚\n",
      "âœ… æœ€ç»ˆåœç”¨è¯åˆ—è¡¨å·²ä¿å­˜åˆ°: ../data_processed\\stopwords.csv\n",
      "\n",
      "å¼€å§‹æµå¼å¤„ç†å’Œæ¸…æ´—æ–‡æœ¬...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "æµå¼æ¸…æ´—Tokens (ç»ˆæç‰ˆ):   0%|          | 0/180630 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bd0c0a5fbd54de7b0fc01a92a8ddc03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¸…æ´—æµç¨‹å®Œæˆï¼è€—æ—¶: 108.12 åˆ†é’Ÿã€‚\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 3: æœ€ç»ˆæ£€æŸ¥ä¸ä¿å­˜**\n",
    "\n",
    "**ç›®æ ‡:** æŠ½æ ·æ£€æŸ¥æ¸…æ´—æ•ˆæœï¼Œå¹¶ä¿å­˜æœ€ç»ˆäº§ç‰©ã€‚è¿™æ˜¯æˆ‘ä»¬**æ·±åº¦æ¸…æ´—æ–‡æœ¬**å®è§‚é˜¶æ®µçš„æœ€ç»ˆäº¤ä»˜ã€‚"
   ],
   "id": "87848316f94ea172"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T16:35:26.805747Z",
     "start_time": "2025-08-07T16:34:53.892570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 3: æœ€ç»ˆæ£€æŸ¥ä¸ä¿å­˜ ---\n",
    "# =============================================================================\n",
    "\n",
    "# ä½œç”¨: è´Ÿè´£å¯¹æ¸…æ´—åçš„ç»“æœè¿›è¡ŒæŠ½æ ·æ£€æŸ¥ï¼Œå¹¶ä¿å­˜ä¸ºæœ€ç»ˆçš„Pickleå’ŒCSVæ–‡ä»¶ã€‚\n",
    "if 'df' in locals() and 'tokens_for_lda' in df.columns:\n",
    "    print(\"\\n--- é˜¶æ®µå››: æœ€ç»ˆæ£€æŸ¥ä¸ä¿å­˜ ---\")\n",
    "\n",
    "    # ä½œç”¨: éšæœºæŠ½å–5ç¯‡æ–‡ç« ï¼Œæ‰“å°å…¶å›ºåŒ–åæ–‡æœ¬å’Œæœ€ç»ˆtokenåˆ—è¡¨ï¼Œç”¨äºäººå·¥å¿«é€ŸéªŒè¯æ¸…æ´—æ•ˆæœã€‚\n",
    "    print(\"\\n--- æŠ½æ ·æ£€æŸ¥ç»“æœ ---\")\n",
    "    sample_size = min(5, len(df))\n",
    "    if sample_size > 0:\n",
    "        pd.set_option('display.max_colwidth', 300)\n",
    "        display(df.sample(sample_size)[['content_solidified', 'tokens_for_lda']])\n",
    "\n",
    "    # ä½œç”¨: å°†æœ€ç»ˆç»“æœä¿å­˜ä¸ºä¸¤ç§æ ¼å¼ã€‚\n",
    "    try:\n",
    "        # Pickleæ ¼å¼: ç”¨äºåç»­Pythonè„šæœ¬é«˜æ•ˆè¯»å–ï¼Œä¿ç•™æ•°æ®ç±»å‹ã€‚\n",
    "        df.to_pickle(CLEANED_TOKENS_PKL_PATH)\n",
    "        print(f\"\\nâœ… [æœºå™¨ç”¨] åŒ…å«æœ€ç»ˆTokensçš„DataFrameå·²ä¿å­˜åˆ°Pickle: {CLEANED_TOKENS_PKL_PATH}\")\n",
    "\n",
    "        # CSVæ ¼å¼: ç”¨äºäººå·¥æŸ¥é˜…å’Œå®¡è®¡ï¼Œtokenåˆ—è¡¨ä¼šè½¬æ¢ä¸ºç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²ã€‚\n",
    "        df_for_csv = df.copy()\n",
    "        df_for_csv['tokens_for_lda'] = df_for_csv['tokens_for_lda'].apply(lambda tokens: ' '.join(tokens))\n",
    "        df_for_csv.to_csv(CLEANED_TOKENS_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "        print(f\"âœ… [äººç±»ç”¨] åŒ…å«æœ€ç»ˆTokensçš„DataFrameå·²ä¿å­˜åˆ°CSV: {CLEANED_TOKENS_CSV_PATH}\")\n",
    "\n",
    "        print(f\"\\nğŸ‰ğŸ‰ğŸ‰ æ·±åº¦æ–‡æœ¬æ¸…æ´—æµç¨‹å…¨éƒ¨å®Œæˆï¼ ğŸ‰ğŸ‰ğŸ‰\")\n",
    "        print(f\"\\nä¸‹ä¸€æ­¥æ˜¯è¿è¡Œ '04_Topic_Modeling.ipynb' æˆ– '04_sLDA_Modeling.ipynb'ï¼Œå¼€å§‹ä¸»é¢˜å»ºæ¨¡åˆ†æã€‚\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "else:\n",
    "    print(\"æ²¡æœ‰å¯ä¾›ä¿å­˜çš„æœ€ç»ˆTokensæ•°æ®ã€‚\")"
   ],
   "id": "ec4f4e4650b3e7c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- é˜¶æ®µå››: æœ€ç»ˆæ£€æŸ¥ä¸ä¿å­˜ ---\n",
      "\n",
      "--- æŠ½æ ·æ£€æŸ¥ç»“æœ ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                 content_solidified  \\\n",
       "104063  The Southwest Border Is Open for Business . Over the last few weeks, mayors, sheriffs, business leaders and citizens have joined together with a simple but powerful message: America's Southwest border communities are open for business. This is a message the American people need to hear. Unfortun...   \n",
       "41544   justice_department Examines CIA Role In Probe Into Hughes's China Dealings . WASHINGTON -- The justice_department is examining whether the central_intelligence_agency impeded an investigation of Hughes Electronics Corp.'s business dealings in China. Investigators are trying to determine whether ...   \n",
       "5842    Film: The Mousekewitz Migration . I wouldn't say that walt_disney warped me exactly; however, on a recent trip to Italy I bought a set of ceramic bowls because the deer painted on them reminded me of Bambi. I still have small yellow records with the sound track from Cinderella tucked away in a c...   \n",
       "101770  Why We're Always Fooled by north_korea . According to Siegfried Hecker, the former director of the Los Alamos National Laboratory, north_korea is working on two new nuclear facilities, a light water power reactor in early stages of construction, and a \"modern, clean centrifuge plant\" for uranium...   \n",
       "116722  Big hong_kong Investors See Time to Sell --- Some of the Wealthiest Families Believe the Red-Hot Real-Estate Market Is Coming to an End; 'Selling at the Very Top' . hong_kong -- With the government growing confident that it has halted the meteoric rise in property prices, some of this city's big...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     tokens_for_lda  \n",
       "104063  [southwest, border, open, business, mayor, sheriff, business, leader, citizen, join, simple, powerful, message, america, southwest, border, open, business, message, american, hear, unfortunate, widespread, misperception, southwest, wrack, violence, spill, mexico, ongoing, drug, war, different, a...  \n",
       "41544   [justice_department, examine, cia, probe, hughes, china, washington, justice_department, examine, central_intelligence_agency, impede, investigation, hughes, electronics, corp., business, china, investigator, cia, official, act, improper, alert, hughes, electronics, employee, intelligence, commi...  \n",
       "5842    [film, mousekewitz, migration, walt_disney, warp, exact, trip, italy, buy, ceramic, bowl, deer, paint, remind, bambi, yellow, record, sound, track, cinderella, tuck, closet, simple, draw, childhood, mythology, disney, animated, teach, dream, evil, anxiety, especial, realize, coat, syrup, safe, b...  \n",
       "101770  [fool, north_korea, accord, siegfried, hecker, los, alamos, national, laboratory, north_korea, work, nuclear, facility, light, water, power, reactor, stage, modern, clean, centrifuge, plant, uranium, enrichment, mr., hecker, visit, facility, weekend, appear, near, complete, centrifuge, plant, pa...  \n",
       "116722  [hong_kong, investor, sell, wealthiest, families, red_hot, real_estate, market, end, sell, hong_kong, government, grow, confident, halt, meteoric, rise, property, price, city, real_estate, investor, hong_kong, wealthy, initial, public, offering, hotel, office, real_estate, asset, coming, price, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_solidified</th>\n",
       "      <th>tokens_for_lda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104063</th>\n",
       "      <td>The Southwest Border Is Open for Business . Over the last few weeks, mayors, sheriffs, business leaders and citizens have joined together with a simple but powerful message: America's Southwest border communities are open for business. This is a message the American people need to hear. Unfortun...</td>\n",
       "      <td>[southwest, border, open, business, mayor, sheriff, business, leader, citizen, join, simple, powerful, message, america, southwest, border, open, business, message, american, hear, unfortunate, widespread, misperception, southwest, wrack, violence, spill, mexico, ongoing, drug, war, different, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41544</th>\n",
       "      <td>justice_department Examines CIA Role In Probe Into Hughes's China Dealings . WASHINGTON -- The justice_department is examining whether the central_intelligence_agency impeded an investigation of Hughes Electronics Corp.'s business dealings in China. Investigators are trying to determine whether ...</td>\n",
       "      <td>[justice_department, examine, cia, probe, hughes, china, washington, justice_department, examine, central_intelligence_agency, impede, investigation, hughes, electronics, corp., business, china, investigator, cia, official, act, improper, alert, hughes, electronics, employee, intelligence, commi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5842</th>\n",
       "      <td>Film: The Mousekewitz Migration . I wouldn't say that walt_disney warped me exactly; however, on a recent trip to Italy I bought a set of ceramic bowls because the deer painted on them reminded me of Bambi. I still have small yellow records with the sound track from Cinderella tucked away in a c...</td>\n",
       "      <td>[film, mousekewitz, migration, walt_disney, warp, exact, trip, italy, buy, ceramic, bowl, deer, paint, remind, bambi, yellow, record, sound, track, cinderella, tuck, closet, simple, draw, childhood, mythology, disney, animated, teach, dream, evil, anxiety, especial, realize, coat, syrup, safe, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101770</th>\n",
       "      <td>Why We're Always Fooled by north_korea . According to Siegfried Hecker, the former director of the Los Alamos National Laboratory, north_korea is working on two new nuclear facilities, a light water power reactor in early stages of construction, and a \"modern, clean centrifuge plant\" for uranium...</td>\n",
       "      <td>[fool, north_korea, accord, siegfried, hecker, los, alamos, national, laboratory, north_korea, work, nuclear, facility, light, water, power, reactor, stage, modern, clean, centrifuge, plant, uranium, enrichment, mr., hecker, visit, facility, weekend, appear, near, complete, centrifuge, plant, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116722</th>\n",
       "      <td>Big hong_kong Investors See Time to Sell --- Some of the Wealthiest Families Believe the Red-Hot Real-Estate Market Is Coming to an End; 'Selling at the Very Top' . hong_kong -- With the government growing confident that it has halted the meteoric rise in property prices, some of this city's big...</td>\n",
       "      <td>[hong_kong, investor, sell, wealthiest, families, red_hot, real_estate, market, end, sell, hong_kong, government, grow, confident, halt, meteoric, rise, property, price, city, real_estate, investor, hong_kong, wealthy, initial, public, offering, hotel, office, real_estate, asset, coming, price, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… [æœºå™¨ç”¨] åŒ…å«æœ€ç»ˆTokensçš„DataFrameå·²ä¿å­˜åˆ°Pickle: ../data_processed\\china_news_cleaned_tokens.pkl\n",
      "âœ… [äººç±»ç”¨] åŒ…å«æœ€ç»ˆTokensçš„DataFrameå·²ä¿å­˜åˆ°CSV: ../data_processed\\china_news_cleaned_tokens_for_review.csv\n",
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ æ·±åº¦æ–‡æœ¬æ¸…æ´—æµç¨‹å…¨éƒ¨å®Œæˆï¼ ğŸ‰ğŸ‰ğŸ‰\n",
      "\n",
      "ä¸‹ä¸€æ­¥æ˜¯è¿è¡Œ '04_Topic_Modeling.ipynb' æˆ– '04_sLDA_Modeling.ipynb'ï¼Œå¼€å§‹ä¸»é¢˜å»ºæ¨¡åˆ†æã€‚\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
