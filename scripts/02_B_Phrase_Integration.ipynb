{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 1: ç¯å¢ƒè®¾ç½®ã€åº“å¯¼å…¥ä¸è·¯å¾„ç®¡ç†**\n",
    "\n",
    "**ç›®æ ‡:** åˆå§‹åŒ–é¡¹ç›®ç¯å¢ƒï¼ŒåŠ è½½æ‰€æœ‰å¿…éœ€çš„åº“ï¼Œå¹¶æ ¹æ® `RUNNING_ENV` å’Œ `TEST_MODE` æ™ºèƒ½é…ç½®æ‰€æœ‰è¾“å…¥è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„ã€‚æ­¤ä»£ç å—æ˜¯æ•´ä¸ªNotebookçš„æ§åˆ¶ä¸­å¿ƒï¼Œè´Ÿè´£ç¡®ä¿ä»£ç çš„å¯ç§»æ¤æ€§ã€å¯é‡å¤æ€§å’Œæ˜“äºè°ƒè¯•ã€‚"
   ],
   "id": "b462e457f5bc2ac9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:05:59.664341Z",
     "start_time": "2025-08-02T10:05:59.240680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 1: ç¯å¢ƒè®¾ç½®ã€åº“å¯¼å…¥ä¸è·¯å¾„ç®¡ç† ---\n",
    "# =============================================================================\n",
    "\n",
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import gc # åƒåœ¾å›æ”¶åº“ï¼Œç”¨äºä¸»åŠ¨é‡Šæ”¾å†…å­˜\n",
    "\n",
    "# --- æ ¸å¿ƒé…ç½®åŒº ---\n",
    "# ç¡®ä¿è¿™é‡Œçš„é…ç½®ä¸ä½ çš„é¡¹ç›®ç¯å¢ƒä¸€è‡´\n",
    "RUNNING_ENV = 'local'  # å¯é€‰: 'local' æˆ– 'dsw'\n",
    "TEST_MODE = True       # å¯é€‰: True æˆ– False\n",
    "# å¦‚æœä¸æ˜¯æµ‹è¯•æ¨¡å¼ï¼Œå¯ä»¥å¤„ç†æ›´å¤šå€™é€‰ï¼Œä½†äººå·¥å®¡æ ¸å·¥ä½œé‡ä¼šå¢åŠ \n",
    "MAX_CANDIDATES_FOR_REVIEW = 5000\n",
    "MIN_FREQUENCY_THRESHOLD = 5 if TEST_MODE else 10\n",
    "\n",
    "# --- è·¯å¾„æ™ºèƒ½ç®¡ç† ---\n",
    "print(f\"æ£€æµ‹åˆ°è¿è¡Œç¯å¢ƒä¸º: ã€{RUNNING_ENV.upper()}ã€‘\")\n",
    "TEMP_DIR = '/tmp'\n",
    "\n",
    "if RUNNING_ENV == 'local':\n",
    "    print(\"ä½¿ç”¨ 'local' æ¨¡å¼çš„ç›¸å¯¹è·¯å¾„ã€‚\")\n",
    "    BASE_DATA_PROCESSED_PATH = '../data_processed'\n",
    "elif RUNNING_ENV == 'dsw':\n",
    "    print(\"ä½¿ç”¨ 'dsw' æ¨¡å¼çš„ç»å¯¹è·¯å¾„ã€‚\")\n",
    "    BASE_DATA_PROCESSED_PATH = '/mnt/data/data_processed'\n",
    "    if not os.path.exists(TEMP_DIR): os.makedirs(TEMP_DIR)\n",
    "else:\n",
    "    raise ValueError(f\"æœªçŸ¥çš„ RUNNING_ENV: '{RUNNING_ENV}'. è¯·é€‰æ‹© 'local' æˆ– 'dsw'ã€‚\")\n",
    "\n",
    "# --- å®šä¹‰æ‰€æœ‰â€œåŸå§‹â€æ–‡ä»¶è·¯å¾„ ---\n",
    "# è¾“å…¥æ–‡ä»¶\n",
    "TOKEN_LISTS_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'intermediate_raw_token_lists.pkl')\n",
    "CANDIDATES_GENSIM_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'candidates_gensim.pkl')\n",
    "CANDIDATES_NER_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'candidates_ner.pkl')\n",
    "# è¾“å‡ºæ–‡ä»¶\n",
    "CANDIDATES_FOR_REVIEW_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'candidate_phrases_for_review.csv')\n",
    "\n",
    "# --- åˆå§‹åŒ–å®é™…ä½¿ç”¨çš„è·¯å¾„å˜é‡ ---\n",
    "TOKEN_LISTS_PATH = TOKEN_LISTS_ORIGINAL\n",
    "CANDIDATES_GENSIM_PATH = CANDIDATES_GENSIM_ORIGINAL\n",
    "CANDIDATES_NER_PATH = CANDIDATES_NER_ORIGINAL\n",
    "CANDIDATES_FOR_REVIEW_PATH = CANDIDATES_FOR_REVIEW_ORIGINAL\n",
    "\n",
    "# --- DSWç¯å¢ƒä¸‹çš„I/Oä¼˜åŒ– ---\n",
    "if RUNNING_ENV == 'dsw':\n",
    "    print(\"DSW ç¯å¢ƒæ¨¡å¼å·²æ¿€æ´»ï¼Œå°†æ™ºèƒ½æ£€æŸ¥å¹¶ä½¿ç”¨æœ¬åœ°ä¸´æ—¶ç›®å½• /tmp ...\")\n",
    "    # å®šä¹‰ä¸´æ—¶è·¯å¾„\n",
    "    TEMP_TOKEN_LISTS = os.path.join(TEMP_DIR, 'intermediate_raw_token_lists.pkl')\n",
    "    TEMP_CANDIDATES_GENSIM = os.path.join(TEMP_DIR, 'candidates_gensim.pkl')\n",
    "    TEMP_CANDIDATES_NER = os.path.join(TEMP_DIR, 'candidates_ner.pkl')\n",
    "    TEMP_CANDIDATES_FOR_REVIEW = os.path.join(TEMP_DIR, 'candidate_phrases_for_review.csv')\n",
    "\n",
    "    # æ™ºèƒ½å¤åˆ¶å‡½æ•°\n",
    "    def sync_to_tmp(original_path, temp_path):\n",
    "        if os.path.exists(original_path):\n",
    "            if not os.path.exists(temp_path) or os.path.getsize(original_path) != os.path.getsize(temp_path):\n",
    "                print(f\"æ­£åœ¨ä» {original_path} åŒæ­¥åˆ° {temp_path}...\")\n",
    "                shutil.copy(original_path, temp_path)\n",
    "                print(\"åŒæ­¥å®Œæˆã€‚\")\n",
    "            else:\n",
    "                 print(f\"ä¸´æ—¶æ–‡ä»¶ {temp_path} å·²æ˜¯æœ€æ–°ï¼Œè·³è¿‡åŒæ­¥ã€‚\")\n",
    "            return temp_path\n",
    "        else:\n",
    "            print(f\"âš ï¸ è­¦å‘Š: æºæ–‡ä»¶ {original_path} ä¸å­˜åœ¨ã€‚\")\n",
    "            return None\n",
    "\n",
    "    # å¯¹æ‰€æœ‰è¾“å…¥æ–‡ä»¶æ‰§è¡ŒåŒæ­¥å¹¶é‡å®šå‘è·¯å¾„\n",
    "    TOKEN_LISTS_PATH = sync_to_tmp(TOKEN_LISTS_ORIGINAL, TEMP_TOKEN_LISTS)\n",
    "    CANDIDATES_GENSIM_PATH = sync_to_tmp(CANDIDATES_GENSIM_ORIGINAL, TEMP_CANDIDATES_GENSIM)\n",
    "    CANDIDATES_NER_PATH = sync_to_tmp(CANDIDATES_NER_ORIGINAL, TEMP_CANDIDATES_NER)\n",
    "    CANDIDATES_FOR_REVIEW_PATH = TEMP_CANDIDATES_FOR_REVIEW # è¾“å‡ºè·¯å¾„ç›´æ¥æŒ‡å‘/tmp\n",
    "\n",
    "# --- æ‰“å°æœ€ç»ˆé…ç½®ä¿¡æ¯ ---\n",
    "print(\"\\n--- ç¯å¢ƒå‡†å¤‡ ---\")\n",
    "if TEST_MODE:\n",
    "    print(f\"ğŸš€ğŸš€ğŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼ğŸš€ğŸš€ğŸš€\")\n",
    "else:\n",
    "    print(f\"ğŸš¢ğŸš¢ğŸš¢ è¿è¡Œåœ¨ã€å®Œæ•´æ•°æ®æ¨¡å¼ã€‘ä¸‹ã€‚ğŸš¢ğŸš¢ğŸš¢\")\n",
    "\n",
    "print(f\"Gensimå€™é€‰è¾“å…¥: {CANDIDATES_GENSIM_PATH}\")\n",
    "print(f\"NERå€™é€‰è¾“å…¥: {CANDIDATES_NER_PATH}\")\n",
    "print(f\"Tokenåˆ—è¡¨è¾“å…¥ (ç”¨äºè®¡ç®—é¢‘ç‡): {TOKEN_LISTS_PATH}\")\n",
    "print(f\"å¾…å®¡æ ¸æ–‡ä»¶è¾“å‡º: {CANDIDATES_FOR_REVIEW_PATH}\")\n",
    "print(f\"æœ€ä½é¢‘ç‡é˜ˆå€¼: {MIN_FREQUENCY_THRESHOLD}\")"
   ],
   "id": "da782d3bae5ebad9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æµ‹åˆ°è¿è¡Œç¯å¢ƒä¸º: ã€LOCALã€‘\n",
      "ä½¿ç”¨ 'local' æ¨¡å¼çš„ç›¸å¯¹è·¯å¾„ã€‚\n",
      "\n",
      "--- ç¯å¢ƒå‡†å¤‡ ---\n",
      "ğŸš€ğŸš€ğŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼ğŸš€ğŸš€ğŸš€\n",
      "Gensimå€™é€‰è¾“å…¥: ../data_processed\\candidates_gensim.pkl\n",
      "NERå€™é€‰è¾“å…¥: ../data_processed\\candidates_ner.pkl\n",
      "Tokenåˆ—è¡¨è¾“å…¥ (ç”¨äºè®¡ç®—é¢‘ç‡): ../data_processed\\intermediate_raw_token_lists.pkl\n",
      "å¾…å®¡æ ¸æ–‡ä»¶è¾“å‡º: ../data_processed\\candidate_phrases_for_review.csv\n",
      "æœ€ä½é¢‘ç‡é˜ˆå€¼: 5\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 2: åŠ è½½å€™é€‰çŸ­è¯­ä¸Tokenåˆ—è¡¨**\n",
    "\n",
    "**ç›®æ ‡:** å°†ä¸Šä¸€æµç¨‹ï¼ˆ`02_Entity_Phrase_Solidification.ipynb`ï¼‰ç”Ÿæˆçš„ä¸¤ä¸ªå€™é€‰çŸ­è¯­æ± ï¼ˆGensimå’ŒNERï¼‰ä»¥åŠç”¨äºè®¡æ•°çš„åŸå§‹Tokenåˆ—è¡¨ä»ç£ç›˜åŠ è½½åˆ°å†…å­˜ä¸­ã€‚æ­¤æ­¥éª¤æ˜¯åç»­æ‰€æœ‰æ•´åˆå·¥ä½œçš„æ•°æ®åŸºç¡€ã€‚"
   ],
   "id": "5367ef931f09081c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:05:59.890484Z",
     "start_time": "2025-08-02T10:05:59.827140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 2: åŠ è½½å€™é€‰çŸ­è¯­ä¸Tokenåˆ—è¡¨ ---\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- é˜¶æ®µ 2.1: åŠ è½½å€™é€‰çŸ­è¯­ä¸Tokenåˆ—è¡¨ ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# åˆå§‹åŒ–ä¸ºç©ºï¼Œä»¥é˜²æ–‡ä»¶åŠ è½½å¤±è´¥\n",
    "gensim_candidates, ner_candidates, raw_token_lists = {}, {}, []\n",
    "\n",
    "try:\n",
    "    with open(CANDIDATES_GENSIM_PATH, 'rb') as f:\n",
    "        gensim_candidates = pickle.load(f)\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(gensim_candidates)} ä¸ª Gensim å€™é€‰çŸ­è¯­ã€‚\")\n",
    "\n",
    "    with open(CANDIDATES_NER_PATH, 'rb') as f:\n",
    "        ner_candidates = pickle.load(f)\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(ner_candidates)} ä¸ª NER å€™é€‰å®ä½“ã€‚\")\n",
    "\n",
    "    with open(TOKEN_LISTS_PATH, 'rb') as f:\n",
    "        raw_token_lists = pickle.load(f)\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(raw_token_lists)} ç¯‡æ–‡ç« çš„Tokenåˆ—è¡¨ã€‚\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„è¾“å…¥æ–‡ä»¶: {e.filename}ã€‚\")\n",
    "    print(\"è¯·ç¡®ä¿å·²æˆåŠŸè¿è¡Œ '02_Entity_Phrase_Solidification.ipynb'ã€‚\")\n",
    "\n",
    "print(f\"åŠ è½½è€—æ—¶: {time.time() - start_time:.2f} ç§’ã€‚\")"
   ],
   "id": "1f5a6493400a67fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- é˜¶æ®µ 2.1: åŠ è½½å€™é€‰çŸ­è¯­ä¸Tokenåˆ—è¡¨ ---\n",
      "âœ… æˆåŠŸåŠ è½½ 3172 ä¸ª Gensim å€™é€‰çŸ­è¯­ã€‚\n",
      "âœ… æˆåŠŸåŠ è½½ 7920 ä¸ª NER å€™é€‰å®ä½“ã€‚\n",
      "âœ… æˆåŠŸåŠ è½½ 1000 ç¯‡æ–‡ç« çš„Tokenåˆ—è¡¨ã€‚\n",
      "åŠ è½½è€—æ—¶: 0.06 ç§’ã€‚\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 3: æ„å»ºä¸åˆå¹¶æ•°æ®æ¡† (å«é¢‘ç‡è®¡ç®—ä¼˜åŒ–)**\n",
    "\n",
    "**ç›®æ ‡:** å°†ä¸¤ä¸ªæ¥æºçš„å€™é€‰çŸ­è¯­ç»Ÿä¸€åˆ°å•ä¸ªPandas DataFrameä¸­ï¼Œå¹¶é«˜æ•ˆåœ°ä¸ºGensimå‘ç°çš„çŸ­è¯­è®¡ç®—å…¶åœ¨æ•´ä¸ªè¯­æ–™åº“ä¸­çš„å‡ºç°é¢‘ç‡ã€‚\n",
    "\n",
    "**ä¼˜åŒ–ç‚¹:** æ­¤å¤„é‡‡ç”¨äº†Qwenå»ºè®®çš„ä¼˜åŒ–æ–¹æ¡ˆã€‚æˆ‘ä»¬ä¸å†ä½¿ç”¨æ•ˆç‡è¾ƒä½çš„å­—ç¬¦ä¸²`.count()`æ–¹æ³•ï¼Œè€Œæ˜¯é€šè¿‡**é¢„å¤„ç†å€™é€‰çŸ­è¯­**ï¼ˆæŒ‰é•¿åº¦åˆ†ç»„ï¼‰å’Œ**æ»‘åŠ¨çª—å£éå†Tokenåˆ—è¡¨**çš„æ–¹å¼ï¼Œæ˜¾è‘—æå‡é¢‘ç‡è®¡ç®—çš„æ€§èƒ½ã€‚"
   ],
   "id": "c99be61520b73247"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:06:00.257472Z",
     "start_time": "2025-08-02T10:05:59.903646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 3: æ„å»ºä¸åˆå¹¶æ•°æ®æ¡† (å«é¢‘ç‡è®¡ç®—ä¼˜åŒ–) ---\n",
    "# =============================================================================\n",
    "\n",
    "if gensim_candidates and ner_candidates:\n",
    "    print(\"\\n--- é˜¶æ®µ 2.2: æ„å»ºä¸åˆå¹¶æ•°æ®æ¡† ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 3.1 å¤„ç†Gensimæ•°æ®\n",
    "    print(\"æ­£åœ¨å¤„ç† Gensim å€™é€‰...\")\n",
    "    df_gensim = pd.DataFrame(gensim_candidates.items(), columns=['candidate_phrase', 'score'])\n",
    "    df_gensim['source'] = 'Gensim'\n",
    "    print(f\"  - Gensim DataFrame åˆ›å»ºå®Œæˆï¼Œå½¢çŠ¶: {df_gensim.shape}\")\n",
    "\n",
    "    # 3.2 å¤„ç†NERæ•°æ®\n",
    "    print(\"æ­£åœ¨å¤„ç† NER å€™é€‰...\")\n",
    "    df_ner = pd.DataFrame(ner_candidates.items(), columns=['candidate_phrase', 'frequency'])\n",
    "    df_ner['source'] = 'NER'\n",
    "    print(f\"  - NER DataFrame åˆ›å»ºå®Œæˆï¼Œå½¢çŠ¶: {df_ner.shape}\")\n",
    "\n",
    "    # 3.3 [ä¼˜åŒ–ç‰ˆ] é«˜æ•ˆè®¡ç®—GensimçŸ­è¯­é¢‘ç‡\n",
    "    print(\"æ­£åœ¨ä¸º Gensim çŸ­è¯­é«˜æ•ˆè®¡ç®—é¢‘ç‡ (ä¼˜åŒ–ç‰ˆ)...\")\n",
    "\n",
    "    # é¢„å¤„ç†ï¼šå°†çŸ­è¯­æŒ‰å…¶åŒ…å«çš„tokenæ•°é‡åˆ†ç»„\n",
    "    gensim_phrases_by_len = {}\n",
    "    for phrase_str in df_gensim['candidate_phrase']:\n",
    "        tokens = phrase_str.split()\n",
    "        n = len(tokens)\n",
    "        if n > 1: # åªå¤„ç†å¤šè¯çŸ­è¯­\n",
    "            if n not in gensim_phrases_by_len:\n",
    "                gensim_phrases_by_len[n] = set()\n",
    "            gensim_phrases_by_len[n].add(tuple(tokens)) # ä½¿ç”¨å…ƒç»„ä»¥æ”¯æŒå“ˆå¸Œ\n",
    "\n",
    "    phrase_counts = Counter()\n",
    "\n",
    "    # éå†æ‰€æœ‰æ–‡ç« çš„tokenåˆ—è¡¨\n",
    "    for doc_tokens_case_sensitive in tqdm(raw_token_lists, desc=\"é«˜æ•ˆè®¡ç®—Gensimé¢‘ç‡\"):\n",
    "        doc_tokens = [token.lower() for token in doc_tokens_case_sensitive]\n",
    "        doc_len = len(doc_tokens)\n",
    "\n",
    "        # å¯¹æ¯ç§å¯èƒ½çš„çŸ­è¯­é•¿åº¦è¿›è¡Œæ»‘åŠ¨çª—å£æ£€æŸ¥\n",
    "        for n, phrases_set in gensim_phrases_by_len.items():\n",
    "            if doc_len < n:\n",
    "                continue\n",
    "            for i in range(doc_len - n + 1):\n",
    "                window_tuple = tuple(doc_tokens[i : i + n])\n",
    "                if window_tuple in phrases_set:\n",
    "                    phrase_counts[\" \".join(window_tuple)] += 1\n",
    "\n",
    "    # å°†è®¡ç®—å‡ºçš„é¢‘ç‡æ˜ å°„å›df_gensim\n",
    "    df_gensim['frequency'] = df_gensim['candidate_phrase'].map(phrase_counts)\n",
    "    df_gensim['frequency'] = df_gensim['frequency'].fillna(0).astype(int)\n",
    "    print(\"  - Gensim é¢‘ç‡è®¡ç®—å®Œæˆã€‚\")\n",
    "\n",
    "    # é‡Šæ”¾å†…å­˜\n",
    "    del raw_token_lists, phrase_counts, gensim_phrases_by_len\n",
    "    gc.collect()\n",
    "\n",
    "    # 3.4 åˆå¹¶\n",
    "    print(\"æ­£åœ¨åˆå¹¶ä¸¤ä¸ªæ¥æºçš„æ•°æ®æ¡†...\")\n",
    "    df_candidates = pd.concat([df_ner, df_gensim], ignore_index=True)\n",
    "    print(f\"  - åˆå¹¶å®Œæˆã€‚æ€»å€™é€‰æ•°: {len(df_candidates)}\")\n",
    "    print(f\"æ„å»ºä¸åˆå¹¶è€—æ—¶: {time.time() - start_time:.2f} ç§’ã€‚\")\n",
    "else:\n",
    "    print(\"å€™é€‰æ•°æ®æœªåŠ è½½ï¼Œè·³è¿‡æ­¤å—ã€‚\")"
   ],
   "id": "ebc7b25bf54395d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- é˜¶æ®µ 2.2: æ„å»ºä¸åˆå¹¶æ•°æ®æ¡† ---\n",
      "æ­£åœ¨å¤„ç† Gensim å€™é€‰...\n",
      "  - Gensim DataFrame åˆ›å»ºå®Œæˆï¼Œå½¢çŠ¶: (3172, 3)\n",
      "æ­£åœ¨å¤„ç† NER å€™é€‰...\n",
      "  - NER DataFrame åˆ›å»ºå®Œæˆï¼Œå½¢çŠ¶: (7920, 3)\n",
      "æ­£åœ¨ä¸º Gensim çŸ­è¯­é«˜æ•ˆè®¡ç®—é¢‘ç‡ (ä¼˜åŒ–ç‰ˆ)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "é«˜æ•ˆè®¡ç®—Gensimé¢‘ç‡:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64c1eed1116a4e1683421963e77ebdd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Gensim é¢‘ç‡è®¡ç®—å®Œæˆã€‚\n",
      "æ­£åœ¨åˆå¹¶ä¸¤ä¸ªæ¥æºçš„æ•°æ®æ¡†...\n",
      "  - åˆå¹¶å®Œæˆã€‚æ€»å€™é€‰æ•°: 11092\n",
      "æ„å»ºä¸åˆå¹¶è€—æ—¶: 0.35 ç§’ã€‚\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 4: æ¸…æ´—ã€è¿‡æ»¤ã€æ’åºä¸å»é‡**\n",
    "\n",
    "**ç›®æ ‡:** å¯¹åˆå¹¶åçš„å€™é€‰æ± è¿›è¡Œâ€œæçº¯â€ï¼Œç§»é™¤æ˜æ˜¾æ— ç”¨çš„æ¡ç›®ï¼ˆå¦‚ä½é¢‘ã€å«æ•°å­—ï¼‰ï¼Œå¹¶æ ¹æ®`NERä¼˜å…ˆ`çš„åŸåˆ™è¿›è¡Œå»é‡ï¼Œæœ€åæŒ‰`é¢‘ç‡`é™åºæ’åˆ—ï¼Œç”Ÿæˆä¸€ä»½æ•´æ´ã€æœ‰åºã€å¾…äººå·¥å®¡æ ¸çš„åˆ—è¡¨ã€‚"
   ],
   "id": "5967af15648eda2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:06:00.302161Z",
     "start_time": "2025-08-02T10:06:00.278496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 4: æ¸…æ´—ã€è¿‡æ»¤ã€æ’åºä¸å»é‡ ---\n",
    "# =============================================================================\n",
    "\n",
    "if 'df_candidates' in locals() and not df_candidates.empty:\n",
    "    print(\"\\n--- é˜¶æ®µ 2.3: æ¸…æ´—ã€è¿‡æ»¤ã€æ’åºä¸å»é‡ ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 4.1 æ¸…æ´—ä¸æ ‡å‡†åŒ–\n",
    "    print(f\"åŸå§‹å€™é€‰æ•°: {len(df_candidates)}\")\n",
    "    df_candidates['candidate_phrase'] = df_candidates['candidate_phrase'].str.strip()\n",
    "    df_candidates.dropna(subset=['candidate_phrase', 'frequency'], inplace=True)\n",
    "    df_candidates = df_candidates[df_candidates['candidate_phrase'] != '']\n",
    "    # è¿‡æ»¤æ‰åŒ…å«æ•°å­—çš„çŸ­è¯­å’Œè¿‡çŸ­çš„çŸ­è¯­\n",
    "    df_candidates = df_candidates[~df_candidates['candidate_phrase'].str.contains(r'\\d')]\n",
    "    df_candidates = df_candidates[df_candidates['candidate_phrase'].str.len() > 3]\n",
    "    print(f\"æ¸…æ´—åå€™é€‰æ•°: {len(df_candidates)}\")\n",
    "\n",
    "    # 4.2 è¿‡æ»¤ä½é¢‘é¡¹\n",
    "    df_candidates = df_candidates[df_candidates['frequency'] >= MIN_FREQUENCY_THRESHOLD]\n",
    "    print(f\"è¿‡æ»¤ä½é¢‘é¡¹å (é¢‘ç‡ >= {MIN_FREQUENCY_THRESHOLD}): {len(df_candidates)}\")\n",
    "\n",
    "    # 4.3 å»é‡ä¸ä¼˜å…ˆçº§å¤„ç† (æ ¸å¿ƒé€»è¾‘)\n",
    "    # å°†'source'åˆ—è½¬ä¸ºæœ‰åºçš„Categoricalç±»å‹ï¼Œä»¥å®šä¹‰ä¼˜å…ˆçº§\n",
    "    df_candidates['source'] = pd.Categorical(df_candidates['source'], categories=['Gensim', 'NER'], ordered=True)\n",
    "    # æŒ‰ å€™é€‰çŸ­è¯­ åˆ†ç»„ï¼ŒæŒ‰ source ä¼˜å…ˆçº§æ’åº (NERåœ¨åï¼Œæ‰€ä»¥æ˜¯æ›´é«˜ä¼˜å…ˆçº§)\n",
    "    df_candidates.sort_values(['candidate_phrase', 'source'], ascending=[True, True], inplace=True)\n",
    "    # å»é‡ï¼Œä¿ç•™æœ€åä¸€ä¸ªï¼ˆå³ä¼˜å…ˆçº§æœ€é«˜çš„NERï¼‰\n",
    "    df_candidates.drop_duplicates(subset=['candidate_phrase'], keep='last', inplace=True)\n",
    "    print(f\"å»é‡å (NERä¼˜å…ˆ): {len(df_candidates)}\")\n",
    "\n",
    "    # 4.4 æœ€ç»ˆæ’åº\n",
    "    df_candidates.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    print(\"å·²æŒ‰é¢‘ç‡é™åºæ’åˆ—ã€‚\")\n",
    "\n",
    "    # å¦‚æœä¸æ˜¯æµ‹è¯•æ¨¡å¼ï¼Œåˆ™æˆªå–å‰ MAX_CANDIDATES_FOR_REVIEW ä¸ª\n",
    "    if not TEST_MODE and len(df_candidates) > MAX_CANDIDATES_FOR_REVIEW:\n",
    "        df_candidates = df_candidates.head(MAX_CANDIDATES_FOR_REVIEW)\n",
    "        print(f\"å·²æˆªå–é¢‘ç‡æœ€é«˜çš„ {MAX_CANDIDATES_FOR_REVIEW} ä¸ªå€™é€‰è¿›è¡Œå®¡æ ¸ã€‚\")\n",
    "\n",
    "    # 4.5 æ·»åŠ å®¡æ ¸åˆ—\n",
    "    df_candidates['action_code'] = ''\n",
    "    df_candidates['standard_form'] = ''\n",
    "\n",
    "    # è°ƒæ•´åˆ—é¡ºåºä»¥ä¾¿å®¡æ ¸\n",
    "    final_columns = ['candidate_phrase', 'frequency', 'source', 'score', 'action_code', 'standard_form']\n",
    "    df_final_review = df_candidates.reindex(columns=final_columns)\n",
    "\n",
    "    print(f\"æœ€ç»ˆå¾…å®¡æ ¸å€™é€‰æ•°: {len(df_final_review)}\")\n",
    "    print(f\"æ¸…æ´—ä¸æ’åºè€—æ—¶: {time.time() - start_time:.2f} ç§’ã€‚\")\n",
    "\n",
    "else:\n",
    "    print(\"å€™é€‰DataFrameæœªåˆ›å»ºï¼Œè·³è¿‡æ­¤å—ã€‚\")"
   ],
   "id": "c5f8c82fc39ec2d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- é˜¶æ®µ 2.3: æ¸…æ´—ã€è¿‡æ»¤ã€æ’åºä¸å»é‡ ---\n",
      "åŸå§‹å€™é€‰æ•°: 11092\n",
      "æ¸…æ´—åå€™é€‰æ•°: 10779\n",
      "è¿‡æ»¤ä½é¢‘é¡¹å (é¢‘ç‡ >= 5): 3364\n",
      "å»é‡å (NERä¼˜å…ˆ): 3142\n",
      "å·²æŒ‰é¢‘ç‡é™åºæ’åˆ—ã€‚\n",
      "æœ€ç»ˆå¾…å®¡æ ¸å€™é€‰æ•°: 3142\n",
      "æ¸…æ´—ä¸æ’åºè€—æ—¶: 0.02 ç§’ã€‚\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 5: ä¿å­˜å¾…å®¡æ ¸æ–‡ä»¶**\n",
    "\n",
    "**ç›®æ ‡:** å°†æœ€ç»ˆå¤„ç†å®Œæˆçš„ã€å¾…äººå·¥å®¡æ ¸çš„DataFrameä¿å­˜ä¸ºCSVæ–‡ä»¶ã€‚åŒæ—¶ï¼Œæ ¹æ®è¿è¡Œç¯å¢ƒï¼Œè´Ÿè´£å°†ç»“æœä»ä¸´æ—¶ç›®å½•åŒæ­¥å›é¡¹ç›®çš„æ•°æ®ç›®å½•ï¼Œå¹¶ç»™å‡ºæ¸…æ™°çš„ä¸‹ä¸€æ­¥æ“ä½œæŒ‡å¼•ã€‚"
   ],
   "id": "9078d430b6e13fb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:06:00.341065Z",
     "start_time": "2025-08-02T10:06:00.325065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 5: ä¿å­˜å¾…å®¡æ ¸æ–‡ä»¶ ---\n",
    "# =============================================================================\n",
    "\n",
    "if 'df_final_review' in locals() and not df_final_review.empty:\n",
    "    print(\"\\n--- é˜¶æ®µ 2.4: ä¿å­˜å¾…å®¡æ ¸æ–‡ä»¶ ---\")\n",
    "    try:\n",
    "        # ä¿®æ­£FutureWarning\n",
    "        df_final_review['frequency'] = df_final_review['frequency'].fillna(0).astype(int)\n",
    "\n",
    "        df_final_review.to_csv(CANDIDATES_FOR_REVIEW_PATH, index=False, encoding='utf-8-sig') # ä½¿ç”¨ utf-8-sig ç¡®ä¿Excelèƒ½æ­£ç¡®æ‰“å¼€\n",
    "\n",
    "        final_output_path = CANDIDATES_FOR_REVIEW_PATH\n",
    "        # å¦‚æœæ˜¯DSWç¯å¢ƒï¼Œå°†ç»“æœä»/tmpåŒæ­¥å›ç½‘ç»œå­˜å‚¨\n",
    "        if RUNNING_ENV == 'dsw':\n",
    "            print(f\"æ­£åœ¨ä» {CANDIDATES_FOR_REVIEW_PATH} åŒæ­¥å› {CANDIDATES_FOR_REVIEW_ORIGINAL}...\")\n",
    "            shutil.copy(CANDIDATES_FOR_REVIEW_PATH, CANDIDATES_FOR_REVIEW_ORIGINAL)\n",
    "            final_output_path = CANDIDATES_FOR_REVIEW_ORIGINAL\n",
    "            print(\"åŒæ­¥å®Œæˆã€‚\")\n",
    "\n",
    "        print(f\"\\nâœ…âœ…âœ… æˆåŠŸç”Ÿæˆå¾…å®¡æ ¸æ–‡ä»¶ï¼ âœ…âœ…âœ…\")\n",
    "        print(f\"æ–‡ä»¶è·¯å¾„: {final_output_path}\")\n",
    "        print(f\"åŒ…å« {len(df_final_review)} æ¡å¾…å®¡æ ¸çš„å€™é€‰çŸ­è¯­ã€‚\")\n",
    "        print(\"\\nä¸‹ä¸€æ­¥æ“ä½œ:\")\n",
    "        print(\"1. æ‰“å¼€è¿™ä¸ªCSVæ–‡ä»¶ (æ¨èä½¿ç”¨Excelæˆ–Google Sheets)ã€‚\")\n",
    "        print(\"2. æ ¹æ®éœ€æ±‚æ–‡æ¡£çš„æŒ‡å¼•ï¼Œåœ¨ 'action_code' å’Œ 'standard_form' åˆ—å¡«å†™æ‚¨çš„å†³ç­–ã€‚\")\n",
    "        print(\"3. ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶ï¼Œä¸ºä¸‹ä¸€é˜¶æ®µ 'è§„åˆ™ç”Ÿæˆä¸åº”ç”¨' åšå‡†å¤‡ã€‚\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "else:\n",
    "    print(\"æ²¡æœ‰å¯ä¾›ä¿å­˜çš„å¾…å®¡æ ¸æ•°æ®ã€‚\")"
   ],
   "id": "be14b7f4c731267",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- é˜¶æ®µ 2.4: ä¿å­˜å¾…å®¡æ ¸æ–‡ä»¶ ---\n",
      "\n",
      "âœ…âœ…âœ… æˆåŠŸç”Ÿæˆå¾…å®¡æ ¸æ–‡ä»¶ï¼ âœ…âœ…âœ…\n",
      "æ–‡ä»¶è·¯å¾„: ../data_processed\\candidate_phrases_for_review.csv\n",
      "åŒ…å« 3142 æ¡å¾…å®¡æ ¸çš„å€™é€‰çŸ­è¯­ã€‚\n",
      "\n",
      "ä¸‹ä¸€æ­¥æ“ä½œ:\n",
      "1. æ‰“å¼€è¿™ä¸ªCSVæ–‡ä»¶ (æ¨èä½¿ç”¨Excelæˆ–Google Sheets)ã€‚\n",
      "2. æ ¹æ®éœ€æ±‚æ–‡æ¡£çš„æŒ‡å¼•ï¼Œåœ¨ 'action_code' å’Œ 'standard_form' åˆ—å¡«å†™æ‚¨çš„å†³ç­–ã€‚\n",
      "3. ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶ï¼Œä¸ºä¸‹ä¸€é˜¶æ®µ 'è§„åˆ™ç”Ÿæˆä¸åº”ç”¨' åšå‡†å¤‡ã€‚\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
