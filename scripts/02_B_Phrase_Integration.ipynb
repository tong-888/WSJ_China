{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 1: 环境设置、库导入与路径管理**\n",
    "\n",
    "**目标:** 初始化项目环境，加载所有必需的库，并根据 `RUNNING_ENV` 和 `TEST_MODE` 智能配置所有输入输出文件的路径。此代码块是整个Notebook的控制中心，负责确保代码的可移植性、可重复性和易于调试。"
   ],
   "id": "b462e457f5bc2ac9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:05:59.664341Z",
     "start_time": "2025-08-02T10:05:59.240680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 1: 环境设置、库导入与路径管理 ---\n",
    "# =============================================================================\n",
    "\n",
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import gc # 垃圾回收库，用于主动释放内存\n",
    "\n",
    "# --- 核心配置区 ---\n",
    "# 确保这里的配置与你的项目环境一致\n",
    "RUNNING_ENV = 'local'  # 可选: 'local' 或 'dsw'\n",
    "TEST_MODE = True       # 可选: True 或 False\n",
    "# 如果不是测试模式，可以处理更多候选，但人工审核工作量会增加\n",
    "MAX_CANDIDATES_FOR_REVIEW = 5000\n",
    "MIN_FREQUENCY_THRESHOLD = 5 if TEST_MODE else 10\n",
    "\n",
    "# --- 路径智能管理 ---\n",
    "print(f\"检测到运行环境为: 【{RUNNING_ENV.upper()}】\")\n",
    "TEMP_DIR = '/tmp'\n",
    "\n",
    "if RUNNING_ENV == 'local':\n",
    "    print(\"使用 'local' 模式的相对路径。\")\n",
    "    BASE_DATA_PROCESSED_PATH = '../data_processed'\n",
    "elif RUNNING_ENV == 'dsw':\n",
    "    print(\"使用 'dsw' 模式的绝对路径。\")\n",
    "    BASE_DATA_PROCESSED_PATH = '/mnt/data/data_processed'\n",
    "    if not os.path.exists(TEMP_DIR): os.makedirs(TEMP_DIR)\n",
    "else:\n",
    "    raise ValueError(f\"未知的 RUNNING_ENV: '{RUNNING_ENV}'. 请选择 'local' 或 'dsw'。\")\n",
    "\n",
    "# --- 定义所有“原始”文件路径 ---\n",
    "# 输入文件\n",
    "TOKEN_LISTS_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'intermediate_raw_token_lists.pkl')\n",
    "CANDIDATES_GENSIM_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'candidates_gensim.pkl')\n",
    "CANDIDATES_NER_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'candidates_ner.pkl')\n",
    "# 输出文件\n",
    "CANDIDATES_FOR_REVIEW_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'candidate_phrases_for_review.csv')\n",
    "\n",
    "# --- 初始化实际使用的路径变量 ---\n",
    "TOKEN_LISTS_PATH = TOKEN_LISTS_ORIGINAL\n",
    "CANDIDATES_GENSIM_PATH = CANDIDATES_GENSIM_ORIGINAL\n",
    "CANDIDATES_NER_PATH = CANDIDATES_NER_ORIGINAL\n",
    "CANDIDATES_FOR_REVIEW_PATH = CANDIDATES_FOR_REVIEW_ORIGINAL\n",
    "\n",
    "# --- DSW环境下的I/O优化 ---\n",
    "if RUNNING_ENV == 'dsw':\n",
    "    print(\"DSW 环境模式已激活，将智能检查并使用本地临时目录 /tmp ...\")\n",
    "    # 定义临时路径\n",
    "    TEMP_TOKEN_LISTS = os.path.join(TEMP_DIR, 'intermediate_raw_token_lists.pkl')\n",
    "    TEMP_CANDIDATES_GENSIM = os.path.join(TEMP_DIR, 'candidates_gensim.pkl')\n",
    "    TEMP_CANDIDATES_NER = os.path.join(TEMP_DIR, 'candidates_ner.pkl')\n",
    "    TEMP_CANDIDATES_FOR_REVIEW = os.path.join(TEMP_DIR, 'candidate_phrases_for_review.csv')\n",
    "\n",
    "    # 智能复制函数\n",
    "    def sync_to_tmp(original_path, temp_path):\n",
    "        if os.path.exists(original_path):\n",
    "            if not os.path.exists(temp_path) or os.path.getsize(original_path) != os.path.getsize(temp_path):\n",
    "                print(f\"正在从 {original_path} 同步到 {temp_path}...\")\n",
    "                shutil.copy(original_path, temp_path)\n",
    "                print(\"同步完成。\")\n",
    "            else:\n",
    "                 print(f\"临时文件 {temp_path} 已是最新，跳过同步。\")\n",
    "            return temp_path\n",
    "        else:\n",
    "            print(f\"⚠️ 警告: 源文件 {original_path} 不存在。\")\n",
    "            return None\n",
    "\n",
    "    # 对所有输入文件执行同步并重定向路径\n",
    "    TOKEN_LISTS_PATH = sync_to_tmp(TOKEN_LISTS_ORIGINAL, TEMP_TOKEN_LISTS)\n",
    "    CANDIDATES_GENSIM_PATH = sync_to_tmp(CANDIDATES_GENSIM_ORIGINAL, TEMP_CANDIDATES_GENSIM)\n",
    "    CANDIDATES_NER_PATH = sync_to_tmp(CANDIDATES_NER_ORIGINAL, TEMP_CANDIDATES_NER)\n",
    "    CANDIDATES_FOR_REVIEW_PATH = TEMP_CANDIDATES_FOR_REVIEW # 输出路径直接指向/tmp\n",
    "\n",
    "# --- 打印最终配置信息 ---\n",
    "print(\"\\n--- 环境准备 ---\")\n",
    "if TEST_MODE:\n",
    "    print(f\"🚀🚀🚀 运行在【快速测试模式】下！🚀🚀🚀\")\n",
    "else:\n",
    "    print(f\"🚢🚢🚢 运行在【完整数据模式】下。🚢🚢🚢\")\n",
    "\n",
    "print(f\"Gensim候选输入: {CANDIDATES_GENSIM_PATH}\")\n",
    "print(f\"NER候选输入: {CANDIDATES_NER_PATH}\")\n",
    "print(f\"Token列表输入 (用于计算频率): {TOKEN_LISTS_PATH}\")\n",
    "print(f\"待审核文件输出: {CANDIDATES_FOR_REVIEW_PATH}\")\n",
    "print(f\"最低频率阈值: {MIN_FREQUENCY_THRESHOLD}\")"
   ],
   "id": "da782d3bae5ebad9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到运行环境为: 【LOCAL】\n",
      "使用 'local' 模式的相对路径。\n",
      "\n",
      "--- 环境准备 ---\n",
      "🚀🚀🚀 运行在【快速测试模式】下！🚀🚀🚀\n",
      "Gensim候选输入: ../data_processed\\candidates_gensim.pkl\n",
      "NER候选输入: ../data_processed\\candidates_ner.pkl\n",
      "Token列表输入 (用于计算频率): ../data_processed\\intermediate_raw_token_lists.pkl\n",
      "待审核文件输出: ../data_processed\\candidate_phrases_for_review.csv\n",
      "最低频率阈值: 5\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 2: 加载候选短语与Token列表**\n",
    "\n",
    "**目标:** 将上一流程（`02_Entity_Phrase_Solidification.ipynb`）生成的两个候选短语池（Gensim和NER）以及用于计数的原始Token列表从磁盘加载到内存中。此步骤是后续所有整合工作的数据基础。"
   ],
   "id": "5367ef931f09081c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:05:59.890484Z",
     "start_time": "2025-08-02T10:05:59.827140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 2: 加载候选短语与Token列表 ---\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 阶段 2.1: 加载候选短语与Token列表 ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 初始化为空，以防文件加载失败\n",
    "gensim_candidates, ner_candidates, raw_token_lists = {}, {}, []\n",
    "\n",
    "try:\n",
    "    with open(CANDIDATES_GENSIM_PATH, 'rb') as f:\n",
    "        gensim_candidates = pickle.load(f)\n",
    "    print(f\"✅ 成功加载 {len(gensim_candidates)} 个 Gensim 候选短语。\")\n",
    "\n",
    "    with open(CANDIDATES_NER_PATH, 'rb') as f:\n",
    "        ner_candidates = pickle.load(f)\n",
    "    print(f\"✅ 成功加载 {len(ner_candidates)} 个 NER 候选实体。\")\n",
    "\n",
    "    with open(TOKEN_LISTS_PATH, 'rb') as f:\n",
    "        raw_token_lists = pickle.load(f)\n",
    "    print(f\"✅ 成功加载 {len(raw_token_lists)} 篇文章的Token列表。\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 错误: 缺少必要的输入文件: {e.filename}。\")\n",
    "    print(\"请确保已成功运行 '02_Entity_Phrase_Solidification.ipynb'。\")\n",
    "\n",
    "print(f\"加载耗时: {time.time() - start_time:.2f} 秒。\")"
   ],
   "id": "1f5a6493400a67fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段 2.1: 加载候选短语与Token列表 ---\n",
      "✅ 成功加载 3172 个 Gensim 候选短语。\n",
      "✅ 成功加载 7920 个 NER 候选实体。\n",
      "✅ 成功加载 1000 篇文章的Token列表。\n",
      "加载耗时: 0.06 秒。\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 3: 构建与合并数据框 (含频率计算优化)**\n",
    "\n",
    "**目标:** 将两个来源的候选短语统一到单个Pandas DataFrame中，并高效地为Gensim发现的短语计算其在整个语料库中的出现频率。\n",
    "\n",
    "**优化点:** 此处采用了Qwen建议的优化方案。我们不再使用效率较低的字符串`.count()`方法，而是通过**预处理候选短语**（按长度分组）和**滑动窗口遍历Token列表**的方式，显著提升频率计算的性能。"
   ],
   "id": "c99be61520b73247"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:06:00.257472Z",
     "start_time": "2025-08-02T10:05:59.903646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 3: 构建与合并数据框 (含频率计算优化) ---\n",
    "# =============================================================================\n",
    "\n",
    "if gensim_candidates and ner_candidates:\n",
    "    print(\"\\n--- 阶段 2.2: 构建与合并数据框 ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 3.1 处理Gensim数据\n",
    "    print(\"正在处理 Gensim 候选...\")\n",
    "    df_gensim = pd.DataFrame(gensim_candidates.items(), columns=['candidate_phrase', 'score'])\n",
    "    df_gensim['source'] = 'Gensim'\n",
    "    print(f\"  - Gensim DataFrame 创建完成，形状: {df_gensim.shape}\")\n",
    "\n",
    "    # 3.2 处理NER数据\n",
    "    print(\"正在处理 NER 候选...\")\n",
    "    df_ner = pd.DataFrame(ner_candidates.items(), columns=['candidate_phrase', 'frequency'])\n",
    "    df_ner['source'] = 'NER'\n",
    "    print(f\"  - NER DataFrame 创建完成，形状: {df_ner.shape}\")\n",
    "\n",
    "    # 3.3 [优化版] 高效计算Gensim短语频率\n",
    "    print(\"正在为 Gensim 短语高效计算频率 (优化版)...\")\n",
    "\n",
    "    # 预处理：将短语按其包含的token数量分组\n",
    "    gensim_phrases_by_len = {}\n",
    "    for phrase_str in df_gensim['candidate_phrase']:\n",
    "        tokens = phrase_str.split()\n",
    "        n = len(tokens)\n",
    "        if n > 1: # 只处理多词短语\n",
    "            if n not in gensim_phrases_by_len:\n",
    "                gensim_phrases_by_len[n] = set()\n",
    "            gensim_phrases_by_len[n].add(tuple(tokens)) # 使用元组以支持哈希\n",
    "\n",
    "    phrase_counts = Counter()\n",
    "\n",
    "    # 遍历所有文章的token列表\n",
    "    for doc_tokens_case_sensitive in tqdm(raw_token_lists, desc=\"高效计算Gensim频率\"):\n",
    "        doc_tokens = [token.lower() for token in doc_tokens_case_sensitive]\n",
    "        doc_len = len(doc_tokens)\n",
    "\n",
    "        # 对每种可能的短语长度进行滑动窗口检查\n",
    "        for n, phrases_set in gensim_phrases_by_len.items():\n",
    "            if doc_len < n:\n",
    "                continue\n",
    "            for i in range(doc_len - n + 1):\n",
    "                window_tuple = tuple(doc_tokens[i : i + n])\n",
    "                if window_tuple in phrases_set:\n",
    "                    phrase_counts[\" \".join(window_tuple)] += 1\n",
    "\n",
    "    # 将计算出的频率映射回df_gensim\n",
    "    df_gensim['frequency'] = df_gensim['candidate_phrase'].map(phrase_counts)\n",
    "    df_gensim['frequency'] = df_gensim['frequency'].fillna(0).astype(int)\n",
    "    print(\"  - Gensim 频率计算完成。\")\n",
    "\n",
    "    # 释放内存\n",
    "    del raw_token_lists, phrase_counts, gensim_phrases_by_len\n",
    "    gc.collect()\n",
    "\n",
    "    # 3.4 合并\n",
    "    print(\"正在合并两个来源的数据框...\")\n",
    "    df_candidates = pd.concat([df_ner, df_gensim], ignore_index=True)\n",
    "    print(f\"  - 合并完成。总候选数: {len(df_candidates)}\")\n",
    "    print(f\"构建与合并耗时: {time.time() - start_time:.2f} 秒。\")\n",
    "else:\n",
    "    print(\"候选数据未加载，跳过此块。\")"
   ],
   "id": "ebc7b25bf54395d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 阶段 2.2: 构建与合并数据框 ---\n",
      "正在处理 Gensim 候选...\n",
      "  - Gensim DataFrame 创建完成，形状: (3172, 3)\n",
      "正在处理 NER 候选...\n",
      "  - NER DataFrame 创建完成，形状: (7920, 3)\n",
      "正在为 Gensim 短语高效计算频率 (优化版)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "高效计算Gensim频率:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64c1eed1116a4e1683421963e77ebdd2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Gensim 频率计算完成。\n",
      "正在合并两个来源的数据框...\n",
      "  - 合并完成。总候选数: 11092\n",
      "构建与合并耗时: 0.35 秒。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 4: 清洗、过滤、排序与去重**\n",
    "\n",
    "**目标:** 对合并后的候选池进行“提纯”，移除明显无用的条目（如低频、含数字），并根据`NER优先`的原则进行去重，最后按`频率`降序排列，生成一份整洁、有序、待人工审核的列表。"
   ],
   "id": "5967af15648eda2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:06:00.302161Z",
     "start_time": "2025-08-02T10:06:00.278496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 4: 清洗、过滤、排序与去重 ---\n",
    "# =============================================================================\n",
    "\n",
    "if 'df_candidates' in locals() and not df_candidates.empty:\n",
    "    print(\"\\n--- 阶段 2.3: 清洗、过滤、排序与去重 ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 4.1 清洗与标准化\n",
    "    print(f\"原始候选数: {len(df_candidates)}\")\n",
    "    df_candidates['candidate_phrase'] = df_candidates['candidate_phrase'].str.strip()\n",
    "    df_candidates.dropna(subset=['candidate_phrase', 'frequency'], inplace=True)\n",
    "    df_candidates = df_candidates[df_candidates['candidate_phrase'] != '']\n",
    "    # 过滤掉包含数字的短语和过短的短语\n",
    "    df_candidates = df_candidates[~df_candidates['candidate_phrase'].str.contains(r'\\d')]\n",
    "    df_candidates = df_candidates[df_candidates['candidate_phrase'].str.len() > 3]\n",
    "    print(f\"清洗后候选数: {len(df_candidates)}\")\n",
    "\n",
    "    # 4.2 过滤低频项\n",
    "    df_candidates = df_candidates[df_candidates['frequency'] >= MIN_FREQUENCY_THRESHOLD]\n",
    "    print(f\"过滤低频项后 (频率 >= {MIN_FREQUENCY_THRESHOLD}): {len(df_candidates)}\")\n",
    "\n",
    "    # 4.3 去重与优先级处理 (核心逻辑)\n",
    "    # 将'source'列转为有序的Categorical类型，以定义优先级\n",
    "    df_candidates['source'] = pd.Categorical(df_candidates['source'], categories=['Gensim', 'NER'], ordered=True)\n",
    "    # 按 候选短语 分组，按 source 优先级排序 (NER在后，所以是更高优先级)\n",
    "    df_candidates.sort_values(['candidate_phrase', 'source'], ascending=[True, True], inplace=True)\n",
    "    # 去重，保留最后一个（即优先级最高的NER）\n",
    "    df_candidates.drop_duplicates(subset=['candidate_phrase'], keep='last', inplace=True)\n",
    "    print(f\"去重后 (NER优先): {len(df_candidates)}\")\n",
    "\n",
    "    # 4.4 最终排序\n",
    "    df_candidates.sort_values(by='frequency', ascending=False, inplace=True)\n",
    "    print(\"已按频率降序排列。\")\n",
    "\n",
    "    # 如果不是测试模式，则截取前 MAX_CANDIDATES_FOR_REVIEW 个\n",
    "    if not TEST_MODE and len(df_candidates) > MAX_CANDIDATES_FOR_REVIEW:\n",
    "        df_candidates = df_candidates.head(MAX_CANDIDATES_FOR_REVIEW)\n",
    "        print(f\"已截取频率最高的 {MAX_CANDIDATES_FOR_REVIEW} 个候选进行审核。\")\n",
    "\n",
    "    # 4.5 添加审核列\n",
    "    df_candidates['action_code'] = ''\n",
    "    df_candidates['standard_form'] = ''\n",
    "\n",
    "    # 调整列顺序以便审核\n",
    "    final_columns = ['candidate_phrase', 'frequency', 'source', 'score', 'action_code', 'standard_form']\n",
    "    df_final_review = df_candidates.reindex(columns=final_columns)\n",
    "\n",
    "    print(f\"最终待审核候选数: {len(df_final_review)}\")\n",
    "    print(f\"清洗与排序耗时: {time.time() - start_time:.2f} 秒。\")\n",
    "\n",
    "else:\n",
    "    print(\"候选DataFrame未创建，跳过此块。\")"
   ],
   "id": "c5f8c82fc39ec2d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 阶段 2.3: 清洗、过滤、排序与去重 ---\n",
      "原始候选数: 11092\n",
      "清洗后候选数: 10779\n",
      "过滤低频项后 (频率 >= 5): 3364\n",
      "去重后 (NER优先): 3142\n",
      "已按频率降序排列。\n",
      "最终待审核候选数: 3142\n",
      "清洗与排序耗时: 0.02 秒。\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 5: 保存待审核文件**\n",
    "\n",
    "**目标:** 将最终处理完成的、待人工审核的DataFrame保存为CSV文件。同时，根据运行环境，负责将结果从临时目录同步回项目的数据目录，并给出清晰的下一步操作指引。"
   ],
   "id": "9078d430b6e13fb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:06:00.341065Z",
     "start_time": "2025-08-02T10:06:00.325065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 5: 保存待审核文件 ---\n",
    "# =============================================================================\n",
    "\n",
    "if 'df_final_review' in locals() and not df_final_review.empty:\n",
    "    print(\"\\n--- 阶段 2.4: 保存待审核文件 ---\")\n",
    "    try:\n",
    "        # 修正FutureWarning\n",
    "        df_final_review['frequency'] = df_final_review['frequency'].fillna(0).astype(int)\n",
    "\n",
    "        df_final_review.to_csv(CANDIDATES_FOR_REVIEW_PATH, index=False, encoding='utf-8-sig') # 使用 utf-8-sig 确保Excel能正确打开\n",
    "\n",
    "        final_output_path = CANDIDATES_FOR_REVIEW_PATH\n",
    "        # 如果是DSW环境，将结果从/tmp同步回网络存储\n",
    "        if RUNNING_ENV == 'dsw':\n",
    "            print(f\"正在从 {CANDIDATES_FOR_REVIEW_PATH} 同步回 {CANDIDATES_FOR_REVIEW_ORIGINAL}...\")\n",
    "            shutil.copy(CANDIDATES_FOR_REVIEW_PATH, CANDIDATES_FOR_REVIEW_ORIGINAL)\n",
    "            final_output_path = CANDIDATES_FOR_REVIEW_ORIGINAL\n",
    "            print(\"同步完成。\")\n",
    "\n",
    "        print(f\"\\n✅✅✅ 成功生成待审核文件！ ✅✅✅\")\n",
    "        print(f\"文件路径: {final_output_path}\")\n",
    "        print(f\"包含 {len(df_final_review)} 条待审核的候选短语。\")\n",
    "        print(\"\\n下一步操作:\")\n",
    "        print(\"1. 打开这个CSV文件 (推荐使用Excel或Google Sheets)。\")\n",
    "        print(\"2. 根据需求文档的指引，在 'action_code' 和 'standard_form' 列填写您的决策。\")\n",
    "        print(\"3. 保存修改后的文件，为下一阶段 '规则生成与应用' 做准备。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 保存文件时发生错误: {e}\")\n",
    "else:\n",
    "    print(\"没有可供保存的待审核数据。\")"
   ],
   "id": "be14b7f4c731267",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 阶段 2.4: 保存待审核文件 ---\n",
      "\n",
      "✅✅✅ 成功生成待审核文件！ ✅✅✅\n",
      "文件路径: ../data_processed\\candidate_phrases_for_review.csv\n",
      "包含 3142 条待审核的候选短语。\n",
      "\n",
      "下一步操作:\n",
      "1. 打开这个CSV文件 (推荐使用Excel或Google Sheets)。\n",
      "2. 根据需求文档的指引，在 'action_code' 和 'standard_form' 列填写您的决策。\n",
      "3. 保存修改后的文件，为下一阶段 '规则生成与应用' 做准备。\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
