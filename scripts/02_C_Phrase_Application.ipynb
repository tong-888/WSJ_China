{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 1: 环境设置、库导入与路径管理**\n",
    "\n",
    "**目标:** 初始化项目环境，加载所有必需的库，并根据 `RUNNING_ENV` 和 `TEST_MODE` 智能配置所有输入输出文件的路径。此代码块负责读取人工审核的最终成果，并为大规模应用这些规则到新闻语料库上做好准备。"
   ],
   "id": "42d02ea2f96328ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:18.514305Z",
     "start_time": "2025-08-07T14:21:18.027203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 1: 环境设置、库导入与路径管理 ---\n",
    "# =============================================================================\n",
    "\n",
    "# 作用: 导入所有项目运行所需的Python库。\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import shutil\n",
    "import ahocorasick\n",
    "import gc\n",
    "\n",
    "# --- 核心配置区 ---\n",
    "# 作用: 全局控制参数，方便调试与切换运行模式。\n",
    "RUNNING_ENV = 'local'\n",
    "TEST_MODE = False\n",
    "TEST_SAMPLE_SIZE = 1000\n",
    "\n",
    "# --- 并行处理配置 ---\n",
    "# 作用: 智能检测CPU核心数，并为多进程处理设定合理的进程数。\n",
    "cpu_cores = psutil.cpu_count(logical=True)\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8)\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "\n",
    "# --- 路径智能管理 ---\n",
    "# 作用: 根据运行环境（本地或服务器）自动构建正确的文件路径。\n",
    "print(f\"检测到运行环境为: 【{RUNNING_ENV.upper()}】\")\n",
    "TEMP_DIR = '/tmp'\n",
    "\n",
    "if RUNNING_ENV == 'local':\n",
    "    print(\"使用 'local' 模式的相对路径。\")\n",
    "    BASE_DATA_PROCESSED_PATH = '../data_processed'\n",
    "elif RUNNING_ENV == 'dsw':\n",
    "    print(\"使用 'dsw' 模式的绝对路径。\")\n",
    "    BASE_DATA_PROCESSED_PATH = '/mnt/data/data_processed'\n",
    "    if not os.path.exists(TEMP_DIR): os.makedirs(TEMP_DIR)\n",
    "else:\n",
    "    raise ValueError(f\"未知的 RUNNING_ENV: '{RUNNING_ENV}'. 请选择 'local' 或 'dsw'。\")\n",
    "\n",
    "# 作用: 定义所有输入输出文件的“原始”存储位置。\n",
    "# 输入文件\n",
    "FINAL_CHINA_NEWS_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'final_china_news.csv')\n",
    "REVIEWED_CANDIDATES_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'candidate_phrases_for_review_reviewed.csv')\n",
    "EXPERT_RULES_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'expert_rules.csv')\n",
    "# 输出文件\n",
    "MERGE_DICT_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'merge_dict.pkl')\n",
    "NEW_STOPWORDS_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'new_stopwords.pkl')\n",
    "SOLIDIFIED_TEXT_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_solidified.pkl')\n",
    "\n",
    "# 作用: 初始化代码块实际使用的路径变量，默认为原始路径。\n",
    "FINAL_CHINA_NEWS_PATH = FINAL_CHINA_NEWS_ORIGINAL\n",
    "REVIEWED_CANDIDATES_PATH = REVIEWED_CANDIDATES_ORIGINAL\n",
    "EXPERT_RULES_PATH = EXPERT_RULES_ORIGINAL\n",
    "MERGE_DICT_PATH = MERGE_DICT_ORIGINAL\n",
    "NEW_STOPWORDS_PATH = NEW_STOPWORDS_ORIGINAL\n",
    "SOLIDIFIED_TEXT_PATH = SOLIDIFIED_TEXT_ORIGINAL\n",
    "\n",
    "# 作用: 在服务器环境下，智能地将高I/O负载的文件复制到本地临时目录进行操作，以提升速度和稳定性。\n",
    "if RUNNING_ENV == 'dsw':\n",
    "    print(\"DSW 环境模式已激活，将智能检查并使用本地临时目录 /tmp ...\")\n",
    "\n",
    "    def sync_to_tmp_if_needed(original_path, temp_dir):\n",
    "        if not os.path.exists(original_path):\n",
    "            raise FileNotFoundError(f\"关键输入文件在源路径不存在: {original_path}\")\n",
    "        filename = os.path.basename(original_path)\n",
    "        temp_path = os.path.join(temp_dir, filename)\n",
    "        if not os.path.exists(temp_path) or os.path.getsize(original_path) != os.path.getsize(temp_path):\n",
    "            print(f\"正在从 {original_path} 同步到 {temp_path}...\")\n",
    "            shutil.copy(original_path, temp_path)\n",
    "            print(\"同步完成。\")\n",
    "        else:\n",
    "            print(f\"临时文件 {temp_path} 已是最新，跳过同步。\")\n",
    "        return temp_path\n",
    "\n",
    "    try:\n",
    "        # 作用: 重定向路径变量，使后续代码透明地使用临时目录中的文件。\n",
    "        FINAL_CHINA_NEWS_PATH = sync_to_tmp_if_needed(FINAL_CHINA_NEWS_ORIGINAL, TEMP_DIR)\n",
    "        REVIEWED_CANDIDATES_PATH = sync_to_tmp_if_needed(REVIEWED_CANDIDATES_ORIGINAL, TEMP_DIR)\n",
    "        EXPERT_RULES_PATH = sync_to_tmp_if_needed(EXPERT_RULES_ORIGINAL, TEMP_DIR)\n",
    "\n",
    "        MERGE_DICT_PATH = os.path.join(TEMP_DIR, 'merge_dict.pkl')\n",
    "        NEW_STOPWORDS_PATH = os.path.join(TEMP_DIR, 'new_stopwords.pkl')\n",
    "        SOLIDIFIED_TEXT_PATH = os.path.join(TEMP_DIR, 'china_news_solidified.pkl')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ 致命错误: {e}\")\n",
    "        raise e\n",
    "\n",
    "# 作用: 在程序开始时清晰地展示所有最终配置，便于检查和追溯。\n",
    "print(\"\\n--- 环境准备 ---\")\n",
    "if TEST_MODE:\n",
    "    print(f\"🚀🚀🚀 运行在【快速测试模式】下，将处理前 {TEST_SAMPLE_SIZE} 行新闻！🚀🚀🚀\")\n",
    "else:\n",
    "    print(f\"🚢🚢🚢 运行在【完整数据模式】下，将处理所有新闻。🚢🚢🚢\")\n",
    "print(f\"新闻语料输入: {FINAL_CHINA_NEWS_PATH}\")\n",
    "print(f\"审核候选输入: {REVIEWED_CANDIDATES_PATH}\")\n",
    "print(f\"专家规则输入: {EXPERT_RULES_PATH}\")\n",
    "print(f\"合并词典输出: {MERGE_DICT_PATH}\")\n",
    "print(f\"停用词输出: {NEW_STOPWORDS_PATH}\")\n",
    "print(f\"固化文本输出: {SOLIDIFIED_TEXT_PATH}\")\n",
    "print(f\"将使用 {N_PROCESSES} 个进程进行并行处理。\")"
   ],
   "id": "b0c80bce054c950",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到运行环境为: 【LOCAL】\n",
      "使用 'local' 模式的相对路径。\n",
      "\n",
      "--- 环境准备 ---\n",
      "🚢🚢🚢 运行在【完整数据模式】下，将处理所有新闻。🚢🚢🚢\n",
      "新闻语料输入: ../data_processed\\final_china_news.csv\n",
      "审核候选输入: ../data_processed\\candidate_phrases_for_review_reviewed.csv\n",
      "专家规则输入: ../data_processed\\expert_rules.csv\n",
      "合并词典输出: ../data_processed\\merge_dict.pkl\n",
      "停用词输出: ../data_processed\\new_stopwords.pkl\n",
      "固化文本输出: ../data_processed\\china_news_solidified.pkl\n",
      "将使用 8 个进程进行并行处理。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 2: 从决策文件生成规则词典**\n",
    "\n",
    "**目标:** 读取经过人工审核的两个CSV文件 (`expert_rules.csv` 和 `candidate_phrases_for_review_reviewed.csv`)，并根据“专家优先”的原则，生成最终的合并词典 (`merge_dict`) 和自定义停用词集合 (`new_stopwords`)。"
   ],
   "id": "7e0dfe99e14d0fc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:18.823254Z",
     "start_time": "2025-08-07T14:21:18.696816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 2: 从决策文件生成规则词典 ---\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- 阶段 4.1: 开始生成规则词典与停用词列表 ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 作用: 初始化用于存储合并规则的字典和存储自定义停用词的集合。\n",
    "merge_dict = {}\n",
    "new_stopwords = set()\n",
    "\n",
    "try:\n",
    "    # 作用: 首先加载并处理专家规则文件（最高优先级）。\n",
    "    if os.path.exists(EXPERT_RULES_PATH):\n",
    "        df_expert = pd.read_csv(EXPERT_RULES_PATH)\n",
    "        for _, row in df_expert.iterrows():\n",
    "            phrase = str(row['phrase_to_merge']).strip().lower()\n",
    "            standard = str(row['standard_form']).strip()\n",
    "            if phrase and standard:\n",
    "                merge_dict[phrase] = standard\n",
    "        print(f\"✅ 从 {EXPERT_RULES_PATH} 加载了 {len(df_expert)} 条专家规则。\")\n",
    "    else:\n",
    "        print(f\"ℹ️ 未找到专家规则文件: {EXPERT_RULES_PATH}，跳过。\")\n",
    "\n",
    "    # 作用: 接着加载并处理经过人工审核的候选短语文件。\n",
    "    df_reviewed = pd.read_csv(REVIEWED_CANDIDATES_PATH)\n",
    "    print(f\"✅ 从 {REVIEWED_CANDIDATES_PATH} 加载了 {len(df_reviewed)} 条已审核的候选。\")\n",
    "\n",
    "    # 作用: 对'action_code'列进行清洗，确保其为整数类型，以便进行判断。\n",
    "    df_reviewed['action_code'] = pd.to_numeric(df_reviewed['action_code'], errors='coerce')\n",
    "    df_reviewed.dropna(subset=['action_code'], inplace=True)\n",
    "    df_reviewed['action_code'] = df_reviewed['action_code'].astype(int)\n",
    "\n",
    "    reviewed_merges = 0\n",
    "    reviewed_stopwords = 0\n",
    "\n",
    "    # 作用: 遍历审核后的文件，根据action_code将规则分配到合并字典或停用词集合。\n",
    "    for _, row in df_reviewed.iterrows():\n",
    "        phrase = str(row['candidate_phrase']).strip().lower()\n",
    "        if not phrase: continue\n",
    "\n",
    "        if row['action_code'] == 1: # 值为1表示合并\n",
    "            standard = str(row['standard_form']).strip()\n",
    "            # 确保专家规则优先，如果该短语已存在于字典中，则不覆盖。\n",
    "            if phrase and standard and phrase not in merge_dict:\n",
    "                merge_dict[phrase] = standard\n",
    "                reviewed_merges += 1\n",
    "        elif row['action_code'] == 2: # 值为2表示设为停用词\n",
    "            new_stopwords.update(phrase.split())\n",
    "            reviewed_stopwords += 1\n",
    "\n",
    "    print(f\"  - 从审核文件中处理了 {reviewed_merges} 条新的合并规则。\")\n",
    "    print(f\"  - 从审核文件中处理了 {reviewed_stopwords} 条新的停用词短语。\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"📊 最终规则库统计:\")\n",
    "    print(f\"   - 总合并规则数 (merge_dict): {len(merge_dict)}\")\n",
    "    print(f\"   - 总新增停用词数 (new_stopwords): {len(new_stopwords)}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 作用: 将最终生成的规则对象保存到磁盘，以便后续流程使用。\n",
    "    with open(MERGE_DICT_PATH, 'wb') as f:\n",
    "        pickle.dump(merge_dict, f)\n",
    "    print(f\"✅ [机器用] 合并词典已保存到: {MERGE_DICT_PATH}\")\n",
    "\n",
    "    with open(NEW_STOPWORDS_PATH, 'wb') as f:\n",
    "        pickle.dump(new_stopwords, f)\n",
    "    print(f\"✅ [机器用] 新增停用词列表已保存到: {NEW_STOPWORDS_PATH}\")\n",
    "\n",
    "    # 作用: 将合并字典保存为CSV格式，用于人工检查和存档。\n",
    "    MERGE_DICT_CSV_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'merge_dict_for_review.csv')\n",
    "    df_merge_dict = pd.DataFrame(list(merge_dict.items()), columns=['phrase_to_merge', 'standard_form'])\n",
    "    df_merge_dict.to_csv(MERGE_DICT_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ [人类用] 合并词典已保存到CSV: {MERGE_DICT_CSV_PATH}\")\n",
    "\n",
    "    print(f\"规则生成耗时: {time.time() - start_time:.2f} 秒。\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 错误: 缺少必要的输入文件: {e.filename}。\")\n",
    "    print(\"请确保已完成人工审核并正确放置文件。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 处理规则文件时发生未知错误: {e}\")"
   ],
   "id": "1445db563ae0930b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 阶段 4.1: 开始生成规则词典与停用词列表 ---\n",
      "ℹ️ 未找到专家规则文件: ../data_processed\\expert_rules.csv，跳过。\n",
      "✅ 从 ../data_processed\\candidate_phrases_for_review_reviewed.csv 加载了 3140 条已审核的候选。\n",
      "  - 从审核文件中处理了 1826 条新的合并规则。\n",
      "  - 从审核文件中处理了 846 条新的停用词短语。\n",
      "------------------------------\n",
      "📊 最终规则库统计:\n",
      "   - 总合并规则数 (merge_dict): 1826\n",
      "   - 总新增停用词数 (new_stopwords): 736\n",
      "------------------------------\n",
      "✅ [机器用] 合并词典已保存到: ../data_processed\\merge_dict.pkl\n",
      "✅ [机器用] 新增停用词列表已保存到: ../data_processed\\new_stopwords.pkl\n",
      "✅ [人类用] 合并词典已保存到CSV: ../data_processed\\merge_dict_for_review.csv\n",
      "规则生成耗时: 0.12 秒。\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 3: 构建高效替换引擎**\n",
    "\n",
    "**目标:** 利用 `pyahocorasick` 库，将上一步生成的、可能非常庞大的 `merge_dict` 编译成一个Aho-Corasick自动机。这个自动机是实现大规模、高性能文本替换的关键，其查找效率远超常规的循环或正则表达式方法。"
   ],
   "id": "9cf48a17e6f83d66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:18.861346Z",
     "start_time": "2025-08-07T14:21:18.838345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 3: 构建高效替换引擎 ---\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- 阶段 4.2a: 构建 Aho-Corasick 高效替换引擎 ---\")\n",
    "\n",
    "automaton = None\n",
    "\n",
    "# 作用: 将合并字典编译成一个Aho-Corasick自动机，用于后续大规模、高性能的多模式字符串匹配和替换。\n",
    "if 'merge_dict' in locals() and merge_dict:\n",
    "    automaton = ahocorasick.Automaton()\n",
    "\n",
    "    # 作用: 将每个待查找的短语及其对应的(长度, 标准形式)存入自动机。\n",
    "    # 存入长度是为了在匹配时直接获取，避免重复计算，提升效率。\n",
    "    for phrase, standard_form in tqdm(merge_dict.items(), desc=\"构建自动机\"):\n",
    "        automaton.add_word(phrase, (len(phrase), standard_form))\n",
    "\n",
    "    # 作用: 完成自动机的构建，使其进入可匹配状态。\n",
    "    automaton.make_automaton()\n",
    "    print(f\"✅ Aho-Corasick 自动机构建完成，包含 {len(merge_dict)} 条规则。\")\n",
    "else:\n",
    "    print(\"⚠️ 警告: 合并词典 (merge_dict) 为空或未定义，无法构建替换引擎。\")"
   ],
   "id": "5d428750433dce89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 阶段 4.2a: 构建 Aho-Corasick 高效替换引擎 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "构建自动机:   0%|          | 0/1826 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99227b41a6484bf99d95e87058fe0ff6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Aho-Corasick 自动机构建完成，包含 1826 条规则。\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 4: 应用规则固化文本**\n",
    "\n",
    "**目标:** 加载完整的中国新闻语料库，并应用上一步构建的自动机，对每一篇文章的`CONTENT`进行实体和短语的“固化”替换。这是整个流程的核心执行步骤。"
   ],
   "id": "cde2e3a395fa5e5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:40.188512Z",
     "start_time": "2025-08-07T14:21:18.892347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 4: 应用规则固化文本 (带全方位验证监控) ---\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- 阶段 4.2b: 开始单线程文本固化处理 ---\")\n",
    "start_time_solidify = time.time()\n",
    "\n",
    "# 作用: 初始化函数内部的监控计数器。\n",
    "def initialize_counters():\n",
    "    solidify_text_definitively.total_matches = 0\n",
    "    solidify_text_definitively.processed_with_matches = 0\n",
    "\n",
    "def solidify_text_definitively(text, automaton_obj):\n",
    "    \"\"\"\n",
    "    作用：对单个文本字符串应用Aho-Corasick自动机进行实体固化。\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not automaton_obj:\n",
    "        return \"\"\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    parts = []\n",
    "    last_end = 0\n",
    "\n",
    "    all_matches = []\n",
    "    for end_index, (phrase_len, standard_form) in automaton_obj.iter(text_lower):\n",
    "        start_index = end_index - phrase_len + 1\n",
    "        is_start_boundary = (start_index == 0) or (not text_lower[start_index - 1].isalnum())\n",
    "        is_end_boundary = (end_index + 1 == len(text_lower)) or (not text_lower[end_index + 1].isalnum())\n",
    "\n",
    "        if is_start_boundary and is_end_boundary:\n",
    "            all_matches.append((start_index, end_index, standard_form))\n",
    "\n",
    "    # 作用: [监控点] 如果找到了任何匹配项，则更新计数器。\n",
    "    if len(all_matches) > 0:\n",
    "        solidify_text_definitively.total_matches += len(all_matches)\n",
    "        solidify_text_definitively.processed_with_matches += 1\n",
    "\n",
    "    if not all_matches:\n",
    "        return text\n",
    "\n",
    "    final_matches = []\n",
    "    i = 0\n",
    "    while i < len(all_matches):\n",
    "        current_best_match = all_matches[i]\n",
    "        j = i + 1\n",
    "        while j < len(all_matches) and all_matches[j][0] <= current_best_match[1]:\n",
    "            if (all_matches[j][1] - all_matches[j][0]) > (current_best_match[1] - current_best_match[0]):\n",
    "                current_best_match = all_matches[j]\n",
    "            j += 1\n",
    "\n",
    "        final_matches.append(current_best_match)\n",
    "        i = j\n",
    "\n",
    "    for start_index, end_index, standard_form in final_matches:\n",
    "        if start_index >= last_end:\n",
    "            parts.append(text[last_end:start_index])\n",
    "            parts.append(standard_form)\n",
    "            last_end = end_index + 1\n",
    "\n",
    "    parts.append(text[last_end:])\n",
    "\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "try:\n",
    "    # 作用：加载筛选后的新闻数据。\n",
    "    if not os.path.exists(FINAL_CHINA_NEWS_PATH):\n",
    "         raise FileNotFoundError(f\"新闻语料文件未找到: {FINAL_CHINA_NEWS_PATH}\")\n",
    "\n",
    "    read_nrows = TEST_SAMPLE_SIZE if TEST_MODE else None\n",
    "    print(\"正在加载新闻数据...\")\n",
    "    df = pd.read_csv(FINAL_CHINA_NEWS_PATH, nrows=read_nrows)\n",
    "\n",
    "    # 作用: [验证点] 验证加载的数据量和基本信息。\n",
    "    print(f\"✅ 成功加载 {len(df)} 篇新闻进行处理。\")\n",
    "    if not df.empty:\n",
    "        print(f\"📅 数据时间范围: {df['DATE'].min()} 到 {df['DATE'].max()}\")\n",
    "        print(f\"📊 数据列信息: {list(df.columns)}\")\n",
    "\n",
    "    # 作用：确保'TITLE'和'CONTENT'列为字符串类型，并将NaN值转换为空字符串。\n",
    "    df['TITLE'] = df['TITLE'].astype(str).fillna('')\n",
    "    df['CONTENT'] = df['CONTENT'].astype(str).fillna('')\n",
    "\n",
    "    # 作用：应用实体固化规则。\n",
    "    if 'automaton' in locals() and automaton:\n",
    "        print(\"\\n开始文本固化处理...\")\n",
    "\n",
    "        # 作用: [验证点] 打印计划处理的文本总数。\n",
    "        total_texts = len(df)\n",
    "        print(f\"📈 计划处理文本数量: {total_texts}\")\n",
    "\n",
    "        # 作用: 在处理前重置/初始化函数内的监控计数器。\n",
    "        initialize_counters()\n",
    "\n",
    "        # 作用：将TITLE和CONTENT在内存中动态合并成一个文本流，并应用固化函数。\n",
    "        text_iterator = (title + ' . ' + content for title, content in zip(df['TITLE'], df['CONTENT']))\n",
    "\n",
    "        tqdm.pandas(desc=\"固化文本\")\n",
    "        df['content_solidified'] = pd.Series(text_iterator, index=df.index).progress_apply(\n",
    "            lambda text: solidify_text_definitively(text, automaton)\n",
    "        )\n",
    "\n",
    "        print(f\"✅ 文本固化完成！耗时: {(time.time() - start_time_solidify) / 60:.2f} 分钟。\")\n",
    "\n",
    "        # 作用: [验证点] 打印详尽的处理结果统计报告。\n",
    "        processed_count = df['content_solidified'].count()\n",
    "        print(f\"\\n--- 处理结果验证 ---\")\n",
    "        print(f\"✅ 实际处理文本数量: {processed_count}\")\n",
    "        if processed_count == total_texts:\n",
    "            print(\"✅ 完整性检查通过：所有文本行均已成功处理。\")\n",
    "        else:\n",
    "            print(f\"⚠️ 完整性检查失败：处理数量不匹配！原始: {total_texts}, 处理后非空: {processed_count}\")\n",
    "\n",
    "        original_merged_text = df['TITLE'] + ' . ' + df['CONTENT']\n",
    "        changed_count = (df['content_solidified'] != original_merged_text).sum()\n",
    "\n",
    "        print(f\"\\n--- 固化效果统计 ---\")\n",
    "        print(f\"🔄 文本被修改（固化）的数量: {changed_count} / {total_texts}\")\n",
    "        if total_texts > 0:\n",
    "            print(f\"📊 固化率: {changed_count / total_texts * 100:.2f}%\")\n",
    "        print(f\"🔢 函数内部统计: 在 {solidify_text_definitively.processed_with_matches} 篇被修改的文章中，总共执行了 {solidify_text_definitively.total_matches} 次短语匹配。\")\n",
    "\n",
    "        # 作用：随机抽样文章，打印其原始与固化后的内容，用于人工快速验证。\n",
    "        print(\"\\n--- 抽样检查结果 ---\")\n",
    "        sample_size = min(5, len(df))\n",
    "        if sample_size > 0:\n",
    "            pd.set_option('display.max_colwidth', 400)\n",
    "            for i, row in df.sample(sample_size).iterrows():\n",
    "                print(f\"--- 文章 {i} ---\")\n",
    "                print(f\"【原始TITLE】: {str(row['TITLE'])}\")\n",
    "                print(f\"【原始CONTENT】: {str(row['CONTENT'])[:300]}...\")\n",
    "                print(f\"【固化后】: {str(row['content_solidified'])[:400]}...\")\n",
    "                print(\"-\" * 20)\n",
    "    else:\n",
    "        print(\"\\n自动机未构建，跳过文本固化。将直接合并TITLE和CONTENT。\")\n",
    "        df['content_solidified'] = df['TITLE'] + ' . ' + df['CONTENT']\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 错误: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 固化文本过程中发生未知错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "id": "44208e52ffdfebb9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 阶段 4.2b: 开始单线程文本固化处理 ---\n",
      "正在加载新闻数据...\n",
      "✅ 成功加载 180630 篇新闻进行处理。\n",
      "📅 数据时间范围: 1984-01-02 到 2025-03-15\n",
      "📊 数据列信息: ['DATE', 'TITLE', 'CONTENT']\n",
      "\n",
      "开始文本固化处理...\n",
      "📈 计划处理文本数量: 180630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "固化文本:   0%|          | 0/180630 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f0f8bc968874291980695e2445172ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 文本固化完成！耗时: 0.35 分钟。\n",
      "\n",
      "--- 处理结果验证 ---\n",
      "✅ 实际处理文本数量: 180630\n",
      "✅ 完整性检查通过：所有文本行均已成功处理。\n",
      "\n",
      "--- 固化效果统计 ---\n",
      "🔄 文本被修改（固化）的数量: 173985 / 180630\n",
      "📊 固化率: 96.32%\n",
      "🔢 函数内部统计: 在 173985 篇被修改的文章中，总共执行了 2246311 次短语匹配。\n",
      "\n",
      "--- 抽样检查结果 ---\n",
      "--- 文章 99837 ---\n",
      "【原始TITLE】: Are Plastics Making Us Fat?\n",
      "【原始CONTENT】: Weight-loss crazes are as American as apple pie -- make that Slim Fast shakes. But despite our countless diet fads, the obesity rate has more than doubled in the last 30 years. Perhaps that's because Americans haven't tried \"The New American Diet,\" which promises to reveal \"why your weight isn't you...\n",
      "【固化后】: Are Plastics Making Us Fat? . Weight-loss crazes are as American as apple pie -- make that Slim Fast shakes. But despite our countless diet fads, the obesity rate has more than doubled in the last 30 years. Perhaps that's because Americans haven't tried \"The New American Diet,\" which promises to reveal \"why your weight isn't your fault\" and reverse \"the obesogen effect.\" Haven't heard of the \"obes...\n",
      "--------------------\n",
      "--- 文章 161841 ---\n",
      "【原始TITLE】: Business & Finance\n",
      "【原始CONTENT】: Palantir is expected to fetch a lofty valuation in its transition to a public company despite an unusually aggressive governance structure, in the latest sign of investors' voracious appetite for new shares. The Blue Cross Blue Shield insurance group has negotiated a tentative settlement in an antit...\n",
      "【固化后】: Business & Finance . Palantir is expected to fetch a lofty valuation in its transition to a public company despite an unusually aggressive governance structure, in the latest sign of investors' voracious appetite for new shares. The Blue Cross Blue Shield insurance group has negotiated a tentative settlement in an antitrust suit_filed on behalf of customers, a deal that would require a payout of a...\n",
      "--------------------\n",
      "--- 文章 16674 ---\n",
      "【原始TITLE】: Small Business Summit Is in the Works; SBICs Probed\n",
      "【原始CONTENT】: PLANNING BEGINS for a White House Conference on Small Business in 1994. Legislation calling for the conference, a sort of national summit meeting on small-business issues, is in the early stages of congressional action. Barring unforeseen problems, lawmakers could send a bill to President Bush by la...\n",
      "【固化后】: Small Business Summit Is in the Works; SBICs Probed . PLANNING BEGINS for a white_house Conference on Small Business in 1994. Legislation calling for the conference, a sort of national summit_meeting on small-business issues, is in the early stages of congressional action. Barring unforeseen problems, lawmakers could send a bill to President Bush by late summer. The administration and entrepreneur...\n",
      "--------------------\n",
      "--- 文章 53481 ---\n",
      "【原始TITLE】: New Securities Issues\n",
      "【原始CONTENT】: The following were among yesterday's offerings and pricings in U.S. and non-U.S. capital markets, with terms and syndicate manager, based on information provided by Dow Jones Newswires and Factiva. (A basis point is one-hundredth of a percentage point; 100 basis points equals a percentage point.) CO...\n",
      "【固化后】: New Securities Issues . The following were among yesterday's offerings and pricings in U.S. and non-U.S. capital_markets, with terms and syndicate manager, based on information provided by dow_jones Newswires and Factiva. (A basis point is one-hundredth of a percentage_point; 100 basis points equals a percentage_point.) CORPORATE Ambac Financial Group Inc. -- $200 million offering of senior notes ...\n",
      "--------------------\n",
      "--- 文章 133315 ---\n",
      "【原始TITLE】: Yahoo Investors Tighten Screws --- Groups push to slash workforce, replace CEO Mayer, find buyer for core Web business\n",
      "【原始CONTENT】: Corrections & Amplifications Eric Jackson, managing director of SpringOwl Asset Management LLC, has about 22,000 Twitter followers. A Business & Tech article Monday about investor pressure on Yahoo Inc. incorrectly said he has 67,000. (WSJ December 15, 2015) Yahoo Inc. is facing new pressure from in...\n",
      "【固化后】: Yahoo Investors Tighten Screws --- Groups push to slash workforce, replace CEO Mayer, find buyer for core Web business . Corrections & Amplifications Eric Jackson, managing_director of SpringOwl Asset Management LLC, has about 22,000 Twitter followers. A Business & Tech article Monday about investor pressure on Yahoo Inc. incorrectly said he has 67,000. (WSJ December 15, 2015) Yahoo Inc. is facing...\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **块 5: 保存最终结果**\n",
    "\n",
    "**目标:** 将包含新增的`content_solidified`列的DataFrame完整保存到磁盘。推荐使用Pickle格式，因为它能无损、高效地存储Pandas对象，为后续的深度清洗步骤（`04_...`）提供一个干净的起点。"
   ],
   "id": "98bf445ab8353e63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:54.762739Z",
     "start_time": "2025-08-07T14:21:40.218834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- 块 5: 保存最终结果 ---\n",
    "# =============================================================================\n",
    "\n",
    "# 作用: 负责对固化后的结果进行检查，并保存为最终的Pickle和CSV文件。\n",
    "if 'df' in locals() and 'content_solidified' in df.columns:\n",
    "    print(\"\\n--- 阶段 4.2c: 保存固化后的结果 ---\")\n",
    "    try:\n",
    "        # 作用: 定义两种格式的输出路径。\n",
    "        SOLIDIFIED_TEXT_PKL_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_solidified.pkl')\n",
    "        SOLIDIFIED_TEXT_CSV_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_solidified_for_review.csv')\n",
    "\n",
    "        # 作用: 保存为Pickle格式，用于后续Python脚本高效读取。\n",
    "        df.to_pickle(SOLIDIFIED_TEXT_PKL_PATH)\n",
    "        print(f\"✅ [机器用] 包含固化文本的DataFrame已保存到Pickle: {SOLIDIFIED_TEXT_PKL_PATH}\")\n",
    "\n",
    "        # 作用: 保存为CSV格式，用于人工查阅和审计。\n",
    "        df.to_csv(SOLIDIFIED_TEXT_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "        print(f\"✅ [人类用] 包含固化文本的DataFrame已保存到CSV: {SOLIDIFIED_TEXT_CSV_PATH}\")\n",
    "\n",
    "        # 作用: 在服务器环境下，将产出文件从临时目录同步回项目的数据目录。\n",
    "        if RUNNING_ENV == 'dsw':\n",
    "            print(f\"DSW模式：正在将结果同步回项目目录...\")\n",
    "            shutil.copy(SOLIDIFIED_TEXT_PKL_PATH, SOLIDIFIED_TEXT_ORIGINAL)\n",
    "            shutil.copy(SOLIDIFIED_TEXT_CSV_PATH,\n",
    "                        os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_solidified_for_review.csv'))\n",
    "            print(\"同步完成。\")\n",
    "\n",
    "        # 作用: 打印清晰的总结和下一步操作指引。\n",
    "        print(f\"\\n✅✅✅ “实体固化”流程全部完成！ ✅✅✅\")\n",
    "        print(f\"产出文件:\")\n",
    "        print(f\"  - {SOLIDIFIED_TEXT_PKL_PATH}\")\n",
    "        print(f\"  - {SOLIDIFIED_TEXT_CSV_PATH}\")\n",
    "        print(\"\\n下一步是运行 '03_Deep_Text_Cleaning.ipynb'，对'content_solidified'列进行最终的标准化和噪声移除。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 保存文件时发生错误: {e}\")\n",
    "else:\n",
    "    print(\"没有可供保存的固化文本数据。\")"
   ],
   "id": "3bfb0d249f79a8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 阶段 4.2c: 保存固化后的结果 ---\n",
      "✅ [机器用] 包含固化文本的DataFrame已保存到Pickle: ../data_processed\\china_news_solidified.pkl\n",
      "✅ [人类用] 包含固化文本的DataFrame已保存到CSV: ../data_processed\\china_news_solidified_for_review.csv\n",
      "\n",
      "✅✅✅ “实体固化”流程全部完成！ ✅✅✅\n",
      "产出文件:\n",
      "  - ../data_processed\\china_news_solidified.pkl\n",
      "  - ../data_processed\\china_news_solidified_for_review.csv\n",
      "\n",
      "下一步是运行 '03_Deep_Text_Cleaning.ipynb'，对'content_solidified'列进行最终的标准化和噪声移除。\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:54.780154Z",
     "start_time": "2025-08-07T14:21:54.777843Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d08fea711a03f705",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
