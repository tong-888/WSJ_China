{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 1: ç¯å¢ƒè®¾ç½®ã€åº“å¯¼å…¥ä¸è·¯å¾„ç®¡ç†**\n",
    "\n",
    "**ç›®æ ‡:** åˆå§‹åŒ–é¡¹ç›®ç¯å¢ƒï¼ŒåŠ è½½æ‰€æœ‰å¿…éœ€çš„åº“ï¼Œå¹¶æ ¹æ® `RUNNING_ENV` å’Œ `TEST_MODE` æ™ºèƒ½é…ç½®æ‰€æœ‰è¾“å…¥è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„ã€‚æ­¤ä»£ç å—è´Ÿè´£è¯»å–äººå·¥å®¡æ ¸çš„æœ€ç»ˆæˆæœï¼Œå¹¶ä¸ºå¤§è§„æ¨¡åº”ç”¨è¿™äº›è§„åˆ™åˆ°æ–°é—»è¯­æ–™åº“ä¸Šåšå¥½å‡†å¤‡ã€‚"
   ],
   "id": "42d02ea2f96328ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:18.514305Z",
     "start_time": "2025-08-07T14:21:18.027203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 1: ç¯å¢ƒè®¾ç½®ã€åº“å¯¼å…¥ä¸è·¯å¾„ç®¡ç† ---\n",
    "# =============================================================================\n",
    "\n",
    "# ä½œç”¨: å¯¼å…¥æ‰€æœ‰é¡¹ç›®è¿è¡Œæ‰€éœ€çš„Pythonåº“ã€‚\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "import shutil\n",
    "import ahocorasick\n",
    "import gc\n",
    "\n",
    "# --- æ ¸å¿ƒé…ç½®åŒº ---\n",
    "# ä½œç”¨: å…¨å±€æ§åˆ¶å‚æ•°ï¼Œæ–¹ä¾¿è°ƒè¯•ä¸åˆ‡æ¢è¿è¡Œæ¨¡å¼ã€‚\n",
    "RUNNING_ENV = 'local'\n",
    "TEST_MODE = False\n",
    "TEST_SAMPLE_SIZE = 1000\n",
    "\n",
    "# --- å¹¶è¡Œå¤„ç†é…ç½® ---\n",
    "# ä½œç”¨: æ™ºèƒ½æ£€æµ‹CPUæ ¸å¿ƒæ•°ï¼Œå¹¶ä¸ºå¤šè¿›ç¨‹å¤„ç†è®¾å®šåˆç†çš„è¿›ç¨‹æ•°ã€‚\n",
    "cpu_cores = psutil.cpu_count(logical=True)\n",
    "N_PROCESSES = min(cpu_cores - 1 if cpu_cores > 1 else 1, 8)\n",
    "if N_PROCESSES < 1: N_PROCESSES = 1\n",
    "\n",
    "# --- è·¯å¾„æ™ºèƒ½ç®¡ç† ---\n",
    "# ä½œç”¨: æ ¹æ®è¿è¡Œç¯å¢ƒï¼ˆæœ¬åœ°æˆ–æœåŠ¡å™¨ï¼‰è‡ªåŠ¨æ„å»ºæ­£ç¡®çš„æ–‡ä»¶è·¯å¾„ã€‚\n",
    "print(f\"æ£€æµ‹åˆ°è¿è¡Œç¯å¢ƒä¸º: ã€{RUNNING_ENV.upper()}ã€‘\")\n",
    "TEMP_DIR = '/tmp'\n",
    "\n",
    "if RUNNING_ENV == 'local':\n",
    "    print(\"ä½¿ç”¨ 'local' æ¨¡å¼çš„ç›¸å¯¹è·¯å¾„ã€‚\")\n",
    "    BASE_DATA_PROCESSED_PATH = '../data_processed'\n",
    "elif RUNNING_ENV == 'dsw':\n",
    "    print(\"ä½¿ç”¨ 'dsw' æ¨¡å¼çš„ç»å¯¹è·¯å¾„ã€‚\")\n",
    "    BASE_DATA_PROCESSED_PATH = '/mnt/data/data_processed'\n",
    "    if not os.path.exists(TEMP_DIR): os.makedirs(TEMP_DIR)\n",
    "else:\n",
    "    raise ValueError(f\"æœªçŸ¥çš„ RUNNING_ENV: '{RUNNING_ENV}'. è¯·é€‰æ‹© 'local' æˆ– 'dsw'ã€‚\")\n",
    "\n",
    "# ä½œç”¨: å®šä¹‰æ‰€æœ‰è¾“å…¥è¾“å‡ºæ–‡ä»¶çš„â€œåŸå§‹â€å­˜å‚¨ä½ç½®ã€‚\n",
    "# è¾“å…¥æ–‡ä»¶\n",
    "FINAL_CHINA_NEWS_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'final_china_news.csv')\n",
    "REVIEWED_CANDIDATES_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'candidate_phrases_for_review_reviewed.csv')\n",
    "EXPERT_RULES_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'expert_rules.csv')\n",
    "# è¾“å‡ºæ–‡ä»¶\n",
    "MERGE_DICT_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'merge_dict.pkl')\n",
    "NEW_STOPWORDS_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'new_stopwords.pkl')\n",
    "SOLIDIFIED_TEXT_ORIGINAL = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_solidified.pkl')\n",
    "\n",
    "# ä½œç”¨: åˆå§‹åŒ–ä»£ç å—å®é™…ä½¿ç”¨çš„è·¯å¾„å˜é‡ï¼Œé»˜è®¤ä¸ºåŸå§‹è·¯å¾„ã€‚\n",
    "FINAL_CHINA_NEWS_PATH = FINAL_CHINA_NEWS_ORIGINAL\n",
    "REVIEWED_CANDIDATES_PATH = REVIEWED_CANDIDATES_ORIGINAL\n",
    "EXPERT_RULES_PATH = EXPERT_RULES_ORIGINAL\n",
    "MERGE_DICT_PATH = MERGE_DICT_ORIGINAL\n",
    "NEW_STOPWORDS_PATH = NEW_STOPWORDS_ORIGINAL\n",
    "SOLIDIFIED_TEXT_PATH = SOLIDIFIED_TEXT_ORIGINAL\n",
    "\n",
    "# ä½œç”¨: åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸‹ï¼Œæ™ºèƒ½åœ°å°†é«˜I/Oè´Ÿè½½çš„æ–‡ä»¶å¤åˆ¶åˆ°æœ¬åœ°ä¸´æ—¶ç›®å½•è¿›è¡Œæ“ä½œï¼Œä»¥æå‡é€Ÿåº¦å’Œç¨³å®šæ€§ã€‚\n",
    "if RUNNING_ENV == 'dsw':\n",
    "    print(\"DSW ç¯å¢ƒæ¨¡å¼å·²æ¿€æ´»ï¼Œå°†æ™ºèƒ½æ£€æŸ¥å¹¶ä½¿ç”¨æœ¬åœ°ä¸´æ—¶ç›®å½• /tmp ...\")\n",
    "\n",
    "    def sync_to_tmp_if_needed(original_path, temp_dir):\n",
    "        if not os.path.exists(original_path):\n",
    "            raise FileNotFoundError(f\"å…³é”®è¾“å…¥æ–‡ä»¶åœ¨æºè·¯å¾„ä¸å­˜åœ¨: {original_path}\")\n",
    "        filename = os.path.basename(original_path)\n",
    "        temp_path = os.path.join(temp_dir, filename)\n",
    "        if not os.path.exists(temp_path) or os.path.getsize(original_path) != os.path.getsize(temp_path):\n",
    "            print(f\"æ­£åœ¨ä» {original_path} åŒæ­¥åˆ° {temp_path}...\")\n",
    "            shutil.copy(original_path, temp_path)\n",
    "            print(\"åŒæ­¥å®Œæˆã€‚\")\n",
    "        else:\n",
    "            print(f\"ä¸´æ—¶æ–‡ä»¶ {temp_path} å·²æ˜¯æœ€æ–°ï¼Œè·³è¿‡åŒæ­¥ã€‚\")\n",
    "        return temp_path\n",
    "\n",
    "    try:\n",
    "        # ä½œç”¨: é‡å®šå‘è·¯å¾„å˜é‡ï¼Œä½¿åç»­ä»£ç é€æ˜åœ°ä½¿ç”¨ä¸´æ—¶ç›®å½•ä¸­çš„æ–‡ä»¶ã€‚\n",
    "        FINAL_CHINA_NEWS_PATH = sync_to_tmp_if_needed(FINAL_CHINA_NEWS_ORIGINAL, TEMP_DIR)\n",
    "        REVIEWED_CANDIDATES_PATH = sync_to_tmp_if_needed(REVIEWED_CANDIDATES_ORIGINAL, TEMP_DIR)\n",
    "        EXPERT_RULES_PATH = sync_to_tmp_if_needed(EXPERT_RULES_ORIGINAL, TEMP_DIR)\n",
    "\n",
    "        MERGE_DICT_PATH = os.path.join(TEMP_DIR, 'merge_dict.pkl')\n",
    "        NEW_STOPWORDS_PATH = os.path.join(TEMP_DIR, 'new_stopwords.pkl')\n",
    "        SOLIDIFIED_TEXT_PATH = os.path.join(TEMP_DIR, 'china_news_solidified.pkl')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âŒ è‡´å‘½é”™è¯¯: {e}\")\n",
    "        raise e\n",
    "\n",
    "# ä½œç”¨: åœ¨ç¨‹åºå¼€å§‹æ—¶æ¸…æ™°åœ°å±•ç¤ºæ‰€æœ‰æœ€ç»ˆé…ç½®ï¼Œä¾¿äºæ£€æŸ¥å’Œè¿½æº¯ã€‚\n",
    "print(\"\\n--- ç¯å¢ƒå‡†å¤‡ ---\")\n",
    "if TEST_MODE:\n",
    "    print(f\"ğŸš€ğŸš€ğŸš€ è¿è¡Œåœ¨ã€å¿«é€Ÿæµ‹è¯•æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†å‰ {TEST_SAMPLE_SIZE} è¡Œæ–°é—»ï¼ğŸš€ğŸš€ğŸš€\")\n",
    "else:\n",
    "    print(f\"ğŸš¢ğŸš¢ğŸš¢ è¿è¡Œåœ¨ã€å®Œæ•´æ•°æ®æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†æ‰€æœ‰æ–°é—»ã€‚ğŸš¢ğŸš¢ğŸš¢\")\n",
    "print(f\"æ–°é—»è¯­æ–™è¾“å…¥: {FINAL_CHINA_NEWS_PATH}\")\n",
    "print(f\"å®¡æ ¸å€™é€‰è¾“å…¥: {REVIEWED_CANDIDATES_PATH}\")\n",
    "print(f\"ä¸“å®¶è§„åˆ™è¾“å…¥: {EXPERT_RULES_PATH}\")\n",
    "print(f\"åˆå¹¶è¯å…¸è¾“å‡º: {MERGE_DICT_PATH}\")\n",
    "print(f\"åœç”¨è¯è¾“å‡º: {NEW_STOPWORDS_PATH}\")\n",
    "print(f\"å›ºåŒ–æ–‡æœ¬è¾“å‡º: {SOLIDIFIED_TEXT_PATH}\")\n",
    "print(f\"å°†ä½¿ç”¨ {N_PROCESSES} ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚\")"
   ],
   "id": "b0c80bce054c950",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æµ‹åˆ°è¿è¡Œç¯å¢ƒä¸º: ã€LOCALã€‘\n",
      "ä½¿ç”¨ 'local' æ¨¡å¼çš„ç›¸å¯¹è·¯å¾„ã€‚\n",
      "\n",
      "--- ç¯å¢ƒå‡†å¤‡ ---\n",
      "ğŸš¢ğŸš¢ğŸš¢ è¿è¡Œåœ¨ã€å®Œæ•´æ•°æ®æ¨¡å¼ã€‘ä¸‹ï¼Œå°†å¤„ç†æ‰€æœ‰æ–°é—»ã€‚ğŸš¢ğŸš¢ğŸš¢\n",
      "æ–°é—»è¯­æ–™è¾“å…¥: ../data_processed\\final_china_news.csv\n",
      "å®¡æ ¸å€™é€‰è¾“å…¥: ../data_processed\\candidate_phrases_for_review_reviewed.csv\n",
      "ä¸“å®¶è§„åˆ™è¾“å…¥: ../data_processed\\expert_rules.csv\n",
      "åˆå¹¶è¯å…¸è¾“å‡º: ../data_processed\\merge_dict.pkl\n",
      "åœç”¨è¯è¾“å‡º: ../data_processed\\new_stopwords.pkl\n",
      "å›ºåŒ–æ–‡æœ¬è¾“å‡º: ../data_processed\\china_news_solidified.pkl\n",
      "å°†ä½¿ç”¨ 8 ä¸ªè¿›ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 2: ä»å†³ç­–æ–‡ä»¶ç”Ÿæˆè§„åˆ™è¯å…¸**\n",
    "\n",
    "**ç›®æ ‡:** è¯»å–ç»è¿‡äººå·¥å®¡æ ¸çš„ä¸¤ä¸ªCSVæ–‡ä»¶ (`expert_rules.csv` å’Œ `candidate_phrases_for_review_reviewed.csv`)ï¼Œå¹¶æ ¹æ®â€œä¸“å®¶ä¼˜å…ˆâ€çš„åŸåˆ™ï¼Œç”Ÿæˆæœ€ç»ˆçš„åˆå¹¶è¯å…¸ (`merge_dict`) å’Œè‡ªå®šä¹‰åœç”¨è¯é›†åˆ (`new_stopwords`)ã€‚"
   ],
   "id": "7e0dfe99e14d0fc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:18.823254Z",
     "start_time": "2025-08-07T14:21:18.696816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 2: ä»å†³ç­–æ–‡ä»¶ç”Ÿæˆè§„åˆ™è¯å…¸ ---\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- é˜¶æ®µ 4.1: å¼€å§‹ç”Ÿæˆè§„åˆ™è¯å…¸ä¸åœç”¨è¯åˆ—è¡¨ ---\")\n",
    "start_time = time.time()\n",
    "\n",
    "# ä½œç”¨: åˆå§‹åŒ–ç”¨äºå­˜å‚¨åˆå¹¶è§„åˆ™çš„å­—å…¸å’Œå­˜å‚¨è‡ªå®šä¹‰åœç”¨è¯çš„é›†åˆã€‚\n",
    "merge_dict = {}\n",
    "new_stopwords = set()\n",
    "\n",
    "try:\n",
    "    # ä½œç”¨: é¦–å…ˆåŠ è½½å¹¶å¤„ç†ä¸“å®¶è§„åˆ™æ–‡ä»¶ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰ã€‚\n",
    "    if os.path.exists(EXPERT_RULES_PATH):\n",
    "        df_expert = pd.read_csv(EXPERT_RULES_PATH)\n",
    "        for _, row in df_expert.iterrows():\n",
    "            phrase = str(row['phrase_to_merge']).strip().lower()\n",
    "            standard = str(row['standard_form']).strip()\n",
    "            if phrase and standard:\n",
    "                merge_dict[phrase] = standard\n",
    "        print(f\"âœ… ä» {EXPERT_RULES_PATH} åŠ è½½äº† {len(df_expert)} æ¡ä¸“å®¶è§„åˆ™ã€‚\")\n",
    "    else:\n",
    "        print(f\"â„¹ï¸ æœªæ‰¾åˆ°ä¸“å®¶è§„åˆ™æ–‡ä»¶: {EXPERT_RULES_PATH}ï¼Œè·³è¿‡ã€‚\")\n",
    "\n",
    "    # ä½œç”¨: æ¥ç€åŠ è½½å¹¶å¤„ç†ç»è¿‡äººå·¥å®¡æ ¸çš„å€™é€‰çŸ­è¯­æ–‡ä»¶ã€‚\n",
    "    df_reviewed = pd.read_csv(REVIEWED_CANDIDATES_PATH)\n",
    "    print(f\"âœ… ä» {REVIEWED_CANDIDATES_PATH} åŠ è½½äº† {len(df_reviewed)} æ¡å·²å®¡æ ¸çš„å€™é€‰ã€‚\")\n",
    "\n",
    "    # ä½œç”¨: å¯¹'action_code'åˆ—è¿›è¡Œæ¸…æ´—ï¼Œç¡®ä¿å…¶ä¸ºæ•´æ•°ç±»å‹ï¼Œä»¥ä¾¿è¿›è¡Œåˆ¤æ–­ã€‚\n",
    "    df_reviewed['action_code'] = pd.to_numeric(df_reviewed['action_code'], errors='coerce')\n",
    "    df_reviewed.dropna(subset=['action_code'], inplace=True)\n",
    "    df_reviewed['action_code'] = df_reviewed['action_code'].astype(int)\n",
    "\n",
    "    reviewed_merges = 0\n",
    "    reviewed_stopwords = 0\n",
    "\n",
    "    # ä½œç”¨: éå†å®¡æ ¸åçš„æ–‡ä»¶ï¼Œæ ¹æ®action_codeå°†è§„åˆ™åˆ†é…åˆ°åˆå¹¶å­—å…¸æˆ–åœç”¨è¯é›†åˆã€‚\n",
    "    for _, row in df_reviewed.iterrows():\n",
    "        phrase = str(row['candidate_phrase']).strip().lower()\n",
    "        if not phrase: continue\n",
    "\n",
    "        if row['action_code'] == 1: # å€¼ä¸º1è¡¨ç¤ºåˆå¹¶\n",
    "            standard = str(row['standard_form']).strip()\n",
    "            # ç¡®ä¿ä¸“å®¶è§„åˆ™ä¼˜å…ˆï¼Œå¦‚æœè¯¥çŸ­è¯­å·²å­˜åœ¨äºå­—å…¸ä¸­ï¼Œåˆ™ä¸è¦†ç›–ã€‚\n",
    "            if phrase and standard and phrase not in merge_dict:\n",
    "                merge_dict[phrase] = standard\n",
    "                reviewed_merges += 1\n",
    "        elif row['action_code'] == 2: # å€¼ä¸º2è¡¨ç¤ºè®¾ä¸ºåœç”¨è¯\n",
    "            new_stopwords.update(phrase.split())\n",
    "            reviewed_stopwords += 1\n",
    "\n",
    "    print(f\"  - ä»å®¡æ ¸æ–‡ä»¶ä¸­å¤„ç†äº† {reviewed_merges} æ¡æ–°çš„åˆå¹¶è§„åˆ™ã€‚\")\n",
    "    print(f\"  - ä»å®¡æ ¸æ–‡ä»¶ä¸­å¤„ç†äº† {reviewed_stopwords} æ¡æ–°çš„åœç”¨è¯çŸ­è¯­ã€‚\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"ğŸ“Š æœ€ç»ˆè§„åˆ™åº“ç»Ÿè®¡:\")\n",
    "    print(f\"   - æ€»åˆå¹¶è§„åˆ™æ•° (merge_dict): {len(merge_dict)}\")\n",
    "    print(f\"   - æ€»æ–°å¢åœç”¨è¯æ•° (new_stopwords): {len(new_stopwords)}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # ä½œç”¨: å°†æœ€ç»ˆç”Ÿæˆçš„è§„åˆ™å¯¹è±¡ä¿å­˜åˆ°ç£ç›˜ï¼Œä»¥ä¾¿åç»­æµç¨‹ä½¿ç”¨ã€‚\n",
    "    with open(MERGE_DICT_PATH, 'wb') as f:\n",
    "        pickle.dump(merge_dict, f)\n",
    "    print(f\"âœ… [æœºå™¨ç”¨] åˆå¹¶è¯å…¸å·²ä¿å­˜åˆ°: {MERGE_DICT_PATH}\")\n",
    "\n",
    "    with open(NEW_STOPWORDS_PATH, 'wb') as f:\n",
    "        pickle.dump(new_stopwords, f)\n",
    "    print(f\"âœ… [æœºå™¨ç”¨] æ–°å¢åœç”¨è¯åˆ—è¡¨å·²ä¿å­˜åˆ°: {NEW_STOPWORDS_PATH}\")\n",
    "\n",
    "    # ä½œç”¨: å°†åˆå¹¶å­—å…¸ä¿å­˜ä¸ºCSVæ ¼å¼ï¼Œç”¨äºäººå·¥æ£€æŸ¥å’Œå­˜æ¡£ã€‚\n",
    "    MERGE_DICT_CSV_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'merge_dict_for_review.csv')\n",
    "    df_merge_dict = pd.DataFrame(list(merge_dict.items()), columns=['phrase_to_merge', 'standard_form'])\n",
    "    df_merge_dict.to_csv(MERGE_DICT_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… [äººç±»ç”¨] åˆå¹¶è¯å…¸å·²ä¿å­˜åˆ°CSV: {MERGE_DICT_CSV_PATH}\")\n",
    "\n",
    "    print(f\"è§„åˆ™ç”Ÿæˆè€—æ—¶: {time.time() - start_time:.2f} ç§’ã€‚\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„è¾“å…¥æ–‡ä»¶: {e.filename}ã€‚\")\n",
    "    print(\"è¯·ç¡®ä¿å·²å®Œæˆäººå·¥å®¡æ ¸å¹¶æ­£ç¡®æ”¾ç½®æ–‡ä»¶ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ å¤„ç†è§„åˆ™æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}\")"
   ],
   "id": "1445db563ae0930b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- é˜¶æ®µ 4.1: å¼€å§‹ç”Ÿæˆè§„åˆ™è¯å…¸ä¸åœç”¨è¯åˆ—è¡¨ ---\n",
      "â„¹ï¸ æœªæ‰¾åˆ°ä¸“å®¶è§„åˆ™æ–‡ä»¶: ../data_processed\\expert_rules.csvï¼Œè·³è¿‡ã€‚\n",
      "âœ… ä» ../data_processed\\candidate_phrases_for_review_reviewed.csv åŠ è½½äº† 3140 æ¡å·²å®¡æ ¸çš„å€™é€‰ã€‚\n",
      "  - ä»å®¡æ ¸æ–‡ä»¶ä¸­å¤„ç†äº† 1826 æ¡æ–°çš„åˆå¹¶è§„åˆ™ã€‚\n",
      "  - ä»å®¡æ ¸æ–‡ä»¶ä¸­å¤„ç†äº† 846 æ¡æ–°çš„åœç”¨è¯çŸ­è¯­ã€‚\n",
      "------------------------------\n",
      "ğŸ“Š æœ€ç»ˆè§„åˆ™åº“ç»Ÿè®¡:\n",
      "   - æ€»åˆå¹¶è§„åˆ™æ•° (merge_dict): 1826\n",
      "   - æ€»æ–°å¢åœç”¨è¯æ•° (new_stopwords): 736\n",
      "------------------------------\n",
      "âœ… [æœºå™¨ç”¨] åˆå¹¶è¯å…¸å·²ä¿å­˜åˆ°: ../data_processed\\merge_dict.pkl\n",
      "âœ… [æœºå™¨ç”¨] æ–°å¢åœç”¨è¯åˆ—è¡¨å·²ä¿å­˜åˆ°: ../data_processed\\new_stopwords.pkl\n",
      "âœ… [äººç±»ç”¨] åˆå¹¶è¯å…¸å·²ä¿å­˜åˆ°CSV: ../data_processed\\merge_dict_for_review.csv\n",
      "è§„åˆ™ç”Ÿæˆè€—æ—¶: 0.12 ç§’ã€‚\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 3: æ„å»ºé«˜æ•ˆæ›¿æ¢å¼•æ“**\n",
    "\n",
    "**ç›®æ ‡:** åˆ©ç”¨ `pyahocorasick` åº“ï¼Œå°†ä¸Šä¸€æ­¥ç”Ÿæˆçš„ã€å¯èƒ½éå¸¸åºå¤§çš„ `merge_dict` ç¼–è¯‘æˆä¸€ä¸ªAho-Corasickè‡ªåŠ¨æœºã€‚è¿™ä¸ªè‡ªåŠ¨æœºæ˜¯å®ç°å¤§è§„æ¨¡ã€é«˜æ€§èƒ½æ–‡æœ¬æ›¿æ¢çš„å…³é”®ï¼Œå…¶æŸ¥æ‰¾æ•ˆç‡è¿œè¶…å¸¸è§„çš„å¾ªç¯æˆ–æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•ã€‚"
   ],
   "id": "9cf48a17e6f83d66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:18.861346Z",
     "start_time": "2025-08-07T14:21:18.838345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 3: æ„å»ºé«˜æ•ˆæ›¿æ¢å¼•æ“ ---\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- é˜¶æ®µ 4.2a: æ„å»º Aho-Corasick é«˜æ•ˆæ›¿æ¢å¼•æ“ ---\")\n",
    "\n",
    "automaton = None\n",
    "\n",
    "# ä½œç”¨: å°†åˆå¹¶å­—å…¸ç¼–è¯‘æˆä¸€ä¸ªAho-Corasickè‡ªåŠ¨æœºï¼Œç”¨äºåç»­å¤§è§„æ¨¡ã€é«˜æ€§èƒ½çš„å¤šæ¨¡å¼å­—ç¬¦ä¸²åŒ¹é…å’Œæ›¿æ¢ã€‚\n",
    "if 'merge_dict' in locals() and merge_dict:\n",
    "    automaton = ahocorasick.Automaton()\n",
    "\n",
    "    # ä½œç”¨: å°†æ¯ä¸ªå¾…æŸ¥æ‰¾çš„çŸ­è¯­åŠå…¶å¯¹åº”çš„(é•¿åº¦, æ ‡å‡†å½¢å¼)å­˜å…¥è‡ªåŠ¨æœºã€‚\n",
    "    # å­˜å…¥é•¿åº¦æ˜¯ä¸ºäº†åœ¨åŒ¹é…æ—¶ç›´æ¥è·å–ï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œæå‡æ•ˆç‡ã€‚\n",
    "    for phrase, standard_form in tqdm(merge_dict.items(), desc=\"æ„å»ºè‡ªåŠ¨æœº\"):\n",
    "        automaton.add_word(phrase, (len(phrase), standard_form))\n",
    "\n",
    "    # ä½œç”¨: å®Œæˆè‡ªåŠ¨æœºçš„æ„å»ºï¼Œä½¿å…¶è¿›å…¥å¯åŒ¹é…çŠ¶æ€ã€‚\n",
    "    automaton.make_automaton()\n",
    "    print(f\"âœ… Aho-Corasick è‡ªåŠ¨æœºæ„å»ºå®Œæˆï¼ŒåŒ…å« {len(merge_dict)} æ¡è§„åˆ™ã€‚\")\n",
    "else:\n",
    "    print(\"âš ï¸ è­¦å‘Š: åˆå¹¶è¯å…¸ (merge_dict) ä¸ºç©ºæˆ–æœªå®šä¹‰ï¼Œæ— æ³•æ„å»ºæ›¿æ¢å¼•æ“ã€‚\")"
   ],
   "id": "5d428750433dce89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- é˜¶æ®µ 4.2a: æ„å»º Aho-Corasick é«˜æ•ˆæ›¿æ¢å¼•æ“ ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "æ„å»ºè‡ªåŠ¨æœº:   0%|          | 0/1826 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99227b41a6484bf99d95e87058fe0ff6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Aho-Corasick è‡ªåŠ¨æœºæ„å»ºå®Œæˆï¼ŒåŒ…å« 1826 æ¡è§„åˆ™ã€‚\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 4: åº”ç”¨è§„åˆ™å›ºåŒ–æ–‡æœ¬**\n",
    "\n",
    "**ç›®æ ‡:** åŠ è½½å®Œæ•´çš„ä¸­å›½æ–°é—»è¯­æ–™åº“ï¼Œå¹¶åº”ç”¨ä¸Šä¸€æ­¥æ„å»ºçš„è‡ªåŠ¨æœºï¼Œå¯¹æ¯ä¸€ç¯‡æ–‡ç« çš„`CONTENT`è¿›è¡Œå®ä½“å’ŒçŸ­è¯­çš„â€œå›ºåŒ–â€æ›¿æ¢ã€‚è¿™æ˜¯æ•´ä¸ªæµç¨‹çš„æ ¸å¿ƒæ‰§è¡Œæ­¥éª¤ã€‚"
   ],
   "id": "cde2e3a395fa5e5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:40.188512Z",
     "start_time": "2025-08-07T14:21:18.892347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 4: åº”ç”¨è§„åˆ™å›ºåŒ–æ–‡æœ¬ (å¸¦å…¨æ–¹ä½éªŒè¯ç›‘æ§) ---\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- é˜¶æ®µ 4.2b: å¼€å§‹å•çº¿ç¨‹æ–‡æœ¬å›ºåŒ–å¤„ç† ---\")\n",
    "start_time_solidify = time.time()\n",
    "\n",
    "# ä½œç”¨: åˆå§‹åŒ–å‡½æ•°å†…éƒ¨çš„ç›‘æ§è®¡æ•°å™¨ã€‚\n",
    "def initialize_counters():\n",
    "    solidify_text_definitively.total_matches = 0\n",
    "    solidify_text_definitively.processed_with_matches = 0\n",
    "\n",
    "def solidify_text_definitively(text, automaton_obj):\n",
    "    \"\"\"\n",
    "    ä½œç”¨ï¼šå¯¹å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²åº”ç”¨Aho-Corasickè‡ªåŠ¨æœºè¿›è¡Œå®ä½“å›ºåŒ–ã€‚\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not automaton_obj:\n",
    "        return \"\"\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    parts = []\n",
    "    last_end = 0\n",
    "\n",
    "    all_matches = []\n",
    "    for end_index, (phrase_len, standard_form) in automaton_obj.iter(text_lower):\n",
    "        start_index = end_index - phrase_len + 1\n",
    "        is_start_boundary = (start_index == 0) or (not text_lower[start_index - 1].isalnum())\n",
    "        is_end_boundary = (end_index + 1 == len(text_lower)) or (not text_lower[end_index + 1].isalnum())\n",
    "\n",
    "        if is_start_boundary and is_end_boundary:\n",
    "            all_matches.append((start_index, end_index, standard_form))\n",
    "\n",
    "    # ä½œç”¨: [ç›‘æ§ç‚¹] å¦‚æœæ‰¾åˆ°äº†ä»»ä½•åŒ¹é…é¡¹ï¼Œåˆ™æ›´æ–°è®¡æ•°å™¨ã€‚\n",
    "    if len(all_matches) > 0:\n",
    "        solidify_text_definitively.total_matches += len(all_matches)\n",
    "        solidify_text_definitively.processed_with_matches += 1\n",
    "\n",
    "    if not all_matches:\n",
    "        return text\n",
    "\n",
    "    final_matches = []\n",
    "    i = 0\n",
    "    while i < len(all_matches):\n",
    "        current_best_match = all_matches[i]\n",
    "        j = i + 1\n",
    "        while j < len(all_matches) and all_matches[j][0] <= current_best_match[1]:\n",
    "            if (all_matches[j][1] - all_matches[j][0]) > (current_best_match[1] - current_best_match[0]):\n",
    "                current_best_match = all_matches[j]\n",
    "            j += 1\n",
    "\n",
    "        final_matches.append(current_best_match)\n",
    "        i = j\n",
    "\n",
    "    for start_index, end_index, standard_form in final_matches:\n",
    "        if start_index >= last_end:\n",
    "            parts.append(text[last_end:start_index])\n",
    "            parts.append(standard_form)\n",
    "            last_end = end_index + 1\n",
    "\n",
    "    parts.append(text[last_end:])\n",
    "\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "try:\n",
    "    # ä½œç”¨ï¼šåŠ è½½ç­›é€‰åçš„æ–°é—»æ•°æ®ã€‚\n",
    "    if not os.path.exists(FINAL_CHINA_NEWS_PATH):\n",
    "         raise FileNotFoundError(f\"æ–°é—»è¯­æ–™æ–‡ä»¶æœªæ‰¾åˆ°: {FINAL_CHINA_NEWS_PATH}\")\n",
    "\n",
    "    read_nrows = TEST_SAMPLE_SIZE if TEST_MODE else None\n",
    "    print(\"æ­£åœ¨åŠ è½½æ–°é—»æ•°æ®...\")\n",
    "    df = pd.read_csv(FINAL_CHINA_NEWS_PATH, nrows=read_nrows)\n",
    "\n",
    "    # ä½œç”¨: [éªŒè¯ç‚¹] éªŒè¯åŠ è½½çš„æ•°æ®é‡å’ŒåŸºæœ¬ä¿¡æ¯ã€‚\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(df)} ç¯‡æ–°é—»è¿›è¡Œå¤„ç†ã€‚\")\n",
    "    if not df.empty:\n",
    "        print(f\"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {df['DATE'].min()} åˆ° {df['DATE'].max()}\")\n",
    "        print(f\"ğŸ“Š æ•°æ®åˆ—ä¿¡æ¯: {list(df.columns)}\")\n",
    "\n",
    "    # ä½œç”¨ï¼šç¡®ä¿'TITLE'å’Œ'CONTENT'åˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œå¹¶å°†NaNå€¼è½¬æ¢ä¸ºç©ºå­—ç¬¦ä¸²ã€‚\n",
    "    df['TITLE'] = df['TITLE'].astype(str).fillna('')\n",
    "    df['CONTENT'] = df['CONTENT'].astype(str).fillna('')\n",
    "\n",
    "    # ä½œç”¨ï¼šåº”ç”¨å®ä½“å›ºåŒ–è§„åˆ™ã€‚\n",
    "    if 'automaton' in locals() and automaton:\n",
    "        print(\"\\nå¼€å§‹æ–‡æœ¬å›ºåŒ–å¤„ç†...\")\n",
    "\n",
    "        # ä½œç”¨: [éªŒè¯ç‚¹] æ‰“å°è®¡åˆ’å¤„ç†çš„æ–‡æœ¬æ€»æ•°ã€‚\n",
    "        total_texts = len(df)\n",
    "        print(f\"ğŸ“ˆ è®¡åˆ’å¤„ç†æ–‡æœ¬æ•°é‡: {total_texts}\")\n",
    "\n",
    "        # ä½œç”¨: åœ¨å¤„ç†å‰é‡ç½®/åˆå§‹åŒ–å‡½æ•°å†…çš„ç›‘æ§è®¡æ•°å™¨ã€‚\n",
    "        initialize_counters()\n",
    "\n",
    "        # ä½œç”¨ï¼šå°†TITLEå’ŒCONTENTåœ¨å†…å­˜ä¸­åŠ¨æ€åˆå¹¶æˆä¸€ä¸ªæ–‡æœ¬æµï¼Œå¹¶åº”ç”¨å›ºåŒ–å‡½æ•°ã€‚\n",
    "        text_iterator = (title + ' . ' + content for title, content in zip(df['TITLE'], df['CONTENT']))\n",
    "\n",
    "        tqdm.pandas(desc=\"å›ºåŒ–æ–‡æœ¬\")\n",
    "        df['content_solidified'] = pd.Series(text_iterator, index=df.index).progress_apply(\n",
    "            lambda text: solidify_text_definitively(text, automaton)\n",
    "        )\n",
    "\n",
    "        print(f\"âœ… æ–‡æœ¬å›ºåŒ–å®Œæˆï¼è€—æ—¶: {(time.time() - start_time_solidify) / 60:.2f} åˆ†é’Ÿã€‚\")\n",
    "\n",
    "        # ä½œç”¨: [éªŒè¯ç‚¹] æ‰“å°è¯¦å°½çš„å¤„ç†ç»“æœç»Ÿè®¡æŠ¥å‘Šã€‚\n",
    "        processed_count = df['content_solidified'].count()\n",
    "        print(f\"\\n--- å¤„ç†ç»“æœéªŒè¯ ---\")\n",
    "        print(f\"âœ… å®é™…å¤„ç†æ–‡æœ¬æ•°é‡: {processed_count}\")\n",
    "        if processed_count == total_texts:\n",
    "            print(\"âœ… å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡ï¼šæ‰€æœ‰æ–‡æœ¬è¡Œå‡å·²æˆåŠŸå¤„ç†ã€‚\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥ï¼šå¤„ç†æ•°é‡ä¸åŒ¹é…ï¼åŸå§‹: {total_texts}, å¤„ç†åéç©º: {processed_count}\")\n",
    "\n",
    "        original_merged_text = df['TITLE'] + ' . ' + df['CONTENT']\n",
    "        changed_count = (df['content_solidified'] != original_merged_text).sum()\n",
    "\n",
    "        print(f\"\\n--- å›ºåŒ–æ•ˆæœç»Ÿè®¡ ---\")\n",
    "        print(f\"ğŸ”„ æ–‡æœ¬è¢«ä¿®æ”¹ï¼ˆå›ºåŒ–ï¼‰çš„æ•°é‡: {changed_count} / {total_texts}\")\n",
    "        if total_texts > 0:\n",
    "            print(f\"ğŸ“Š å›ºåŒ–ç‡: {changed_count / total_texts * 100:.2f}%\")\n",
    "        print(f\"ğŸ”¢ å‡½æ•°å†…éƒ¨ç»Ÿè®¡: åœ¨ {solidify_text_definitively.processed_with_matches} ç¯‡è¢«ä¿®æ”¹çš„æ–‡ç« ä¸­ï¼Œæ€»å…±æ‰§è¡Œäº† {solidify_text_definitively.total_matches} æ¬¡çŸ­è¯­åŒ¹é…ã€‚\")\n",
    "\n",
    "        # ä½œç”¨ï¼šéšæœºæŠ½æ ·æ–‡ç« ï¼Œæ‰“å°å…¶åŸå§‹ä¸å›ºåŒ–åçš„å†…å®¹ï¼Œç”¨äºäººå·¥å¿«é€ŸéªŒè¯ã€‚\n",
    "        print(\"\\n--- æŠ½æ ·æ£€æŸ¥ç»“æœ ---\")\n",
    "        sample_size = min(5, len(df))\n",
    "        if sample_size > 0:\n",
    "            pd.set_option('display.max_colwidth', 400)\n",
    "            for i, row in df.sample(sample_size).iterrows():\n",
    "                print(f\"--- æ–‡ç«  {i} ---\")\n",
    "                print(f\"ã€åŸå§‹TITLEã€‘: {str(row['TITLE'])}\")\n",
    "                print(f\"ã€åŸå§‹CONTENTã€‘: {str(row['CONTENT'])[:300]}...\")\n",
    "                print(f\"ã€å›ºåŒ–åã€‘: {str(row['content_solidified'])[:400]}...\")\n",
    "                print(\"-\" * 20)\n",
    "    else:\n",
    "        print(\"\\nè‡ªåŠ¨æœºæœªæ„å»ºï¼Œè·³è¿‡æ–‡æœ¬å›ºåŒ–ã€‚å°†ç›´æ¥åˆå¹¶TITLEå’ŒCONTENTã€‚\")\n",
    "        df['content_solidified'] = df['TITLE'] + ' . ' + df['CONTENT']\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ é”™è¯¯: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ å›ºåŒ–æ–‡æœ¬è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ],
   "id": "44208e52ffdfebb9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- é˜¶æ®µ 4.2b: å¼€å§‹å•çº¿ç¨‹æ–‡æœ¬å›ºåŒ–å¤„ç† ---\n",
      "æ­£åœ¨åŠ è½½æ–°é—»æ•°æ®...\n",
      "âœ… æˆåŠŸåŠ è½½ 180630 ç¯‡æ–°é—»è¿›è¡Œå¤„ç†ã€‚\n",
      "ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: 1984-01-02 åˆ° 2025-03-15\n",
      "ğŸ“Š æ•°æ®åˆ—ä¿¡æ¯: ['DATE', 'TITLE', 'CONTENT']\n",
      "\n",
      "å¼€å§‹æ–‡æœ¬å›ºåŒ–å¤„ç†...\n",
      "ğŸ“ˆ è®¡åˆ’å¤„ç†æ–‡æœ¬æ•°é‡: 180630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "å›ºåŒ–æ–‡æœ¬:   0%|          | 0/180630 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f0f8bc968874291980695e2445172ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ–‡æœ¬å›ºåŒ–å®Œæˆï¼è€—æ—¶: 0.35 åˆ†é’Ÿã€‚\n",
      "\n",
      "--- å¤„ç†ç»“æœéªŒè¯ ---\n",
      "âœ… å®é™…å¤„ç†æ–‡æœ¬æ•°é‡: 180630\n",
      "âœ… å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡ï¼šæ‰€æœ‰æ–‡æœ¬è¡Œå‡å·²æˆåŠŸå¤„ç†ã€‚\n",
      "\n",
      "--- å›ºåŒ–æ•ˆæœç»Ÿè®¡ ---\n",
      "ğŸ”„ æ–‡æœ¬è¢«ä¿®æ”¹ï¼ˆå›ºåŒ–ï¼‰çš„æ•°é‡: 173985 / 180630\n",
      "ğŸ“Š å›ºåŒ–ç‡: 96.32%\n",
      "ğŸ”¢ å‡½æ•°å†…éƒ¨ç»Ÿè®¡: åœ¨ 173985 ç¯‡è¢«ä¿®æ”¹çš„æ–‡ç« ä¸­ï¼Œæ€»å…±æ‰§è¡Œäº† 2246311 æ¬¡çŸ­è¯­åŒ¹é…ã€‚\n",
      "\n",
      "--- æŠ½æ ·æ£€æŸ¥ç»“æœ ---\n",
      "--- æ–‡ç«  99837 ---\n",
      "ã€åŸå§‹TITLEã€‘: Are Plastics Making Us Fat?\n",
      "ã€åŸå§‹CONTENTã€‘: Weight-loss crazes are as American as apple pie -- make that Slim Fast shakes. But despite our countless diet fads, the obesity rate has more than doubled in the last 30 years. Perhaps that's because Americans haven't tried \"The New American Diet,\" which promises to reveal \"why your weight isn't you...\n",
      "ã€å›ºåŒ–åã€‘: Are Plastics Making Us Fat? . Weight-loss crazes are as American as apple pie -- make that Slim Fast shakes. But despite our countless diet fads, the obesity rate has more than doubled in the last 30 years. Perhaps that's because Americans haven't tried \"The New American Diet,\" which promises to reveal \"why your weight isn't your fault\" and reverse \"the obesogen effect.\" Haven't heard of the \"obes...\n",
      "--------------------\n",
      "--- æ–‡ç«  161841 ---\n",
      "ã€åŸå§‹TITLEã€‘: Business & Finance\n",
      "ã€åŸå§‹CONTENTã€‘: Palantir is expected to fetch a lofty valuation in its transition to a public company despite an unusually aggressive governance structure, in the latest sign of investors' voracious appetite for new shares. The Blue Cross Blue Shield insurance group has negotiated a tentative settlement in an antit...\n",
      "ã€å›ºåŒ–åã€‘: Business & Finance . Palantir is expected to fetch a lofty valuation in its transition to a public company despite an unusually aggressive governance structure, in the latest sign of investors' voracious appetite for new shares. The Blue Cross Blue Shield insurance group has negotiated a tentative settlement in an antitrust suit_filed on behalf of customers, a deal that would require a payout of a...\n",
      "--------------------\n",
      "--- æ–‡ç«  16674 ---\n",
      "ã€åŸå§‹TITLEã€‘: Small Business Summit Is in the Works; SBICs Probed\n",
      "ã€åŸå§‹CONTENTã€‘: PLANNING BEGINS for a White House Conference on Small Business in 1994. Legislation calling for the conference, a sort of national summit meeting on small-business issues, is in the early stages of congressional action. Barring unforeseen problems, lawmakers could send a bill to President Bush by la...\n",
      "ã€å›ºåŒ–åã€‘: Small Business Summit Is in the Works; SBICs Probed . PLANNING BEGINS for a white_house Conference on Small Business in 1994. Legislation calling for the conference, a sort of national summit_meeting on small-business issues, is in the early stages of congressional action. Barring unforeseen problems, lawmakers could send a bill to President Bush by late summer. The administration and entrepreneur...\n",
      "--------------------\n",
      "--- æ–‡ç«  53481 ---\n",
      "ã€åŸå§‹TITLEã€‘: New Securities Issues\n",
      "ã€åŸå§‹CONTENTã€‘: The following were among yesterday's offerings and pricings in U.S. and non-U.S. capital markets, with terms and syndicate manager, based on information provided by Dow Jones Newswires and Factiva. (A basis point is one-hundredth of a percentage point; 100 basis points equals a percentage point.) CO...\n",
      "ã€å›ºåŒ–åã€‘: New Securities Issues . The following were among yesterday's offerings and pricings in U.S. and non-U.S. capital_markets, with terms and syndicate manager, based on information provided by dow_jones Newswires and Factiva. (A basis point is one-hundredth of a percentage_point; 100 basis points equals a percentage_point.) CORPORATE Ambac Financial Group Inc. -- $200 million offering of senior notes ...\n",
      "--------------------\n",
      "--- æ–‡ç«  133315 ---\n",
      "ã€åŸå§‹TITLEã€‘: Yahoo Investors Tighten Screws --- Groups push to slash workforce, replace CEO Mayer, find buyer for core Web business\n",
      "ã€åŸå§‹CONTENTã€‘: Corrections & Amplifications Eric Jackson, managing director of SpringOwl Asset Management LLC, has about 22,000 Twitter followers. A Business & Tech article Monday about investor pressure on Yahoo Inc. incorrectly said he has 67,000. (WSJ December 15, 2015) Yahoo Inc. is facing new pressure from in...\n",
      "ã€å›ºåŒ–åã€‘: Yahoo Investors Tighten Screws --- Groups push to slash workforce, replace CEO Mayer, find buyer for core Web business . Corrections & Amplifications Eric Jackson, managing_director of SpringOwl Asset Management LLC, has about 22,000 Twitter followers. A Business & Tech article Monday about investor pressure on Yahoo Inc. incorrectly said he has 67,000. (WSJ December 15, 2015) Yahoo Inc. is facing...\n",
      "--------------------\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **å— 5: ä¿å­˜æœ€ç»ˆç»“æœ**\n",
    "\n",
    "**ç›®æ ‡:** å°†åŒ…å«æ–°å¢çš„`content_solidified`åˆ—çš„DataFrameå®Œæ•´ä¿å­˜åˆ°ç£ç›˜ã€‚æ¨èä½¿ç”¨Pickleæ ¼å¼ï¼Œå› ä¸ºå®ƒèƒ½æ— æŸã€é«˜æ•ˆåœ°å­˜å‚¨Pandaså¯¹è±¡ï¼Œä¸ºåç»­çš„æ·±åº¦æ¸…æ´—æ­¥éª¤ï¼ˆ`04_...`ï¼‰æä¾›ä¸€ä¸ªå¹²å‡€çš„èµ·ç‚¹ã€‚"
   ],
   "id": "98bf445ab8353e63"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:54.762739Z",
     "start_time": "2025-08-07T14:21:40.218834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- å— 5: ä¿å­˜æœ€ç»ˆç»“æœ ---\n",
    "# =============================================================================\n",
    "\n",
    "# ä½œç”¨: è´Ÿè´£å¯¹å›ºåŒ–åçš„ç»“æœè¿›è¡Œæ£€æŸ¥ï¼Œå¹¶ä¿å­˜ä¸ºæœ€ç»ˆçš„Pickleå’ŒCSVæ–‡ä»¶ã€‚\n",
    "if 'df' in locals() and 'content_solidified' in df.columns:\n",
    "    print(\"\\n--- é˜¶æ®µ 4.2c: ä¿å­˜å›ºåŒ–åçš„ç»“æœ ---\")\n",
    "    try:\n",
    "        # ä½œç”¨: å®šä¹‰ä¸¤ç§æ ¼å¼çš„è¾“å‡ºè·¯å¾„ã€‚\n",
    "        SOLIDIFIED_TEXT_PKL_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_solidified.pkl')\n",
    "        SOLIDIFIED_TEXT_CSV_PATH = os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_solidified_for_review.csv')\n",
    "\n",
    "        # ä½œç”¨: ä¿å­˜ä¸ºPickleæ ¼å¼ï¼Œç”¨äºåç»­Pythonè„šæœ¬é«˜æ•ˆè¯»å–ã€‚\n",
    "        df.to_pickle(SOLIDIFIED_TEXT_PKL_PATH)\n",
    "        print(f\"âœ… [æœºå™¨ç”¨] åŒ…å«å›ºåŒ–æ–‡æœ¬çš„DataFrameå·²ä¿å­˜åˆ°Pickle: {SOLIDIFIED_TEXT_PKL_PATH}\")\n",
    "\n",
    "        # ä½œç”¨: ä¿å­˜ä¸ºCSVæ ¼å¼ï¼Œç”¨äºäººå·¥æŸ¥é˜…å’Œå®¡è®¡ã€‚\n",
    "        df.to_csv(SOLIDIFIED_TEXT_CSV_PATH, index=False, encoding='utf-8-sig')\n",
    "        print(f\"âœ… [äººç±»ç”¨] åŒ…å«å›ºåŒ–æ–‡æœ¬çš„DataFrameå·²ä¿å­˜åˆ°CSV: {SOLIDIFIED_TEXT_CSV_PATH}\")\n",
    "\n",
    "        # ä½œç”¨: åœ¨æœåŠ¡å™¨ç¯å¢ƒä¸‹ï¼Œå°†äº§å‡ºæ–‡ä»¶ä»ä¸´æ—¶ç›®å½•åŒæ­¥å›é¡¹ç›®çš„æ•°æ®ç›®å½•ã€‚\n",
    "        if RUNNING_ENV == 'dsw':\n",
    "            print(f\"DSWæ¨¡å¼ï¼šæ­£åœ¨å°†ç»“æœåŒæ­¥å›é¡¹ç›®ç›®å½•...\")\n",
    "            shutil.copy(SOLIDIFIED_TEXT_PKL_PATH, SOLIDIFIED_TEXT_ORIGINAL)\n",
    "            shutil.copy(SOLIDIFIED_TEXT_CSV_PATH,\n",
    "                        os.path.join(BASE_DATA_PROCESSED_PATH, 'china_news_solidified_for_review.csv'))\n",
    "            print(\"åŒæ­¥å®Œæˆã€‚\")\n",
    "\n",
    "        # ä½œç”¨: æ‰“å°æ¸…æ™°çš„æ€»ç»“å’Œä¸‹ä¸€æ­¥æ“ä½œæŒ‡å¼•ã€‚\n",
    "        print(f\"\\nâœ…âœ…âœ… â€œå®ä½“å›ºåŒ–â€æµç¨‹å…¨éƒ¨å®Œæˆï¼ âœ…âœ…âœ…\")\n",
    "        print(f\"äº§å‡ºæ–‡ä»¶:\")\n",
    "        print(f\"  - {SOLIDIFIED_TEXT_PKL_PATH}\")\n",
    "        print(f\"  - {SOLIDIFIED_TEXT_CSV_PATH}\")\n",
    "        print(\"\\nä¸‹ä¸€æ­¥æ˜¯è¿è¡Œ '03_Deep_Text_Cleaning.ipynb'ï¼Œå¯¹'content_solidified'åˆ—è¿›è¡Œæœ€ç»ˆçš„æ ‡å‡†åŒ–å’Œå™ªå£°ç§»é™¤ã€‚\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "else:\n",
    "    print(\"æ²¡æœ‰å¯ä¾›ä¿å­˜çš„å›ºåŒ–æ–‡æœ¬æ•°æ®ã€‚\")"
   ],
   "id": "3bfb0d249f79a8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- é˜¶æ®µ 4.2c: ä¿å­˜å›ºåŒ–åçš„ç»“æœ ---\n",
      "âœ… [æœºå™¨ç”¨] åŒ…å«å›ºåŒ–æ–‡æœ¬çš„DataFrameå·²ä¿å­˜åˆ°Pickle: ../data_processed\\china_news_solidified.pkl\n",
      "âœ… [äººç±»ç”¨] åŒ…å«å›ºåŒ–æ–‡æœ¬çš„DataFrameå·²ä¿å­˜åˆ°CSV: ../data_processed\\china_news_solidified_for_review.csv\n",
      "\n",
      "âœ…âœ…âœ… â€œå®ä½“å›ºåŒ–â€æµç¨‹å…¨éƒ¨å®Œæˆï¼ âœ…âœ…âœ…\n",
      "äº§å‡ºæ–‡ä»¶:\n",
      "  - ../data_processed\\china_news_solidified.pkl\n",
      "  - ../data_processed\\china_news_solidified_for_review.csv\n",
      "\n",
      "ä¸‹ä¸€æ­¥æ˜¯è¿è¡Œ '03_Deep_Text_Cleaning.ipynb'ï¼Œå¯¹'content_solidified'åˆ—è¿›è¡Œæœ€ç»ˆçš„æ ‡å‡†åŒ–å’Œå™ªå£°ç§»é™¤ã€‚\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T14:21:54.780154Z",
     "start_time": "2025-08-07T14:21:54.777843Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d08fea711a03f705",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
