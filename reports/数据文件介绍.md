
### **项目数据处理顺序说明文档**

**项目:** 基于WSJ新闻的中国经济主题研究 (sLDA)
**目标:** 将海量、原始的全球新闻数据，通过一套严谨、可复现的流程，转化为高质量、高相关性、为监督式主题模型（sLDA）量身定制的“干净词元”数据集。

#### **一、 数据处理流程概览 (Flowchart)**

我们的数据处理管道遵循一个“漏斗模型”，从海量原始数据开始，通过层层筛选和净化，最终得到高度浓缩的核心数据。

```
[原始数据]
    |
    └─ 1.5M+ 篇全球新闻 (Excel/CSV)
    |
    ▼
[阶段零: 数据整合与标准化]
    │
    ├─ Process: 格式统一、去重、排序
    │
    └─ Output: final_merged_all_news.csv (~1.5M+ 篇)
    |
    ▼
[阶段一: 智能中国新闻筛选]
    │
    ├─ Input: final_merged_all_news.csv
    ├─ Knowledge: china_keywords_collection.json (分级关键词)
    ├─ Process: 两阶段筛选 (Flashtext初筛 + spaCy上下文精筛)
    │
    └─ Output: final_china_news.csv (~180k 篇)
    |
    ▼
[阶段二: 实体与短语固化]
    │
    ├─ Input: final_china_news.csv
    ├─ Knowledge: expert_rules.csv & *_reviewed.csv (合并规则)
    ├─ Process: 1. TITLE与CONTENT合并
    │          2. Aho-Corasick 高性能替换
    │
    └─ Output: china_news_solidified.pkl (~180k 篇)
    |
    ▼
[阶段三: 深度文本清洗]
    │
    ├─ Input: china_news_solidified.pkl
    ├─ Knowledge: 三层停用词库 (spaCy默认, 自定义, 项目专属)
    ├─ Process: 1. 智能连字符处理
    │          2. 词性过滤 (POS Tagging)
    │          3. "终极版"词形还原 (Lemmatization)
    │          4. 停用词/短词移除
    │
    └─ 最终产出: china_news_cleaned_tokens.pkl (包含 `tokens_for_lda` 列)
```

---

#### **二、 各阶段数据文件演变详解**

**阶段零：原始数据整合**

*   **输入:** 多个来源、格式不一的 `.xlsx` 和 `.csv` 文件。
*   **处理过程:**
    1.  **标准化:** 将所有文件统一转换为CSV格式，并将日期统一为 `YYYY-MM-DD`。
    2.  **去重与合并:** 在文件内部及文件之间进行去重，然后将所有数据垂直合并。
    3.  **排序:** 按日期对全量数据集进行严格的升序排列。
*   **输出:**
    *   **`final_merged_all_news.csv`**
    *   **内容:** 包含 **1,560,878** 篇全球新闻的原始语料库。
    *   **状态:** 这是我们所有后续分析的、唯一的、可信的起点。

**阶段一：智能中国新闻筛选**

*   **输入:** `final_merged_all_news.csv`
*   **处理过程:** 这是一个两阶段的漏斗，旨在兼顾效率与精度。
    1.  **快速初筛:** 使用 `Flashtext` 算法，对150多万篇文章进行高速扫描，只要文章包含我们知识库 (`china_keywords_collection.json`) 中的**任何一个**关键词，就会被捞出。此阶段的目标是“求全”，确保不遗漏任何潜在相关的文章。
    2.  **上下文精筛:** 对初筛出的约18万篇“候选文章”，我们启用 `spaCy` 进行更深度的语言学分析。通过一套**5级动态积分系统**，我们奖励那些在文章开头就提及核心实体、或通篇高频讨论相关话题的文章，同时惩罚那些在假设、否定语境下提及关键词的文章。
*   **输出:**
    *   **`final_china_news.csv`**
    *   **内容:** 包含 **180,630** 篇与中国高度相关的新闻。
    *   **状态:** 数据量大幅减少，信噪比显著提升。这是我们核心的分析语料库。

**阶段二：实体与短语固化**

*   **输入:** `final_china_news.csv`
*   **处理过程:**
    1.  **合并文本:** 在处理的**第一步**，我们将每篇文章的 `TITLE` 和 `CONTENT` 在内存中动态地合并成一个单一的文本字符串，确保标题中的核心信息不会丢失。
    2.  **规则应用:** 使用 `Aho-Corasick` 算法，依据我们构建的合并规则库 (`merge_dict.pkl`)，对合并后的文本进行大规模、高性能的替换。例如，将 `people's bank of china` 替换为 `peoples_bank_of_china`。
*   **输出:**
    *   **`china_news_solidified.pkl`**
    *   **内容:** 同样包含 **180,630** 篇文章，但新增了一列 `content_solidified`，其中所有重要的多词概念都被“粘合”成不可分割的单元。根据最近一次运行，约 **96.3%** 的文章内容在此阶段被成功修改。
    *   **状态:** 文本的语义单元得到了保护，为后续的清洗和分词做好了准备。

**阶段三：深度文本清洗与词元化 (Tokenization)**

*   **输入:** `china_news_solidified.pkl`
*   **处理过程:** 这是我们数据处理的最后、也是最精细的一步。每一篇固化后的文本都会经过一个严格的清洗管道：
    1.  **智能连字符/分隔符处理:** 将复合词内的连字符 `-` 转为 `_`，同时将用作分隔符的 `--` 或 `---` 移除。
    2.  **spaCy处理:** 使用 `spaCy` 对文本进行分词、词性标注 (POS) 和词形还原的语言学分析。
    3.  **词性过滤:** 只保留最能代表主题信息的词性：名词 (NOUN), 专有名词 (PROPN), 形容词 (ADJ), 动词 (VERB), 和副词 (ADV)。
    4.  **“终极版”词形还原:** 应用我们定制的 `ultimate_lemmatizer`，它结合了 `WordNet` 和 `spaCy` 的优点，能正确地将副词转为形容词（如 `quickly` -> `quick`），同时保留如 `early`, `often` 等有意义的副词。
    5.  **停用词移除:** 使用一个集合了**三方来源**（spaCy默认、人工审核、项目专属）的最终停- 词库，移除所有噪音词。
    6.  **短词移除:** 移除长度小于3的无意义词元。
*   **输出:**
    *   **`china_news_cleaned_tokens.pkl`**
    *   **内容:** 最终的数据集。核心产出是 `tokens_for_lda` 这一列，其中每一行都是一个**纯净的词元列表 (List[str])**。
    *   **状态:** **模型可直接使用的数据。** 这是我们整个数据工程流程的最终交付物。

---

#### **三、 外部知识库与规则文件说明**

我们的流程严重依赖于一套精心管理和持续迭代的外部规则文件，它们是我们“人机协同”理念的核心。

*   **`china_keywords_collection.json`**
    *   **作用:** 用于**阶段一**的智能筛选。
    *   **内容:** 定义了与中国相关的核心关键词、它们的别名、以及从1到5的相关性等级（Tier）。这是我们定义“什么是中国新闻”的基础。

*   **`expert_rules.csv`**
    *   **作用:** 用于**阶段二**的实体固化，具有**最高优先级**。
    *   **内容:** 由我们手动维护的、必须被执行的合并规则。主要用于定义缩写（如 `pboc` -> `peoples_bank_of_china`）、同义词和重要的概念统一。

*   **`candidate_phrases_for_review_reviewed.csv`**
    *   **作用:** 用于**阶段二**的实体固化，作为专家规则的补充。
    *   **内容:** 这是我们“算法发现+人工审核”流程的产物。包含了大量从文本中自动发现、并经由我们手动决策（合并、忽略或设为停用词）的短语。

*   **`project_specific_stopwords.txt`**
    *   **作用:** 用于**阶段三**的深度清洗。
    *   **内容:** 一个简单的文本文件，每行一个我们希望在本项目中移除的、领域特定的噪音词（例如 `company`, `inc`, `wsj`, `journal` 等）。

*   **`stopwords.csv` (自动生成)**
    *   **作用:** **审计与审查**。
    *   **内容:** 这是 `03_Deep_Text_Cleaning.ipynb` 脚本自动生成的**最终停用词全集**，它合并了 spaCy 默认、人工审核 (`_reviewed.csv` 中标记为2的) 和项目专属 (`.txt` 文件) 三个来源的所有停用词。这个文件可以让我们清楚地知道，在模型训练前，到底哪些词被排除了。