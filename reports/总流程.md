
# **总流程**

## 序章：初始数据处理

> 代码文件在另一个项目

**核心目标**：将来源多样、格式不一的原始WSJ新闻数据（涵盖Excel和CSV），整合成一个统一、干净、按时序排列的单一主数据集 (`final_merged_all_news.csv`)，为后续所有分析步骤提供一个标准化的数据基础。

#### **序.1 格式标准化与初步去重 (Standardization & Initial Deduplication)**
*   **动作**：遍历所有原始数据文件。针对 `.xlsx` / `.xls` 格式的Excel文件，执行自动化转换脚本。
*   **处理内容**：
    *   将日期统一为标准的 `YYYY-MM-DD` 格式。
    *   在单个文件内部，基于日期、标题和内容进行初步去重。
    *   将处理后的数据统一保存为 CSV 格式。
*   **目的**：此步骤确保了所有零散的数据源在合并前都具有一致的结构和基本的数据质量。

#### **序.2 关键数据补全 (Critical Data Supplementation)**
*   **背景**：在数据整合过程中，发现特定年份（如2009年）的数据存在显著缺失。
*   **动作**：为保证数据集的完整性和时间序列的连续性，执行专项补充流程。从一个独立的、更完整的历史数据源（Excel格式）中提取缺失年份的数据，并将其精确地合并到主数据集中。
*   **目的**：修复已知的数据断层，提高数据集的整体质量。

#### **序.3 全局垂直合并 (Global Vertical Merging)**
*   **动作**：将所有经过标准化和补充后的CSV文件进行垂直合并。
*   **结果**：整合成一个包含从1984年至2025年所有新闻的单一、巨大的DataFrame。

#### **序.4 最终清洗与排序 (Final Cleaning & Sorting)**
*   **动作**：在合并后的全量数据集上，进行最后一次、也是最彻底的清洗和排序。
*   **处理内容**：
    *   **全局去重**：基于日期、标题和内容的组合，移除在不同源文件中可能存在的跨文件重复条目。
    *   **全局排序**：按日期对整个数据集进行严格的升序排列，确保新闻记录的时序性。
*   **目的**：保证最终数据集的唯一性和时序准确性。

**阶段性产出**

*   一个名为 `final_merged_all_news.csv` 的主数据文件（或为提高效率而转换的 Parquet 格式文件）。
*   该文件包含超过 **150万** 条新闻，时间跨度从 **1984年到2025年**，数据格式统一、无重复且按时序排列。
*   这个文件是整个项目后续所有分析的**唯一、可信的起点**。

---

## 一、筛选中国相关新闻 

> `reports/需求分析_01_China_News_Filtering.md`

**核心目标**：从海量的全球新闻语料库 (`final_merged_all_news.csv`) 中，通过一个“快速初筛”与“上下文精筛”相结合的两阶段流程，精准地筛选出与中国经济、政治、科技等领域高度相关的英文新闻，为后续的深度分析（如主题建模）生成一个高质量、高相关性的数据集。

**设计思路**：为了兼顾处理效率与筛选精度，本阶段采用两步走的策略。首先用最高效的算法进行大规模、宽泛的筛选，确保不遗漏任何潜在相关的文章；然后对这个较小的候选集进行基于上下文和语义的深度精筛，确保最终结果的质量。

### **1.1 准备工作：构建关键词体系与动态评分规则**

在开始筛选前，首先需要定义“什么是中国相关新闻”。这一步通过外部配置文件来完成，增强了流程的灵活性和可维护性。

#### **1.1.1 定义分级关键词体系**：
*   **输入**：`china_keywords_collection.json` 文件。
*   **内容**：该文件以结构化的方式定义了所有与中国相关的关键词。每个关键词不仅包含其多种别名（如 "People's Bank of China", "PBOC"），还被赋予了 **1-5级的相关性等级 (relevance_tier)**。
*   **作用**：这个分级体系是后续动态积分系统的基石。例如，"Xi Jinping" (T5) 的重要性远高于 "yuan" (T2)。

#### **1.1.2 定义动态积分与决策规则**：
*   **目标**：建立一套能够量化文章相关性的评分体系，强力奖励高相关性信号，同时对噪声和误导性信息进行惩罚。
*   **核心规则**：
    *   **高额开篇奖励 (Lead Bonus)**：若文章的**前10个句子**内出现高等级关键词（如T5/T4），则给予一个显著的、一次性的高额加分。这基于“重要文章会开门见山”的假设。
    *   **加权频率计分 (Weighted Frequency Score)**：扫描**整篇文章**，根据出现的每个关键词的等级（T1-T5）进行累积加分。
    *   **上下文风险惩罚 (Contextual Penalty)**：
        *   **假设性语境**：对出现在假设、推测性句子中的关键词（如 "if China were to..."）进行适度扣分。
        *   **否定性语境**：对被直接否定的关键词（如 "not related to Beijing's policy"）进行扣分。
*   **决策阈值**：设定一个最终的**接受分数线 (ACCEPTANCE_THRESHOLD)**，例如5.0分。总分达到此分数线的文章被采纳。

### **1.2 阶段一：快速流式初筛（求全）**

此阶段的目标是以最快的速度，从数百万篇新闻中捞出所有**可能**相关的文章，形成一个候选集。

#### **1.2.1 构建高速匹配引擎**：
*   **工具**：使用 `Flashtext` 库。
*   **动作**：将 `china_keywords_collection.json` 中定义的所有关键词及其别名，全部加载到一个不区分大小写的 `Flashtext` 处理器中。`Flashtext` 专门为大规模多关键词匹配优化，速度极快。

#### **1.2.2 执行流式处理 (Streaming)**：
*   **方法**：使用 `pandas` 的 `chunksize` 参数，以数据块的形式逐块读取巨大的原始新闻文件，避免一次性加载到内存中导致崩溃。
*   **逻辑**：对每一个数据块，使用 `Flashtext` 处理器进行扫描。只要一篇文章**至少包含一个**关键词，无论等级高低，都立即被筛选出来。
*   **产出**：将所有筛选出的文章实时追加写入到一个新的 `china_news_candidates.csv` 文件中。

*经过此阶段，我们得到了一个规模大大减小但覆盖全面的“候选新闻集”，为下一阶段的精细化处理做好了准备。*

### **1.3 阶段二：上下文动态积分精筛（求精）**

此阶段的目标是细致地评估每一篇候选文章的真实相关性，剔除在初筛中被误伤的“噪音文章”。

#### **1.3.1 加载NLP模型与匹配器**：
*   **工具**：使用 `spaCy` 这一强大的NLP库。
*   **动作**：加载预训练的英文模型，并利用 `spaCy` 的 `PhraseMatcher` 将所有关键词构建成匹配规则。`spaCy` 能够提供分句、词形还原、依赖关系分析等深度语言学特征，这是实现上下文判断的基础。

#### **1.3.2 并行处理与评分**：
*   **方法**：利用 `spaCy` 的 `nlp.pipe` 功能，对候选集中的所有文章进行高效的**并行化处理**，充分利用多核CPU资源。
*   **逻辑**：对每篇文章的 `spaCy` 处理结果（`Doc` 对象），依次应用 **1.1** 中定义的各项加分和扣分规则，计算出最终的 `relevance_score`。

#### **1.3.3 决策与产出**：
*   **决策**：将每篇文章的 `relevance_score` 与预设的 `ACCEPTANCE_THRESHOLD` 进行比较。
*   **最终产出**：
    *   **`final_china_news.csv`**：所有得分**达标**的文章被存入此文件。这是本阶段的核心产出，是高质量的、与中国高度相关的新闻集合。
    *   **`china_news_rejected_articles.csv`**：所有得分**未达标**的文章，连同其最终得分和评估详情，被存入此文件。这为后续分析规则、优化阈值提供了宝贵的依据。

*至此，我们便完成了从海量数据中精准筛选中国相关新闻的任务，得到的 `final_china_news.csv` 将作为下一阶段“深度清洗文本”的唯一输入。*

---

## 二、深度清洗文本

> `scripts/02_Entity_Phrase_Solidification.ipynb`
> `scripts/02_B_Phrase_Integration.ipynb`
> `scripts/02_C_Phrase_Application.ipynb`

### **2.1 阶段一：实体与短语的固化**

*   **目标**：在进行任何可能破坏短语结构的操作（如移除停用词）之前，先把所有多词概念 “粘合” 成不可分割的原子单元。

#### **2.1.1 半自动化候选短语发现**：
*   **算法发现**：在当前样本集上，运行 Gensim 的 Phrases 模型和 spaCy 的 NER 提取器，生成一个包含高频共现词组和名词短语的候选列表。
*   **排序与导出**：对候选列表按频率和 “粘合度” 排序，并导出一个 CSV 文件，留出 "action" 和 "standard\_form" 列待填写。

#### **2.1.2 手动审核与决策**：
*   **审核 CSV**：打开 CSV 文件，对排名前列的候选短语进行决策，填写 action (Merge/Stopword/Ignore) 和 standard\_form (如 wall\_street, peoples\_bank\_of\_china)。
*   **补充经验型规则**：基于领域知识，手动在决策文件中添加程序未能发现的合并规则（如缩写 PBOC -> peoples\_bank\_of\_china，概念 Alibaba -> china\_tech\_giant 等）。

#### **2.1.3 生成并应用合并规则**：
*   **生成词典**：写一个脚本，读取审核好的决策 CSV 文件，自动生成一个实体 / 短语合并词典（Python dict）。
*   **应用合并**：遍历样本集中的每一篇文章，使用这个词典，将所有匹配到的短语替换为其标准化的 “超级词”（例如，将文章中的 "people's bank of china" 替换为 "peoples\_bank\_of\_china"）。

### **2.2 阶段二：文本的全面标准化**

*   **目标**：对经过实体固化的文本进行彻底的、逐词级别的标准化处理。
*   **输入**： `china_news_solidified.pkl` 中的 `content_solidified` 列。

#### **2.2.1 统一转为小写 (Lowercase Conversion)**：
将所有文本统一转换为小写。此步骤是安全的，因为所有需要保留大小写信息的专有名词已在上一步被固化为带下划线的“超级词”。

#### **2.2.2 分词 (Tokenization)**：
使用`spaCy`高效地将每一篇长文本字符串，切分成一个由单词和“超级词”组成的列表（`List[str]`）。**特殊处理“超级词”**，确保如 `peoples_bank_of_china` 这样的固化实体被当作一个单独的`token`，而不是被错误地拆分开。

#### **2.2.3 词形还原 (Lemmatization)**：
遍历分词后的每一个`token`，使用`spaCy`的词形还原功能将其转换为词元（基本形态）。例如，`reports` -> `report`，`had` -> `have`。**对“超级词”不进行词形还原**，以保持其固化形态。

### **2.3 阶段三：噪声的最终移除**

*   **目标**：剔除所有对主题建模无用或有害的元素，只留下最纯净的、能代表主题信号的`token`。

#### **2.3.1 移除标点与数字 (Punctuation & Number Removal)**：
从`token`列表中，移除所有纯标点符号的`token`和纯数字的`token`。

#### **2.3.2 移除停用词 (Stopword Removal)**：
*   **构建最终停用词集**：
    *   加载`spaCy`的默认英文停用词列表。
    *   加载我们在 `02_C` 流程中生成的自定义停用词文件 `new_stopwords.pkl`。
    *   **（新增步骤）** 加载一个**手动维护的、项目专属的停用词文件** `project_specific_stopwords.txt`，用于添加在分析过程中发现的、需要被移除的领域噪音词（如 `wsj`, `journal`, `company`, `inc` 等）。
    *   将以上三个来源的停用词合并成一个最终的、全面的停用词集合。
*   **执行过滤**：从`token`列表中，移除所有存在于最终停用词集中的词。

#### **2.3.3 应用词性（Part-of-Speech, POS）过滤**：
*   **逻辑**：为了进一步提纯主题信号，只保留最能代表“内容”的词性，如**名词（Noun）、专有名词（Proper Noun）、形容词（Adjective）和动词（Verb）**。移除如介词、副词、代词等对主题区分度贡献较小的词。
*   **动作**：在词形还原后、移除停用词前，为每个`token`打上词性标签，并在最后一步过滤掉不符合我们白名单词性的`token`。

#### **2.3.4 过滤短词**：
移除长度小于3个字符的`token`，以过滤掉残留的无意义缩写或噪声。

---

## 三、监督式主题建模与分析 (sLDA)

**核心目标：** 将深度清洗后的文本数据与时间序列的汇率数据对齐，构建并训练一个sLDA模型，以发现能够直接解释和预测汇率波动的潜在经济叙事主题。

### **3.1 阶段一：数据对齐与准备**

*   **目标**：为sLDA模型准备好两大核心输入：文档（Token列表）和与之严格对应的响应变量（汇率变动）。

#### **3.1.1 准备响应变量 (Response Variable)**：
*   **动作**：加载人民币汇率（如USD/CNY）的日度时间序列数据。
*   **处理**：计算**日度对数收益率** `log(price_t / price_{t-1})`，作为我们模型要预测的目标 `y`。同时，处理节假日和周末，确保每个交易日都有一个对应的`y`值。

#### **3.1.2 对齐文本与响应变量**：
*   **输入**：`china_news_cleaned_tokens.pkl` 和处理好的日度汇率数据。
*   **逻辑**：
    1.  **按日合并新闻**：将 `china_news_cleaned_tokens.pkl` 中同一天发布的所有新闻的 `tokens_for_lda` 列表合并成一个大的列表，代表“当天的新闻语料”。
    2.  **时间戳对齐**：以**日期**为主键，将合并后的每日新闻语料与前一步计算出的每日汇率变动率 `y` 进行严格匹配。
    3.  **产出**：一个最终的数据集，每一行包含：`[日期, 当日合并后的Token列表, 当日汇率变动率y]`。

### **3.2 阶段二：Gensim语料库生成**

*   **目标**：将对齐好的每日Token列表转换为`gensim`库所需的**词典**和**语料库**格式。

#### **3.2.1 创建并过滤词典**：
*   **动作**：
    1.  使用所有每日新闻的Token列表，创建 `gensim.corpora.Dictionary`。
    2.  应用 `filter_extremes()` 方法（例如，`no_below=20`, `no_above=0.5`）来移除极端词汇，净化词汇表。

#### **3.2.2 创建语料库 (DTM)**：
*   **动作**：使用过滤后的词典，将每个每日新闻的Token列表，通过 `dictionary.doc2bow()` 转换为词袋（BoW）格式。

### **3.3 阶段三：sLDA模型训练与主题发现**

*   **目标**：使用准备好的语料库、词典和响应变量，训练sLDA模型。

#### **3.3.1 模型实现与选择**：
*   **动作**：研究并选择一个合适的sLDA Python实现库（例如，`pyslda`，`tomotopy`，或公开的研究代码）。由于 `gensim` 官方没有直接支持sLDA，这一步需要技术选型。

#### **3.3.2 模型训练**：
*   **输入**：词典、语料库、以及与语料库每行严格对应的汇率变动率 `y` 列表。
*   **超参数调优**：
    *   **主题数 (K)**：这是最重要的超参数。我们需要训练多个不同K值的模型（例如，从20到200），然后通过评估指标（如预测任务的R²或MSE，以及主题的连贯性分数）来选择最优的K。
    *   其他超参数（如 `alpha`, `beta`）可以先使用默认值，后续再进行微调。
*   **产出**：一个训练好的、最优的sLDA模型。

### **3.4 阶段四：结果分析与叙事解读**

*   **目标**：深入解读sLDA模型的输出，理解新闻叙事与汇率波动的关系。

#### **3.4.1 主题解读**：
*   **动作**：
    1.  **提取主题-词汇分布**：对于训练好的模型，提取出每个主题下概率最高的关键词。
    2.  **人工命名主题**：根据每个主题的关键词，为它们赋予有意义的名称（例如，“贸易紧张局势”、“央行干预预期”、“科技股抛售”等）。

#### **3.4.2 分析主题与汇率的关系**：
*   **动作**：
    1.  **提取回归系数**：sLDA模型会直接给出一个回归系数向量 `η`，其中每个元素 `η_k` 代表了**主题k对汇率变动的平均影响**。
    2.  **识别关键主题**：找出那些回归系数 `η_k` 绝对值最大、最显著的主题。这些就是对汇率影响最大的新闻叙事。例如，一个大的负系数可能对应于“资本外流担忧”主题。

#### **3.4.3 叙事归因与案例研究 (Narrative Attribution)**：
*   **动作**：
    1.  **计算每日主题强度**：对于我们时间序列中的每一天，sLDA可以计算出当天新闻中每个主题的强度（即主题分布）。
    2.  **寻找关键事件日**：找到那些汇率发生剧烈波动的日期。
    3.  **进行叙事归因**：查看在这些关键日期，是哪些“关键主题”（根据3.4.2）的强度出现了显著飙升。
    4.  **追溯原文**：回到原始新闻数据库，找出在这些关键日期，对那些飙升的关键主题贡献最大的具体新闻文章，进行定性“细读”，从而为我们的量化结果提供生动、有力的叙事证据。

